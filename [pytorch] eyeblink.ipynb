{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bachnguyen/Desktop/projects/venv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "  5%|â–Œ         | 3/60 [00:03<01:08,  1.21s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m occumpy_mem(cuda_device)\n\u001b[1;32m     26\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(\u001b[39m60\u001b[39m)):\n\u001b[0;32m---> 27\u001b[0m     time\u001b[39m.\u001b[39;49msleep(\u001b[39m1\u001b[39;49m)\n\u001b[1;32m     28\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mDone\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# declare which gpu device to use\n",
    "cuda_device = '0'\n",
    "\n",
    "def check_mem(cuda_device):\n",
    "    devices_info = os.popen('\"/usr/bin/nvidia-smi\" --query-gpu=memory.total,memory.used --format=csv,nounits,noheader').read().strip().split(\"\\n\")\n",
    "    total, used = devices_info[int(cuda_device)].split(',')\n",
    "    return total,used\n",
    "\n",
    "def occumpy_mem(cuda_device):\n",
    "    total, used = check_mem(cuda_device)\n",
    "    total = int(total)\n",
    "    used = int(used)\n",
    "    max_mem = int(total * 0.9)\n",
    "    block_mem = max_mem - used\n",
    "    x = torch.cuda.FloatTensor(256,1024,block_mem)\n",
    "    del x\n",
    "    \n",
    "# if __name__ == '__main__':\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = cuda_device\n",
    "occumpy_mem(cuda_device)\n",
    "for _ in tqdm(range(60)):\n",
    "    time.sleep(1)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bachnguyen/Desktop/projects/venv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bachnguyen/Desktop/projects/venv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.0+cu113\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import skimage\n",
    "from skimage.restoration import denoise_tv_chambolle\n",
    "\n",
    "print(torch.__version__)\n",
    "\n",
    "\n",
    "# transform = {\n",
    "#     'train': transforms.Compose([\n",
    "#         transforms.ToTensor()\n",
    "#     ])\n",
    "# }\n",
    "\n",
    "# base_dir = \"./datasets\"\n",
    "# data = torchvision.datasets.ImageFolder(root=f\"{base_dir}/train/training\", transform=transform['train'])\n",
    "# data_loader = torch.utils.data.DataLoader(dataset=data, batch_size=16, shuffle=False)\n",
    "\n",
    "# batch_idx = 0\n",
    "\n",
    "# for batch in next(iter(data_loader)):\n",
    "#     img, label = batch\n",
    "#     print(f\"[{batch_idx}]: {img.shape} - {label.shape}\")\n",
    "#     if batch_idx == 0:\n",
    "#         print(img)\n",
    "#         print(label)\n",
    "#     batch_idx += 1\n",
    "\n",
    "class EyeBlinkDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, train=True, use_threshold=0, threshold=96):\n",
    "        if train:\n",
    "            self.seq_list = sorted(glob.glob('datasets/train/training/blink/*'), key=lambda x: int(x.split('/')[-1]))\n",
    "            self.seq_list.extend(sorted(glob.glob('datasets/train/training/unblink/*'), key=lambda x: int(x.split('/')[-1])))\n",
    "        else:\n",
    "            self.seq_list = sorted(glob.glob('datasets/test/test/blink/*'), key=lambda x: int(x.split('/')[-1]))\n",
    "            self.seq_list.extend(sorted(glob.glob('datasets/test/test/unblink/*'), key=lambda x: int(x.split('/')[-1])))\n",
    "\n",
    "        ## remove frames without left/right eye\n",
    "        tmp = []; idx_list = []\n",
    "        for i, seq in enumerate(self.seq_list):\n",
    "            frames_fnames = sorted(glob.glob(f'{seq}/13/zuo/*'))\n",
    "            # print(seq, len(frames_fnames))\n",
    "            if len(frames_fnames) == 0:\n",
    "                idx_list.append(i)\n",
    "                tmp.append(seq)\n",
    "        for i in sorted(idx_list, reverse=True):\n",
    "            del self.seq_list[i]\n",
    "        print(tmp)\n",
    "\n",
    "        ## remove frames smaller/larger than threshold\n",
    "        if use_threshold != 0:\n",
    "            idx_list = []\n",
    "            for i, seq in enumerate(self.seq_list):\n",
    "                frames_fnames = sorted(glob.glob(f'{seq}/13/zuo/*'), key=lambda x: int(x.split('/')[-1].replace('eye_zuo.bmp','')) )\n",
    "                \n",
    "                min_size = 9999; max_size=0\n",
    "                for fname in frames_fnames:\n",
    "                    frame = Image.open(fname)\n",
    "                    if frame.size[0] <= min_size: min_size = frame.size[0]\n",
    "                    if frame.size[0] >= max_size: max_size = frame.size[0]\n",
    "                \n",
    "                if use_threshold == -1 and max_size > threshold or use_threshold == 1 and min_size < threshold:\n",
    "                    # print(i, ' - heyyy 1 - ', seq, min_size, max_size)\n",
    "                    idx_list.append(i)\n",
    "                # else:\n",
    "                    # print(i, ' - heyyy 2- ',seq, min_size, max_size)\n",
    "            # print('heyyy', idx_list)\n",
    "            for i in sorted(idx_list, reverse=True):\n",
    "                del self.seq_list[i]\n",
    "\n",
    "        self.resize = transforms.Resize([96,96]) #[96,96])\n",
    "        self.to_tensor = transforms.ToTensor()\n",
    "        self.sizes = []\n",
    "        self.train = train\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.seq_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # print(self.seq_list[idx])\n",
    "        frames_fnames = sorted(glob.glob(f'{self.seq_list[idx]}/13/zuo/*'), key=lambda x: int(x.split('/')[-1].replace('eye_zuo.bmp','')) )\n",
    "        frames_data = np.zeros((96,96,13,3)) #np.zeros((96,96,13,3))\n",
    "\n",
    "        for i, fname in enumerate(frames_fnames):\n",
    "            frame = Image.open(fname)\n",
    "            self.sizes.append(frame.size[0])\n",
    "            #frame = self.resize(frame)\n",
    "            frame = np.array(frame)\n",
    "            if frame.shape[0] < 96: frame = cv2.resize(frame, (96,96), interpolation=cv2.INTER_CUBIC)\n",
    "            if frame.shape[0] > 96: frame = cv2.resize(frame, (96,96), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "            #frame = denoise_tv_chambolle(frame, weight=0.1, channel_axis=-1)\n",
    "            #frame = skimage.transform.resize(frame, (48,48))\n",
    "            frame = frame / 255.0\n",
    "            frames_data[:,:,i,:] = frame\n",
    "        \n",
    "        label = 1 if \"/blink/\" in self.seq_list[idx] else 0\n",
    "        return frames_data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import skimage\n",
    "from skimage.restoration import denoise_tv_chambolle\n",
    "\n",
    "\n",
    "class EyeBlinkDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, train=True):\n",
    "        if train:\n",
    "            self.seq_list = sorted(glob.glob('datasets/train/training/blink/*'), key=lambda x: int(x.split('/')[-1]))\n",
    "            self.seq_list.extend(sorted(glob.glob('datasets/train/training/unblink/*'), key=lambda x: int(x.split('/')[-1])))\n",
    "        else:\n",
    "            self.seq_list = sorted(glob.glob('datasets/test/test/blink/*'), key=lambda x: int(x.split('/')[-1]))\n",
    "            self.seq_list.extend(sorted(glob.glob('datasets/test/test/unblink/*'), key=lambda x: int(x.split('/')[-1])))\n",
    "\n",
    "        ## remove frames without left/right eye\n",
    "        tmp = []; idx_list = []\n",
    "        for i, seq in enumerate(self.seq_list):\n",
    "            frames_fnames = sorted(glob.glob(f'{seq}/13/zuo/*'))\n",
    "            # print(seq, len(frames_fnames))\n",
    "            if len(frames_fnames) == 0:\n",
    "                idx_list.append(i)\n",
    "                tmp.append(seq)\n",
    "        for i in sorted(idx_list, reverse=True):\n",
    "            del self.seq_list[i]\n",
    "        print(tmp)\n",
    "\n",
    "        self.resize = transforms.Resize([96,96]) #[96,96])\n",
    "        self.to_tensor = transforms.ToTensor()\n",
    "        self.sizes = []\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.seq_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        frames_fnames = sorted(glob.glob(f'{self.seq_list[idx]}/13/zuo/*'), key=lambda x: int(x.split('/')[-1].replace('eye_zuo.bmp','')) )\n",
    "        frames_data = np.zeros((96,96,13,3)) #np.zeros((96,96,13,3))\n",
    "\n",
    "        for i, fname in enumerate(frames_fnames):\n",
    "            frame = Image.open(fname)\n",
    "            self.sizes.append(frame.size[0])\n",
    "            #frame = self.resize(frame)\n",
    "            frame = np.array(frame)\n",
    "            frame = cv2.resize(frame, (96,96), interpolation=cv2.INTER_CUBIC)\n",
    "            #frame = denoise_tv_chambolle(frame, weight=0.1, channel_axis=-1)\n",
    "            #frame = skimage.transform.resize(frame, (48,48))\n",
    "            frame = frame / 255.0\n",
    "            frames_data[:,:,i,:] = frame\n",
    "        \n",
    "        label = 1 if \"/blink/\" in self.seq_list[idx] else 0\n",
    "        return frames_data, label\n",
    "'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Conv1x1(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Conv1x1, self).__init__()\n",
    "        self.conv1x1 = nn.Conv3d(in_channels=in_channels, out_channels=out_channels, kernel_size=(3,1,1), stride=(1,2,2), padding=(1,0,0))\n",
    "        self.batchnorm = nn.BatchNorm3d(num_features = out_channels)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1x1(x)\n",
    "        x = self.batchnorm(x)\n",
    "        return x\n",
    "\n",
    "class Conv3x3(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Conv3x3, self).__init__()\n",
    "        self.conv3x3 = nn.Conv3d(in_channels=in_channels, out_channels=out_channels, kernel_size=(3,3,3), stride=(1,1,1), padding=(1,1,1))\n",
    "        self.batchnorm = nn.BatchNorm3d(num_features=out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv3x3(x)\n",
    "        x = self.batchnorm(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "class Conv5x5(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Conv5x5, self).__init__()\n",
    "        self.conv5x5 = nn.Conv3d(in_channels=in_channels, out_channels=out_channels, kernel_size=(3,5,5), stride=(1,1,1), padding=(1,2,2))\n",
    "        self.batchnorm = nn.BatchNorm3d(num_features=out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv5x5(x)\n",
    "        x = self.batchnorm(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "class PyramidBlock2B(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(PyramidBlock2B, self).__init__()\n",
    "        self.branch1 = Conv1x1(in_channels=in_channels, out_channels=out_channels)\n",
    "        self.branch2 = nn.Sequential(\n",
    "            Conv3x3(in_channels=in_channels, out_channels=out_channels),\n",
    "            Conv1x1(in_channels=out_channels, out_channels=out_channels)\n",
    "        )\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.branch1(x)\n",
    "        x2 = self.branch2(x)\n",
    "        x = torch.add(x1,x2)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "class PyramidBlock3B(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(PyramidBlock3B, self).__init__()\n",
    "        self.branch1 = Conv1x1(in_channels=in_channels, out_channels=out_channels)\n",
    "        self.branch2 = nn.Sequential(\n",
    "            Conv3x3(in_channels=in_channels, out_channels=out_channels),\n",
    "            Conv1x1(in_channels=out_channels, out_channels=out_channels)\n",
    "        )\n",
    "        self.branch3 = nn.Sequential(\n",
    "            Conv5x5(in_channels=in_channels, out_channels=out_channels),\n",
    "            Conv3x3(in_channels=out_channels, out_channels=out_channels),\n",
    "            Conv1x1(in_channels=out_channels, out_channels=out_channels)\n",
    "        )\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x1 = self.branch1(x)\n",
    "        x2 = self.branch2(x)\n",
    "        x3 = self.branch3(x)\n",
    "        x = x1+x2+x3\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "class ConvInput(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ConvInput, self).__init__()\n",
    "        self.conv3x3 = nn.Conv3d(in_channels=in_channels, out_channels=out_channels, kernel_size=(3,3,3), stride=(2,1,1), padding=(1,1,1))\n",
    "        self.batchnorm = nn.BatchNorm3d(num_features=out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool3d(kernel_size=(3,3,3), stride=(2,1,1), padding=(1,1,1))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv3x3(x)\n",
    "        x = self.batchnorm(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class P2B2(nn.Module):\n",
    "    def __init__(self, in_channels=3, mid_channels=64):\n",
    "        super(P2B2, self).__init__()\n",
    "        self.conv_input = ConvInput(in_channels=in_channels, out_channels=mid_channels)\n",
    "        self.block1 = PyramidBlock2B(in_channels=mid_channels, out_channels=mid_channels)\n",
    "        self.block2 = PyramidBlock2B(in_channels=mid_channels, out_channels=mid_channels*2)\n",
    "        self.avgpool = nn.AvgPool3d(kernel_size=(4,24,24)) #nn.AvgPool3d(kernel_size=(4,24,24))\n",
    "\n",
    "        self.linear = nn.Linear(128,2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv_input(x)\n",
    "        # print(x.shape)\n",
    "        x = self.block1(x)\n",
    "        # print(x.shape)\n",
    "        x = self.block2(x)\n",
    "        # print(x.shape)\n",
    "        x = self.avgpool(x)\n",
    "        # print(x.shape)\n",
    "        x = x.squeeze()\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "class P3B3(nn.Module):\n",
    "    def __init__(self, in_channels=3, mid_channels=64):\n",
    "        super(P3B3, self).__init__()\n",
    "        self.conv_input = ConvInput(in_channels=in_channels, out_channels=mid_channels)\n",
    "        self.block1 = PyramidBlock3B(in_channels=mid_channels, out_channels=mid_channels)\n",
    "        self.block2 = PyramidBlock3B(in_channels=mid_channels, out_channels=mid_channels*2)\n",
    "        self.block3 = PyramidBlock3B(in_channels=mid_channels*2, out_channels=mid_channels*4)\n",
    "        self.avgpool = nn.AvgPool3d(kernel_size=(4,12,12))\n",
    "        self.linear = nn.Linear(256,2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv_input(x)\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.squeeze()\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "# class P4B3(nn.Module):\n",
    "#         def __init__(self, in_channels=3, mid_channels=64):\n",
    "#             super(P4B3, self).__init__()\n",
    "#             self.conv_input = ConvInput(in_channels=in_channels, out_channels=mid_channels)\n",
    "#             self.block1 = PyramidBlock3B(in_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model - depthwise & pointwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Conv1x1_Separable(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Conv1x1_Separable, self).__init__()\n",
    "        self.conv1_dw = nn.Conv3d(in_channels=in_channels, out_channels=in_channels, kernel_size=(3,1,1), stride=(1,2,2), padding=(1,0,0), groups=in_channels)\n",
    "        self.batchnorm1 = nn.BatchNorm3d(num_features = in_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2_pw = nn.Conv3d(in_channels=in_channels, out_channels=out_channels, kernel_size=(1,1,1), stride=(1,1,1), padding=0)\n",
    "        self.batchnorm2 = nn.BatchNorm3d(num_features= out_channels)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x = self.conv1_dw(x)\n",
    "        # x = self.batchnorm1(x)\n",
    "        # x = self.relu(x)\n",
    "        # x = self.conv2_pw(x)\n",
    "        # x = self.batchnorm2(x)\n",
    "        x = self.batchnorm2(self.conv2_pw(self.relu(self.batchnorm1(self.conv1_dw(x)))))\n",
    "        return x\n",
    "\n",
    "class Conv3x3_Separable(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Conv3x3_Separable, self).__init__()\n",
    "        self.conv1_dw = nn.Conv3d(in_channels=in_channels, out_channels=in_channels, kernel_size=(3,3,3), stride=(1,1,1), padding=(1,1,1), groups=in_channels)\n",
    "        self.batchnorm1 = nn.BatchNorm3d(num_features=in_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2_pw = nn.Conv3d(in_channels=in_channels, out_channels=out_channels, kernel_size=(1,1,1), stride=(1,1,1), padding=0)\n",
    "        self.batchnorm2 = nn.BatchNorm3d(num_features=out_channels)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x = self.conv1_dw(x)\n",
    "        # x = self.batchnorm1(x)\n",
    "        # x = self.relu(x)\n",
    "        # x = self.conv2_pw(x)\n",
    "        # x = self.batchnorm2(x)\n",
    "        # x = self.relu(x)\n",
    "        x = self.relu(self.batchnorm2(self.conv2_pw(self.relu(self.batchnorm1(self.conv1_dw(x))))))\n",
    "        return x\n",
    "\n",
    "class Conv5x5_Separable(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Conv5x5_Separable, self).__init__()\n",
    "        self.conv1_dw = nn.Conv3d(in_channels=in_channels, out_channels=in_channels, kernel_size=(3,5,5), stride=(1,1,1), padding=(1,2,2), groups=in_channels)\n",
    "        self.batchnorm1 = nn.BatchNorm3d(num_features=in_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2_pw = nn.Conv3d(in_channels=in_channels, out_channels=out_channels, kernel_size=(1,1,1), stride=(1,1,1), padding=0 )\n",
    "        self.batchnorm2 = nn.BatchNorm3d(num_features=out_channels)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x = self.conv1_dw(x)\n",
    "        # x = self.batchnorm1(x)\n",
    "        # x = self.relu(x)\n",
    "        # x = self.conv2_pw(x)\n",
    "        # x = self.batchnorm2(x)\n",
    "        # x = self.relu(x)\n",
    "        x = self.relu(self.batchnorm2(self.conv2_pw(self.relu(self.batchnorm1(self.conv1_dw(x))))))\n",
    "        return x\n",
    "\n",
    "class PyramidBlock2B_SeparableConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(PyramidBlock2B_SeparableConv, self).__init__()\n",
    "        self.branch1 = Conv1x1_Separable(in_channels=in_channels, out_channels=out_channels)\n",
    "        self.branch2 = nn.Sequential(\n",
    "            Conv3x3_Separable(in_channels=in_channels, out_channels=out_channels),\n",
    "            Conv1x1_Separable(in_channels=out_channels, out_channels=out_channels)\n",
    "        )\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.branch1(x)\n",
    "        x2 = self.branch2(x)\n",
    "        x = torch.add(x1,x2)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "class PyramidBlock3B_SeparableConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(PyramidBlock3B_SeparableConv, self).__init__()\n",
    "        self.branch1 = Conv1x1_Separable(in_channels=in_channels, out_channels=out_channels)\n",
    "        self.branch2 = nn.Sequential(\n",
    "            Conv3x3_Separable(in_channels=in_channels, out_channels=out_channels),\n",
    "            Conv1x1_Separable(in_channels=out_channels, out_channels=out_channels)\n",
    "        )\n",
    "        self.branch3 = nn.Sequential(\n",
    "            Conv5x5_Separable(in_channels=in_channels, out_channels=out_channels),\n",
    "            Conv3x3_Separable(in_channels=out_channels, out_channels= out_channels),\n",
    "            Conv1x1_Separable(in_channels=out_channels, out_channels=out_channels)\n",
    "        )\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x1 = self.branch1(x)\n",
    "        # x2 = self.branch2(x)\n",
    "        # x3 = self.branch3(x)\n",
    "        # x = x1 + x2 + x3\n",
    "        # x = self.relu(x)\n",
    "        x = self.relu(self.branch1(x)+self.branch2(x)+self.branch3(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class ConvInput(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ConvInput, self).__init__()\n",
    "        self.conv3x3 = nn.Conv3d(in_channels=in_channels, out_channels=out_channels, kernel_size=(3,3,3), stride=(2,1,1), padding=(1,1,1))\n",
    "        self.batchnorm = nn.BatchNorm3d(num_features=out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool3d(kernel_size=(3,3,3), stride=(2,1,1), padding=(1,1,1))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv3x3(x)\n",
    "        x = self.batchnorm(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class P2B2_SeparableConv(nn.Module):\n",
    "    def __init__(self, in_channels=3, mid_channels=64):\n",
    "        super(P2B2_SeparableConv, self).__init__()\n",
    "        self.conv_input = ConvInput(in_channels=in_channels, out_channels=mid_channels)\n",
    "        self.block1 = PyramidBlock2B_SeparableConv(in_channels=mid_channels, out_channels=mid_channels)\n",
    "        self.block2 = PyramidBlock2B_SeparableConv(in_channels=mid_channels, out_channels=mid_channels*2)\n",
    "        self.avgpool = nn.AvgPool3d(kernel_size=(4,24,24)) #nn.AvgPool3d(kernel_size=(4,24,24))\n",
    "\n",
    "        self.linear = nn.Linear(128,2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv_input(x)\n",
    "        # print(x.shape)\n",
    "        x = self.block1(x)\n",
    "        # print(x.shape)\n",
    "        x = self.block2(x)\n",
    "        # print(x.shape)\n",
    "        x = self.avgpool(x)\n",
    "        # print(x.shape)\n",
    "        x = x.squeeze()\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "class P3B3_SeparableConv(nn.Module):\n",
    "    def __init__(self, in_channels=3, mid_channels=64):\n",
    "        super(P3B3_SeparableConv, self).__init__()\n",
    "        self.conv_input = ConvInput(in_channels=in_channels, out_channels=mid_channels)\n",
    "        self.block1 = PyramidBlock3B_SeparableConv(in_channels=mid_channels, out_channels=mid_channels)\n",
    "        self.block2 = PyramidBlock3B_SeparableConv(in_channels=mid_channels, out_channels=mid_channels*2)\n",
    "        self.block3 = PyramidBlock3B_SeparableConv(in_channels=mid_channels*2, out_channels=mid_channels*4)\n",
    "        self.avgpool = nn.AvgPool3d(kernel_size=(4,12,12))\n",
    "\n",
    "        self.linear = nn.Linear(256, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x = self.conv_input(x)\n",
    "        # x = self.block1(x)\n",
    "        # x = self.block2(x)\n",
    "        # x = self.block3(x)\n",
    "        # x = self.avgpool(x)\n",
    "        # x = x.squeeze()\n",
    "        # x = self.linear(x)\n",
    "        x = self.linear(self.avgpool(self.block3(self.block2(self.block1(self.conv_input(x))))).squeeze())\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-13 13:25:36.841857: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-13 13:25:36.935592: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-02-13 13:25:36.956633: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-02-13 13:25:37.292657: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/bachnguyen/Desktop/projects/venv/lib/python3.8/site-packages/cv2/../../lib64:/usr/local/cuda-11.2/lib64:\n",
      "2023-02-13 13:25:37.292708: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/bachnguyen/Desktop/projects/venv/lib/python3.8/site-packages/cv2/../../lib64:/usr/local/cuda-11.2/lib64:\n",
      "2023-02-13 13:25:37.292713: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['datasets/train/training/blink/8', 'datasets/train/training/blink/141', 'datasets/train/training/blink/142', 'datasets/train/training/blink/151', 'datasets/train/training/blink/152', 'datasets/train/training/blink/153', 'datasets/train/training/blink/154', 'datasets/train/training/blink/155', 'datasets/train/training/blink/158', 'datasets/train/training/blink/159', 'datasets/train/training/blink/160', 'datasets/train/training/blink/179', 'datasets/train/training/blink/181', 'datasets/train/training/blink/182', 'datasets/train/training/blink/198', 'datasets/train/training/unblink/55', 'datasets/train/training/unblink/61', 'datasets/train/training/unblink/76', 'datasets/train/training/unblink/78', 'datasets/train/training/unblink/79', 'datasets/train/training/unblink/80', 'datasets/train/training/unblink/82', 'datasets/train/training/unblink/83', 'datasets/train/training/unblink/93', 'datasets/train/training/unblink/94', 'datasets/train/training/unblink/95', 'datasets/train/training/unblink/144', 'datasets/train/training/unblink/146']\n",
      "420\n",
      "epoch: 0 - train_acc: 0.611904761904762 - loss: 0.6724196980977518 - min_loss: 0.6724196980977518\n",
      "epoch: 1 - train_acc: 0.6571428571428571 - loss: 0.6154141821365663 - min_loss: 0.6154141821365663\n",
      "epoch: 2 - train_acc: 0.6785714285714286 - loss: 0.5995065836667729 - min_loss: 0.5995065836667729\n",
      "epoch: 3 - train_acc: 0.6761904761904762 - loss: 0.5987135073201313 - min_loss: 0.5987135073201313\n",
      "epoch: 4 - train_acc: 0.7142857142857143 - loss: 0.5352997203263272 - min_loss: 0.5352997203263272\n",
      "epoch: 5 - train_acc: 0.7380952380952381 - loss: 0.5385907354081286 - min_loss: 0.5352997203263272\n",
      "epoch: 6 - train_acc: 0.7238095238095238 - loss: 0.5215289430617169 - min_loss: 0.5215289430617169\n",
      "epoch: 7 - train_acc: 0.7952380952380952 - loss: 0.47140986836928833 - min_loss: 0.47140986836928833\n",
      "epoch: 8 - train_acc: 0.8023809523809524 - loss: 0.45154575084536613 - min_loss: 0.45154575084536613\n",
      "epoch: 9 - train_acc: 0.8071428571428572 - loss: 0.44112440824184507 - min_loss: 0.44112440824184507\n",
      "epoch: 10 - train_acc: 0.7880952380952381 - loss: 0.4347739491164809 - min_loss: 0.4347739491164809\n",
      "epoch: 11 - train_acc: 0.819047619047619 - loss: 0.41371521818484386 - min_loss: 0.41371521818484386\n",
      "epoch: 12 - train_acc: 0.8214285714285714 - loss: 0.39259215979321593 - min_loss: 0.39259215979321593\n",
      "epoch: 13 - train_acc: 0.8166666666666667 - loss: 0.3919777107146983 - min_loss: 0.3919777107146983\n",
      "epoch: 14 - train_acc: 0.8047619047619048 - loss: 0.4130627208752481 - min_loss: 0.3919777107146983\n",
      "epoch: 15 - train_acc: 0.8142857142857143 - loss: 0.433760517989782 - min_loss: 0.3919777107146983\n",
      "epoch: 16 - train_acc: 0.8261904761904761 - loss: 0.40443142677604393 - min_loss: 0.3919777107146983\n",
      "epoch: 17 - train_acc: 0.8380952380952381 - loss: 0.3523168342367685 - min_loss: 0.3523168342367685\n",
      "epoch: 18 - train_acc: 0.861904761904762 - loss: 0.31027175629477893 - min_loss: 0.31027175629477893\n",
      "epoch: 19 - train_acc: 0.8452380952380952 - loss: 0.31077555371512405 - min_loss: 0.31027175629477893\n",
      "epoch: 20 - train_acc: 0.9 - loss: 0.26905617711958996 - min_loss: 0.26905617711958996\n",
      "epoch: 21 - train_acc: 0.8785714285714286 - loss: 0.29165564765420177 - min_loss: 0.26905617711958996\n",
      "epoch: 22 - train_acc: 0.8571428571428571 - loss: 0.34689333499711794 - min_loss: 0.26905617711958996\n",
      "epoch: 23 - train_acc: 0.8571428571428571 - loss: 0.36512793132218413 - min_loss: 0.26905617711958996\n",
      "epoch: 24 - train_acc: 0.888095238095238 - loss: 0.3101351759914203 - min_loss: 0.26905617711958996\n",
      "epoch: 25 - train_acc: 0.8904761904761904 - loss: 0.2689293682103445 - min_loss: 0.2689293682103445\n",
      "epoch: 26 - train_acc: 0.9333333333333333 - loss: 0.21267372454588043 - min_loss: 0.21267372454588043\n",
      "epoch: 27 - train_acc: 0.9 - loss: 0.218845010151854 - min_loss: 0.21267372454588043\n",
      "epoch: 28 - train_acc: 0.8952380952380953 - loss: 0.23512790281214307 - min_loss: 0.21267372454588043\n",
      "epoch: 29 - train_acc: 0.9428571428571428 - loss: 0.16352735439158175 - min_loss: 0.16352735439158175\n",
      "epoch: 30 - train_acc: 0.9333333333333333 - loss: 0.20210946345221606 - min_loss: 0.16352735439158175\n",
      "epoch: 31 - train_acc: 0.9119047619047619 - loss: 0.2060655892232932 - min_loss: 0.16352735439158175\n",
      "epoch: 32 - train_acc: 0.8833333333333333 - loss: 0.2577430579077651 - min_loss: 0.16352735439158175\n",
      "epoch: 33 - train_acc: 0.9333333333333333 - loss: 0.18658744680441752 - min_loss: 0.16352735439158175\n",
      "epoch: 34 - train_acc: 0.919047619047619 - loss: 0.19108339800659344 - min_loss: 0.16352735439158175\n",
      "epoch: 35 - train_acc: 0.9285714285714286 - loss: 0.18218904393693386 - min_loss: 0.16352735439158175\n",
      "epoch: 36 - train_acc: 0.8904761904761904 - loss: 0.2934548990284556 - min_loss: 0.16352735439158175\n",
      "epoch: 37 - train_acc: 0.9357142857142857 - loss: 0.18923329611353884 - min_loss: 0.16352735439158175\n",
      "epoch: 38 - train_acc: 0.9285714285714286 - loss: 0.21620339482159193 - min_loss: 0.16352735439158175\n",
      "epoch: 39 - train_acc: 0.95 - loss: 0.15238550171900106 - min_loss: 0.15238550171900106\n",
      "epoch: 40 - train_acc: 0.9523809523809523 - loss: 0.1367072240001172 - min_loss: 0.1367072240001172\n",
      "epoch: 41 - train_acc: 0.9666666666666667 - loss: 0.10634203406105878 - min_loss: 0.10634203406105878\n",
      "epoch: 42 - train_acc: 0.9642857142857143 - loss: 0.11746037621151581 - min_loss: 0.10634203406105878\n",
      "epoch: 43 - train_acc: 0.9428571428571428 - loss: 0.13609062011778528 - min_loss: 0.10634203406105878\n",
      "epoch: 44 - train_acc: 0.9452380952380952 - loss: 0.15509524390778245 - min_loss: 0.10634203406105878\n",
      "epoch: 45 - train_acc: 0.95 - loss: 0.11825940527126097 - min_loss: 0.10634203406105878\n",
      "epoch: 46 - train_acc: 0.9761904761904762 - loss: 0.09834160565324973 - min_loss: 0.09834160565324973\n",
      "epoch: 47 - train_acc: 0.9738095238095238 - loss: 0.08466534604809509 - min_loss: 0.08466534604809509\n",
      "epoch: 48 - train_acc: 0.9857142857142858 - loss: 0.07433076897320316 - min_loss: 0.07433076897320316\n",
      "epoch: 49 - train_acc: 0.9547619047619048 - loss: 0.09866105286895918 - min_loss: 0.07433076897320316\n",
      "epoch: 50 - train_acc: 0.95 - loss: 0.1373923411176691 - min_loss: 0.07433076897320316\n",
      "epoch: 51 - train_acc: 0.9238095238095239 - loss: 0.25968472112397495 - min_loss: 0.07433076897320316\n",
      "epoch: 52 - train_acc: 0.9261904761904762 - loss: 0.19514560781653578 - min_loss: 0.07433076897320316\n",
      "epoch: 53 - train_acc: 0.9357142857142857 - loss: 0.135029713313607 - min_loss: 0.07433076897320316\n",
      "epoch: 54 - train_acc: 0.9761904761904762 - loss: 0.08359047482621619 - min_loss: 0.07433076897320316\n",
      "epoch: 55 - train_acc: 0.9761904761904762 - loss: 0.0691967871862917 - min_loss: 0.0691967871862917\n",
      "epoch: 56 - train_acc: 0.9785714285714285 - loss: 0.13942705833425373 - min_loss: 0.0691967871862917\n",
      "epoch: 57 - train_acc: 0.9785714285714285 - loss: 0.08534438608788086 - min_loss: 0.0691967871862917\n",
      "epoch: 58 - train_acc: 0.9833333333333333 - loss: 0.07110419126910658 - min_loss: 0.0691967871862917\n",
      "epoch: 59 - train_acc: 0.9666666666666667 - loss: 0.0976935833980073 - min_loss: 0.0691967871862917\n",
      "epoch: 60 - train_acc: 0.9404761904761905 - loss: 0.15784029662564086 - min_loss: 0.0691967871862917\n",
      "epoch: 61 - train_acc: 0.95 - loss: 0.1451168584272006 - min_loss: 0.0691967871862917\n",
      "epoch: 62 - train_acc: 0.969047619047619 - loss: 0.09690114647544436 - min_loss: 0.0691967871862917\n",
      "epoch: 63 - train_acc: 0.9619047619047619 - loss: 0.11426159628614578 - min_loss: 0.0691967871862917\n",
      "epoch: 64 - train_acc: 0.9619047619047619 - loss: 0.09908074717113916 - min_loss: 0.0691967871862917\n",
      "epoch: 65 - train_acc: 0.9880952380952381 - loss: 0.0596587238823384 - min_loss: 0.0596587238823384\n",
      "epoch: 66 - train_acc: 0.9785714285714285 - loss: 0.08360472456855139 - min_loss: 0.0596587238823384\n",
      "epoch: 67 - train_acc: 0.9571428571428572 - loss: 0.08873450129932785 - min_loss: 0.0596587238823384\n",
      "epoch: 68 - train_acc: 0.9714285714285714 - loss: 0.07656957501737002 - min_loss: 0.0596587238823384\n",
      "epoch: 69 - train_acc: 0.9833333333333333 - loss: 0.056524028284919654 - min_loss: 0.056524028284919654\n",
      "epoch: 70 - train_acc: 0.9809523809523809 - loss: 0.07007518584044459 - min_loss: 0.056524028284919654\n",
      "epoch: 71 - train_acc: 0.9666666666666667 - loss: 0.10179108677913352 - min_loss: 0.056524028284919654\n",
      "epoch: 72 - train_acc: 0.95 - loss: 0.20417858290613744 - min_loss: 0.056524028284919654\n",
      "epoch: 73 - train_acc: 0.9452380952380952 - loss: 0.14269897127473255 - min_loss: 0.056524028284919654\n",
      "epoch: 74 - train_acc: 0.9761904761904762 - loss: 0.08158721172552416 - min_loss: 0.056524028284919654\n",
      "epoch: 75 - train_acc: 0.9809523809523809 - loss: 0.06276361952861192 - min_loss: 0.056524028284919654\n",
      "epoch: 76 - train_acc: 0.9857142857142858 - loss: 0.05128473909135043 - min_loss: 0.05128473909135043\n",
      "epoch: 77 - train_acc: 0.9976190476190476 - loss: 0.03334044684855591 - min_loss: 0.03334044684855591\n",
      "epoch: 78 - train_acc: 0.9761904761904762 - loss: 0.07696430052866458 - min_loss: 0.03334044684855591\n",
      "epoch: 79 - train_acc: 0.9404761904761905 - loss: 0.13708954087777253 - min_loss: 0.03334044684855591\n",
      "epoch: 80 - train_acc: 0.9785714285714285 - loss: 0.07601009114660931 - min_loss: 0.03334044684855591\n",
      "epoch: 81 - train_acc: 0.969047619047619 - loss: 0.0976892577494626 - min_loss: 0.03334044684855591\n",
      "epoch: 82 - train_acc: 0.9785714285714285 - loss: 0.06089447971402721 - min_loss: 0.03334044684855591\n",
      "epoch: 83 - train_acc: 0.9571428571428572 - loss: 0.1064918461958206 - min_loss: 0.03334044684855591\n",
      "epoch: 84 - train_acc: 0.9833333333333333 - loss: 0.11094412439435963 - min_loss: 0.03334044684855591\n",
      "epoch: 85 - train_acc: 0.9571428571428572 - loss: 0.13410151859406907 - min_loss: 0.03334044684855591\n",
      "epoch: 86 - train_acc: 0.9785714285714285 - loss: 0.07238106177047252 - min_loss: 0.03334044684855591\n",
      "epoch: 87 - train_acc: 0.9928571428571429 - loss: 0.03845467732158671 - min_loss: 0.03334044684855591\n",
      "epoch: 88 - train_acc: 0.9904761904761905 - loss: 0.050839445846888064 - min_loss: 0.03334044684855591\n",
      "epoch: 89 - train_acc: 0.9738095238095238 - loss: 0.08159018638304388 - min_loss: 0.03334044684855591\n",
      "epoch: 90 - train_acc: 0.9571428571428572 - loss: 0.10038861459721898 - min_loss: 0.03334044684855591\n",
      "epoch: 91 - train_acc: 0.9857142857142858 - loss: 0.049364505802766016 - min_loss: 0.03334044684855591\n",
      "epoch: 92 - train_acc: 0.9976190476190476 - loss: 0.024546833605020517 - min_loss: 0.024546833605020517\n",
      "epoch: 93 - train_acc: 0.9952380952380953 - loss: 0.023340919580617826 - min_loss: 0.023340919580617826\n",
      "epoch: 94 - train_acc: 0.9952380952380953 - loss: 0.029557795933266125 - min_loss: 0.023340919580617826\n",
      "epoch: 95 - train_acc: 0.9904761904761905 - loss: 0.024762567384768615 - min_loss: 0.023340919580617826\n",
      "epoch: 96 - train_acc: 0.9976190476190476 - loss: 0.027380611303390354 - min_loss: 0.023340919580617826\n",
      "epoch: 97 - train_acc: 0.9857142857142858 - loss: 0.07274592617626122 - min_loss: 0.023340919580617826\n",
      "epoch: 98 - train_acc: 0.9642857142857143 - loss: 0.09267059015978085 - min_loss: 0.023340919580617826\n",
      "epoch: 99 - train_acc: 0.9857142857142858 - loss: 0.04795038573077847 - min_loss: 0.023340919580617826\n",
      "epoch: 100 - train_acc: 0.9809523809523809 - loss: 0.05260400977585129 - min_loss: 0.023340919580617826\n",
      "epoch: 101 - train_acc: 0.9523809523809523 - loss: 0.09819945613390202 - min_loss: 0.023340919580617826\n",
      "epoch: 102 - train_acc: 0.9833333333333333 - loss: 0.04381958094846413 - min_loss: 0.023340919580617826\n",
      "epoch: 103 - train_acc: 0.9666666666666667 - loss: 0.0879271649782003 - min_loss: 0.023340919580617826\n",
      "epoch: 104 - train_acc: 0.9761904761904762 - loss: 0.06442735594076945 - min_loss: 0.023340919580617826\n",
      "epoch: 105 - train_acc: 0.9619047619047619 - loss: 0.11210479624215115 - min_loss: 0.023340919580617826\n",
      "epoch: 106 - train_acc: 0.9738095238095238 - loss: 0.07483842246966053 - min_loss: 0.023340919580617826\n",
      "epoch: 107 - train_acc: 0.9785714285714285 - loss: 0.06656442656489128 - min_loss: 0.023340919580617826\n",
      "epoch: 108 - train_acc: 0.9785714285714285 - loss: 0.0578524317383325 - min_loss: 0.023340919580617826\n",
      "epoch: 109 - train_acc: 0.9928571428571429 - loss: 0.031169017635457433 - min_loss: 0.023340919580617826\n",
      "epoch: 110 - train_acc: 0.9904761904761905 - loss: 0.029948240656727756 - min_loss: 0.023340919580617826\n",
      "epoch: 111 - train_acc: 0.9952380952380953 - loss: 0.021419377267487605 - min_loss: 0.021419377267487605\n",
      "epoch: 112 - train_acc: 0.9857142857142858 - loss: 0.03371569422299318 - min_loss: 0.021419377267487605\n",
      "epoch: 113 - train_acc: 0.9904761904761905 - loss: 0.042168597051887205 - min_loss: 0.021419377267487605\n",
      "epoch: 114 - train_acc: 0.9738095238095238 - loss: 0.060923990705091406 - min_loss: 0.021419377267487605\n",
      "epoch: 115 - train_acc: 0.9809523809523809 - loss: 0.04266953932663464 - min_loss: 0.021419377267487605\n",
      "epoch: 116 - train_acc: 0.9880952380952381 - loss: 0.038278571943498915 - min_loss: 0.021419377267487605\n",
      "epoch: 117 - train_acc: 0.9976190476190476 - loss: 0.019927551618943242 - min_loss: 0.019927551618943242\n",
      "epoch: 118 - train_acc: 0.9952380952380953 - loss: 0.025321018490725797 - min_loss: 0.019927551618943242\n",
      "epoch: 119 - train_acc: 0.9976190476190476 - loss: 0.021937446688796165 - min_loss: 0.019927551618943242\n",
      "epoch: 120 - train_acc: 1.0 - loss: 0.010231922931612535 - min_loss: 0.010231922931612535\n",
      "epoch: 121 - train_acc: 0.9880952380952381 - loss: 0.029356658064189696 - min_loss: 0.010231922931612535\n",
      "epoch: 122 - train_acc: 0.9857142857142858 - loss: 0.04119313623870932 - min_loss: 0.010231922931612535\n",
      "epoch: 123 - train_acc: 0.9928571428571429 - loss: 0.03928231108708386 - min_loss: 0.010231922931612535\n",
      "epoch: 124 - train_acc: 0.9738095238095238 - loss: 0.15073745554711848 - min_loss: 0.010231922931612535\n",
      "epoch: 125 - train_acc: 0.9619047619047619 - loss: 0.10439026104338761 - min_loss: 0.010231922931612535\n",
      "epoch: 126 - train_acc: 0.9571428571428572 - loss: 0.12315720069501485 - min_loss: 0.010231922931612535\n",
      "epoch: 127 - train_acc: 0.969047619047619 - loss: 0.10269808871673251 - min_loss: 0.010231922931612535\n",
      "epoch: 128 - train_acc: 0.9571428571428572 - loss: 0.14356177068021048 - min_loss: 0.010231922931612535\n",
      "epoch: 129 - train_acc: 0.9666666666666667 - loss: 0.13266030965589093 - min_loss: 0.010231922931612535\n",
      "epoch: 130 - train_acc: 0.9642857142857143 - loss: 0.12791795586024385 - min_loss: 0.010231922931612535\n",
      "epoch: 131 - train_acc: 0.9595238095238096 - loss: 0.10293132223170873 - min_loss: 0.010231922931612535\n",
      "epoch: 132 - train_acc: 0.9880952380952381 - loss: 0.042527832712641656 - min_loss: 0.010231922931612535\n",
      "epoch: 133 - train_acc: 0.9833333333333333 - loss: 0.04334558860890282 - min_loss: 0.010231922931612535\n",
      "epoch: 134 - train_acc: 0.9928571428571429 - loss: 0.03803483348412033 - min_loss: 0.010231922931612535\n",
      "epoch: 135 - train_acc: 0.9738095238095238 - loss: 0.061831430052618874 - min_loss: 0.010231922931612535\n",
      "epoch: 136 - train_acc: 0.9857142857142858 - loss: 0.041350747489601804 - min_loss: 0.010231922931612535\n",
      "epoch: 137 - train_acc: 0.9928571428571429 - loss: 0.027250104419169013 - min_loss: 0.010231922931612535\n",
      "epoch: 138 - train_acc: 0.9976190476190476 - loss: 0.020393367110873757 - min_loss: 0.010231922931612535\n",
      "epoch: 139 - train_acc: 0.9952380952380953 - loss: 0.015028116630126069 - min_loss: 0.010231922931612535\n",
      "epoch: 140 - train_acc: 0.9928571428571429 - loss: 0.06664271077624179 - min_loss: 0.010231922931612535\n",
      "epoch: 141 - train_acc: 0.930952380952381 - loss: 0.22128023748143288 - min_loss: 0.010231922931612535\n",
      "epoch: 142 - train_acc: 0.95 - loss: 0.12085771542655149 - min_loss: 0.010231922931612535\n",
      "epoch: 143 - train_acc: 0.9809523809523809 - loss: 0.05463948211254087 - min_loss: 0.010231922931612535\n",
      "epoch: 144 - train_acc: 0.9904761904761905 - loss: 0.038258228746679016 - min_loss: 0.010231922931612535\n",
      "epoch: 145 - train_acc: 0.9809523809523809 - loss: 0.03822931546890435 - min_loss: 0.010231922931612535\n",
      "epoch: 146 - train_acc: 0.9880952380952381 - loss: 0.041891206128213135 - min_loss: 0.010231922931612535\n",
      "epoch: 147 - train_acc: 0.9904761904761905 - loss: 0.04833886073851376 - min_loss: 0.010231922931612535\n",
      "epoch: 148 - train_acc: 0.9857142857142858 - loss: 0.041507807418732036 - min_loss: 0.010231922931612535\n",
      "epoch: 149 - train_acc: 0.9880952380952381 - loss: 0.044603256146479434 - min_loss: 0.010231922931612535\n",
      "epoch: 150 - train_acc: 0.9809523809523809 - loss: 0.051620099186958604 - min_loss: 0.010231922931612535\n",
      "epoch: 151 - train_acc: 0.9809523809523809 - loss: 0.03970200635621385 - min_loss: 0.010231922931612535\n",
      "epoch: 152 - train_acc: 0.9952380952380953 - loss: 0.01889433761591949 - min_loss: 0.010231922931612535\n",
      "epoch: 153 - train_acc: 0.9976190476190476 - loss: 0.015611534990283725 - min_loss: 0.010231922931612535\n",
      "epoch: 154 - train_acc: 0.9952380952380953 - loss: 0.0472782239460766 - min_loss: 0.010231922931612535\n",
      "epoch: 155 - train_acc: 0.9619047619047619 - loss: 0.11652088986194223 - min_loss: 0.010231922931612535\n",
      "epoch: 156 - train_acc: 0.9809523809523809 - loss: 0.07904849939167115 - min_loss: 0.010231922931612535\n",
      "epoch: 157 - train_acc: 0.9928571428571429 - loss: 0.04368003037651691 - min_loss: 0.010231922931612535\n",
      "epoch: 158 - train_acc: 0.9809523809523809 - loss: 0.11515117558615151 - min_loss: 0.010231922931612535\n",
      "epoch: 159 - train_acc: 0.9880952380952381 - loss: 0.04577383654561292 - min_loss: 0.010231922931612535\n",
      "epoch: 160 - train_acc: 0.9952380952380953 - loss: 0.022836965198289594 - min_loss: 0.010231922931612535\n",
      "epoch: 161 - train_acc: 0.9952380952380953 - loss: 0.01311467282692487 - min_loss: 0.010231922931612535\n",
      "epoch: 162 - train_acc: 0.9809523809523809 - loss: 0.035441002915557074 - min_loss: 0.010231922931612535\n",
      "epoch: 163 - train_acc: 0.9952380952380953 - loss: 0.029755560482445337 - min_loss: 0.010231922931612535\n",
      "epoch: 164 - train_acc: 0.9761904761904762 - loss: 0.08578407227346509 - min_loss: 0.010231922931612535\n",
      "epoch: 165 - train_acc: 0.9785714285714285 - loss: 0.06190752842873324 - min_loss: 0.010231922931612535\n",
      "epoch: 166 - train_acc: 0.9880952380952381 - loss: 0.04796420260093966 - min_loss: 0.010231922931612535\n",
      "epoch: 167 - train_acc: 0.9928571428571429 - loss: 0.026186926219804472 - min_loss: 0.010231922931612535\n",
      "epoch: 168 - train_acc: 0.9928571428571429 - loss: 0.027286717667044446 - min_loss: 0.010231922931612535\n",
      "epoch: 169 - train_acc: 0.9952380952380953 - loss: 0.02147582887388612 - min_loss: 0.010231922931612535\n",
      "epoch: 170 - train_acc: 0.9976190476190476 - loss: 0.021202758507173767 - min_loss: 0.010231922931612535\n",
      "epoch: 171 - train_acc: 0.9857142857142858 - loss: 0.04412264125536288 - min_loss: 0.010231922931612535\n",
      "epoch: 172 - train_acc: 0.9952380952380953 - loss: 0.0228912592510633 - min_loss: 0.010231922931612535\n",
      "epoch: 173 - train_acc: 0.9928571428571429 - loss: 0.027477551610980533 - min_loss: 0.010231922931612535\n",
      "epoch: 174 - train_acc: 0.9857142857142858 - loss: 0.03844160579368881 - min_loss: 0.010231922931612535\n",
      "epoch: 175 - train_acc: 0.9952380952380953 - loss: 0.020207579571250033 - min_loss: 0.010231922931612535\n",
      "epoch: 176 - train_acc: 0.9928571428571429 - loss: 0.05804188600816401 - min_loss: 0.010231922931612535\n",
      "epoch: 177 - train_acc: 0.9880952380952381 - loss: 0.048443224001877706 - min_loss: 0.010231922931612535\n",
      "epoch: 178 - train_acc: 0.9738095238095238 - loss: 0.0634153754758913 - min_loss: 0.010231922931612535\n",
      "epoch: 179 - train_acc: 0.9880952380952381 - loss: 0.03860790092140036 - min_loss: 0.010231922931612535\n",
      "epoch: 180 - train_acc: 0.9904761904761905 - loss: 0.03445391485305496 - min_loss: 0.010231922931612535\n",
      "epoch: 181 - train_acc: 0.9809523809523809 - loss: 0.047127275096165974 - min_loss: 0.010231922931612535\n",
      "epoch: 182 - train_acc: 0.9857142857142858 - loss: 0.0319643453493614 - min_loss: 0.010231922931612535\n",
      "epoch: 183 - train_acc: 0.9904761904761905 - loss: 0.034752128396330934 - min_loss: 0.010231922931612535\n",
      "epoch: 184 - train_acc: 0.9928571428571429 - loss: 0.028958787253105064 - min_loss: 0.010231922931612535\n",
      "epoch: 185 - train_acc: 1.0 - loss: 0.011894524609536697 - min_loss: 0.010231922931612535\n",
      "epoch: 186 - train_acc: 0.9904761904761905 - loss: 0.031335260375321285 - min_loss: 0.010231922931612535\n",
      "epoch: 187 - train_acc: 0.9928571428571429 - loss: 0.0186244108790473 - min_loss: 0.010231922931612535\n",
      "epoch: 188 - train_acc: 1.0 - loss: 0.008144347564705437 - min_loss: 0.008144347564705437\n",
      "epoch: 189 - train_acc: 0.9952380952380953 - loss: 0.017377174796782097 - min_loss: 0.008144347564705437\n",
      "epoch: 190 - train_acc: 0.9833333333333333 - loss: 0.03981990676200424 - min_loss: 0.008144347564705437\n",
      "epoch: 191 - train_acc: 1.0 - loss: 0.00684320232793467 - min_loss: 0.00684320232793467\n",
      "epoch: 192 - train_acc: 0.9928571428571429 - loss: 0.07047467709022866 - min_loss: 0.00684320232793467\n",
      "epoch: 193 - train_acc: 0.9595238095238096 - loss: 0.11307535355183147 - min_loss: 0.00684320232793467\n",
      "epoch: 194 - train_acc: 0.9761904761904762 - loss: 0.06717839215611626 - min_loss: 0.00684320232793467\n",
      "epoch: 195 - train_acc: 0.9928571428571429 - loss: 0.02554725671249154 - min_loss: 0.00684320232793467\n",
      "epoch: 196 - train_acc: 0.9976190476190476 - loss: 0.011423756183612202 - min_loss: 0.00684320232793467\n",
      "epoch: 197 - train_acc: 0.9976190476190476 - loss: 0.01314528002741398 - min_loss: 0.00684320232793467\n",
      "epoch: 198 - train_acc: 0.9976190476190476 - loss: 0.01758217364620955 - min_loss: 0.00684320232793467\n",
      "epoch: 199 - train_acc: 0.9880952380952381 - loss: 0.02878099563237089 - min_loss: 0.00684320232793467\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "tb = SummaryWriter()\n",
    "\n",
    "def get_num_correct(preds_logits, labels):\n",
    "    return (preds_logits.argmax(dim=1) == labels).sum().item()\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "data = EyeBlinkDataset(train=True)\n",
    "data_loader = torch.utils.data.DataLoader(data, batch_size=13, shuffle=True)\n",
    "\n",
    "print(len(data))\n",
    "\n",
    "# model = P2B2().double()\n",
    "model = P2B2_SeparableConv().double()\n",
    "# model = P3B3().double()\n",
    "# model = P3B3_SeparableConv().double()\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "# loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters()) #, lr=0.003)\n",
    "\n",
    "min_loss = float('inf')\n",
    "num_epochs = 200\n",
    "\n",
    "EYE = 'left'\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    avg_loss = 0\n",
    "    total_correct = 0\n",
    "\n",
    "    for seqs, labels in data_loader:\n",
    "        seqs = seqs.permute((0,4,3,1,2)).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        preds_logits = model(seqs)\n",
    "        loss = F.cross_entropy(preds_logits, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_loss += loss.detach().item() / len(data_loader)\n",
    "        total_correct += get_num_correct(preds_logits, labels)\n",
    "\n",
    "    if avg_loss < min_loss:\n",
    "        min_loss = avg_loss\n",
    "        # torch.save(model, f'checkpoints/{EYE}/P2B2-num_epochs_{num_epochs}.pt')\n",
    "        torch.save(model, f'checkpoints/{EYE}/P2B2_SeparableConv-num_epochs_{num_epochs}.pt')\n",
    "        # torch.save(model, f'checkpoints/{EYE}/P3B3-num_epochs_{num_epochs}.pt')\n",
    "        # torch.save(model, f'checkpoints/{EYE}/P3B3_SeparableConv-num_epochs_{num_epochs}.pt')\n",
    "\n",
    "    tb.add_scalar('Loss', avg_loss, epoch)\n",
    "    tb.add_scalar('Number correct', total_correct, epoch)\n",
    "    tb.add_scalar('Accuracy', total_correct/len(data), epoch)\n",
    "    print(f'epoch: {epoch} - train_acc: {total_correct/len(data)} - loss: {avg_loss} - min_loss: {min_loss}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training - k fold cross validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "tb = SummaryWriter()\n",
    "\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def fixed_seed(seed_value):\n",
    "    # 1. Set `PYTHONHASHSEED` environment variable at a fixed value\n",
    "    os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "    \n",
    "    # 2. Set `python` built-in pseudo-random generator at a fixed value\n",
    "    random.seed(seed_value)\n",
    "\n",
    "    # 3. Set `numpy` pseudo-random generator at a fixed value\n",
    "    np.random.seed(seed_value)\n",
    "\n",
    "    # 4. Set `pytorch` pseudo-random generator at a fixed value\n",
    "    torch.manual_seed(seed_value)\n",
    "\n",
    "\n",
    "def reset_weights(model):\n",
    "    for layer in model.children():\n",
    "        if hasattr(layer, 'reset_parameters'):\n",
    "            print(f'Reset trainable parameters of layer = {layer}')\n",
    "            layer.reset_parameters()\n",
    "\n",
    "def get_num_correct(preds_logits, labels):\n",
    "    return (preds_logits.argmax(dim=1) == labels).sum().item()\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "# fixed_seed(42)\n",
    "\n",
    "\n",
    "data = EyeBlinkDataset(train=True)\n",
    "# data_loader = torch.utils.data.DataLoader(data, batch_size=16, shuffle=True)\n",
    "\n",
    "print(len(data))\n",
    "\n",
    "\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "for fold, (train_ids, val_ids) in enumerate(kfold.split(data)):\n",
    "    print(f'{fold}: ')\n",
    "    print(f'train_ids: {train_ids}')\n",
    "    print(f'val_ids: {val_ids}')\n",
    "    train_sampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
    "    val_sampler = torch.utils.data.SubsetRandomSampler(val_ids)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(data, batch_size=16, sampler= train_sampler)\n",
    "    val_loader = torch.utils.data.DataLoader(data, batch_size=16, sampler= val_sampler)\n",
    "\n",
    "\n",
    "model = P2B2().double()\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "# loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "min_loss = float('inf')\n",
    "num_epochs = 500\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "\n",
    "    for seqs, labels in data_loader:\n",
    "        seqs = seqs.permute((0,4,3,1,2)).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        preds_logits = model(seqs)\n",
    "        loss = F.cross_entropy(preds_logits, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.detach().item()\n",
    "        total_correct += get_num_correct(preds_logits, labels)\n",
    "\n",
    "    if total_loss < min_loss:\n",
    "        min_loss = total_loss\n",
    "        torch.save(model, f'checkpoints/P2B2-num_epochs_{num_epochs}.pt')\n",
    "\n",
    "    tb.add_scalar('Loss', total_loss, epoch)\n",
    "    tb.add_scalar('Number correct', total_correct, epoch)\n",
    "    tb.add_scalar('Accuracy', total_correct/len(data), epoch)\n",
    "    print(f'epoch: {epoch} - train_acc: {total_correct/len(data)} - loss: {total_loss} - min_loss: {min_loss}')\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training - EarlyStopping + k-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['datasets/train/training/blink/147', 'datasets/train/training/blink/223', 'datasets/train/training/unblink/54', 'datasets/train/training/unblink/57', 'datasets/train/training/unblink/59', 'datasets/train/training/unblink/99', 'datasets/train/training/unblink/100']\n",
      "['datasets/test/test/blink/105']\n",
      "\n",
      "[Fold 0]: \n",
      "len(train_sampler)=411\n",
      "len(val_sampler)=30\n",
      "[  9  30  39  73  75  77  78 116 117 124 131 137 152 155 198 204 237 246\n",
      " 278 280 281 362 383 388 394 396 398 406 436 439]\n",
      "Validation loss decreased (inf --> 0.649581).  Saving model ...\n",
      "epoch: 0 - train_acc: 0.5936739659367397 - train_loss: 0.6938355466396197 - val_acc: 0.6 - val_loss: 0.6495814942339381\n",
      "Validation loss decreased (0.649581 --> 0.534563).  Saving model ...\n",
      "epoch: 1 - train_acc: 0.6520681265206812 - train_loss: 0.6185480922320316 - val_acc: 0.7666666666666667 - val_loss: 0.5345626007442147\n",
      "Validation loss decreased (0.534563 --> 0.480625).  Saving model ...\n",
      "epoch: 2 - train_acc: 0.6715328467153284 - train_loss: 0.601223735127226 - val_acc: 0.7333333333333333 - val_loss: 0.4806248026770621\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 3 - train_acc: 0.7347931873479319 - train_loss: 0.5539863458651078 - val_acc: 0.7 - val_loss: 0.5668887062232534\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 4 - train_acc: 0.7372262773722628 - train_loss: 0.52560676515452 - val_acc: 0.7 - val_loss: 0.6314913263557049\n",
      "EarlyStopping counter: 3 out of 50\n",
      "epoch: 5 - train_acc: 0.7493917274939172 - train_loss: 0.49284167037510146 - val_acc: 0.7333333333333333 - val_loss: 0.5909664314028021\n",
      "Validation loss decreased (0.480625 --> 0.390504).  Saving model ...\n",
      "epoch: 6 - train_acc: 0.8029197080291971 - train_loss: 0.42163542841374013 - val_acc: 0.8 - val_loss: 0.3905035810266924\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 7 - train_acc: 0.8150851581508516 - train_loss: 0.40185734923172756 - val_acc: 0.7666666666666667 - val_loss: 0.4918523884175514\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 8 - train_acc: 0.8199513381995134 - train_loss: 0.40288080666453324 - val_acc: 0.7666666666666667 - val_loss: 0.595196876993332\n",
      "EarlyStopping counter: 3 out of 50\n",
      "epoch: 9 - train_acc: 0.829683698296837 - train_loss: 0.35490811008452416 - val_acc: 0.7333333333333333 - val_loss: 0.5309007908609094\n",
      "EarlyStopping counter: 4 out of 50\n",
      "epoch: 10 - train_acc: 0.8272506082725061 - train_loss: 0.35854428162756224 - val_acc: 0.7333333333333333 - val_loss: 0.759758157702455\n",
      "EarlyStopping counter: 5 out of 50\n",
      "epoch: 11 - train_acc: 0.8394160583941606 - train_loss: 0.35477614734640595 - val_acc: 0.7333333333333333 - val_loss: 0.5243483856361015\n",
      "EarlyStopping counter: 6 out of 50\n",
      "epoch: 12 - train_acc: 0.8491484184914841 - train_loss: 0.3179099558356842 - val_acc: 0.8333333333333334 - val_loss: 0.4583212314186434\n",
      "EarlyStopping counter: 7 out of 50\n",
      "epoch: 13 - train_acc: 0.8759124087591241 - train_loss: 0.3441750275896004 - val_acc: 0.8 - val_loss: 0.4076021132676289\n",
      "EarlyStopping counter: 8 out of 50\n",
      "epoch: 14 - train_acc: 0.8588807785888077 - train_loss: 0.318701929768817 - val_acc: 0.8333333333333334 - val_loss: 0.7197328492785786\n",
      "EarlyStopping counter: 9 out of 50\n",
      "epoch: 15 - train_acc: 0.8637469586374696 - train_loss: 0.28198694368494526 - val_acc: 0.8 - val_loss: 0.5178212560953552\n",
      "Validation loss decreased (0.390504 --> 0.382642).  Saving model ...\n",
      "epoch: 16 - train_acc: 0.8442822384428224 - train_loss: 0.3550649352465608 - val_acc: 0.8 - val_loss: 0.38264153467189144\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 17 - train_acc: 0.8759124087591241 - train_loss: 0.2993178892872395 - val_acc: 0.7666666666666667 - val_loss: 0.5710974556080141\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 18 - train_acc: 0.851581508515815 - train_loss: 0.3091960996012802 - val_acc: 0.7333333333333333 - val_loss: 0.48201259550844877\n",
      "EarlyStopping counter: 3 out of 50\n",
      "epoch: 19 - train_acc: 0.8905109489051095 - train_loss: 0.2693591238901384 - val_acc: 0.8 - val_loss: 0.6407096788306118\n",
      "EarlyStopping counter: 4 out of 50\n",
      "epoch: 20 - train_acc: 0.8856447688564477 - train_loss: 0.2636687673527455 - val_acc: 0.7333333333333333 - val_loss: 0.5304786817043164\n",
      "EarlyStopping counter: 5 out of 50\n",
      "epoch: 21 - train_acc: 0.878345498783455 - train_loss: 0.270852160749914 - val_acc: 0.8 - val_loss: 0.5607288697650712\n",
      "EarlyStopping counter: 6 out of 50\n",
      "epoch: 22 - train_acc: 0.8734793187347932 - train_loss: 0.27444755779982105 - val_acc: 0.8 - val_loss: 0.5218320236855152\n",
      "EarlyStopping counter: 7 out of 50\n",
      "epoch: 23 - train_acc: 0.9051094890510949 - train_loss: 0.21761441704532183 - val_acc: 0.8 - val_loss: 0.41066832251691626\n",
      "EarlyStopping counter: 8 out of 50\n",
      "epoch: 24 - train_acc: 0.8807785888077859 - train_loss: 0.24909764539720453 - val_acc: 0.8666666666666667 - val_loss: 0.7532331474359248\n",
      "EarlyStopping counter: 9 out of 50\n",
      "epoch: 25 - train_acc: 0.9075425790754258 - train_loss: 0.2339792879899403 - val_acc: 0.7333333333333333 - val_loss: 0.7510070255502447\n",
      "EarlyStopping counter: 10 out of 50\n",
      "epoch: 26 - train_acc: 0.8856447688564477 - train_loss: 0.2575309246102101 - val_acc: 0.8 - val_loss: 0.5613466727198354\n",
      "EarlyStopping counter: 11 out of 50\n",
      "epoch: 27 - train_acc: 0.9294403892944039 - train_loss: 0.16837083797019906 - val_acc: 0.8 - val_loss: 0.6821269645434316\n",
      "EarlyStopping counter: 12 out of 50\n",
      "epoch: 28 - train_acc: 0.8953771289537713 - train_loss: 0.22107553132840385 - val_acc: 0.8 - val_loss: 0.7370393905639392\n",
      "EarlyStopping counter: 13 out of 50\n",
      "epoch: 29 - train_acc: 0.9124087591240876 - train_loss: 0.19807973573212342 - val_acc: 0.8333333333333334 - val_loss: 0.6599104429147021\n",
      "EarlyStopping counter: 14 out of 50\n",
      "epoch: 30 - train_acc: 0.9075425790754258 - train_loss: 0.2070107270598884 - val_acc: 0.8 - val_loss: 0.5224978670364336\n",
      "EarlyStopping counter: 15 out of 50\n",
      "epoch: 31 - train_acc: 0.9367396593673966 - train_loss: 0.1614561305831009 - val_acc: 0.8666666666666667 - val_loss: 0.6386568022057534\n",
      "EarlyStopping counter: 16 out of 50\n",
      "epoch: 32 - train_acc: 0.948905109489051 - train_loss: 0.15925760395250385 - val_acc: 0.8666666666666667 - val_loss: 0.47242914295231686\n",
      "EarlyStopping counter: 17 out of 50\n",
      "epoch: 33 - train_acc: 0.9099756690997567 - train_loss: 0.20032229465660834 - val_acc: 0.8 - val_loss: 0.8338496836562399\n",
      "EarlyStopping counter: 18 out of 50\n",
      "epoch: 34 - train_acc: 0.9172749391727494 - train_loss: 0.19989390361184045 - val_acc: 0.8666666666666667 - val_loss: 0.8055093617151176\n",
      "EarlyStopping counter: 19 out of 50\n",
      "epoch: 35 - train_acc: 0.9367396593673966 - train_loss: 0.14840679916631214 - val_acc: 0.7333333333333333 - val_loss: 0.9967943734211376\n",
      "EarlyStopping counter: 20 out of 50\n",
      "epoch: 36 - train_acc: 0.9416058394160584 - train_loss: 0.15968490721670914 - val_acc: 0.8 - val_loss: 0.8569965754498075\n",
      "EarlyStopping counter: 21 out of 50\n",
      "epoch: 37 - train_acc: 0.9367396593673966 - train_loss: 0.15235258081633077 - val_acc: 0.7666666666666667 - val_loss: 1.068392671036829\n",
      "EarlyStopping counter: 22 out of 50\n",
      "epoch: 38 - train_acc: 0.9562043795620438 - train_loss: 0.12851783595475164 - val_acc: 0.8 - val_loss: 1.3133736779759373\n",
      "EarlyStopping counter: 23 out of 50\n",
      "epoch: 39 - train_acc: 0.9343065693430657 - train_loss: 0.16540008544742757 - val_acc: 0.7666666666666667 - val_loss: 2.0371256689227337\n",
      "EarlyStopping counter: 24 out of 50\n",
      "epoch: 40 - train_acc: 0.9416058394160584 - train_loss: 0.1512368329413372 - val_acc: 0.6666666666666666 - val_loss: 1.6574624530451652\n",
      "EarlyStopping counter: 25 out of 50\n",
      "epoch: 41 - train_acc: 0.9221411192214112 - train_loss: 0.17292946539741055 - val_acc: 0.8333333333333334 - val_loss: 0.7555111378808711\n",
      "EarlyStopping counter: 26 out of 50\n",
      "epoch: 42 - train_acc: 0.9586374695863747 - train_loss: 0.129357472429111 - val_acc: 0.8666666666666667 - val_loss: 0.6810001567897269\n",
      "EarlyStopping counter: 27 out of 50\n",
      "epoch: 43 - train_acc: 0.9586374695863747 - train_loss: 0.12846408514566487 - val_acc: 0.8333333333333334 - val_loss: 0.7824544287018249\n",
      "EarlyStopping counter: 28 out of 50\n",
      "epoch: 44 - train_acc: 0.9513381995133819 - train_loss: 0.13151348015644898 - val_acc: 0.8333333333333334 - val_loss: 1.004804075770516\n",
      "EarlyStopping counter: 29 out of 50\n",
      "epoch: 45 - train_acc: 0.9464720194647201 - train_loss: 0.14908451357525318 - val_acc: 0.8 - val_loss: 1.0567033737146234\n",
      "EarlyStopping counter: 30 out of 50\n",
      "epoch: 46 - train_acc: 0.9197080291970803 - train_loss: 0.21092815368061513 - val_acc: 0.8 - val_loss: 1.1600250719365852\n",
      "EarlyStopping counter: 31 out of 50\n",
      "epoch: 47 - train_acc: 0.9197080291970803 - train_loss: 0.17466245144041845 - val_acc: 0.8 - val_loss: 0.931226999737902\n",
      "EarlyStopping counter: 32 out of 50\n",
      "epoch: 48 - train_acc: 0.9635036496350365 - train_loss: 0.13952966201311934 - val_acc: 0.8333333333333334 - val_loss: 1.3508053027457958\n",
      "EarlyStopping counter: 33 out of 50\n",
      "epoch: 49 - train_acc: 0.9537712895377128 - train_loss: 0.14253097177385518 - val_acc: 0.8 - val_loss: 1.0394985248598008\n",
      "EarlyStopping counter: 34 out of 50\n",
      "epoch: 50 - train_acc: 0.9440389294403893 - train_loss: 0.1276089506227432 - val_acc: 0.8 - val_loss: 0.754961309517124\n",
      "EarlyStopping counter: 35 out of 50\n",
      "epoch: 51 - train_acc: 0.9513381995133819 - train_loss: 0.14653475868656074 - val_acc: 0.7333333333333333 - val_loss: 1.0464221588013105\n",
      "EarlyStopping counter: 36 out of 50\n",
      "epoch: 52 - train_acc: 0.9440389294403893 - train_loss: 0.15120309916512786 - val_acc: 0.7666666666666667 - val_loss: 1.108837008085072\n",
      "EarlyStopping counter: 37 out of 50\n",
      "epoch: 53 - train_acc: 0.9537712895377128 - train_loss: 0.132950215442815 - val_acc: 0.7666666666666667 - val_loss: 1.0871956922517065\n",
      "EarlyStopping counter: 38 out of 50\n",
      "epoch: 54 - train_acc: 0.9464720194647201 - train_loss: 0.14764899965095346 - val_acc: 0.8333333333333334 - val_loss: 0.7029597738519404\n",
      "EarlyStopping counter: 39 out of 50\n",
      "epoch: 55 - train_acc: 0.9708029197080292 - train_loss: 0.1005173089917153 - val_acc: 0.8 - val_loss: 0.5162509666755872\n",
      "EarlyStopping counter: 40 out of 50\n",
      "epoch: 56 - train_acc: 0.9805352798053528 - train_loss: 0.09028114738807656 - val_acc: 0.8333333333333334 - val_loss: 1.6808600830157592\n",
      "EarlyStopping counter: 41 out of 50\n",
      "epoch: 57 - train_acc: 0.9708029197080292 - train_loss: 0.07474615815251942 - val_acc: 0.8666666666666667 - val_loss: 0.5585215941901832\n",
      "EarlyStopping counter: 42 out of 50\n",
      "epoch: 58 - train_acc: 0.9708029197080292 - train_loss: 0.07840316020890242 - val_acc: 0.7666666666666667 - val_loss: 0.719487786728174\n",
      "EarlyStopping counter: 43 out of 50\n",
      "epoch: 59 - train_acc: 0.9708029197080292 - train_loss: 0.08048871662868663 - val_acc: 0.8 - val_loss: 1.3464623209201794\n",
      "EarlyStopping counter: 44 out of 50\n",
      "epoch: 60 - train_acc: 0.9683698296836983 - train_loss: 0.08749487147917408 - val_acc: 0.7333333333333333 - val_loss: 1.0567121772368258\n",
      "EarlyStopping counter: 45 out of 50\n",
      "epoch: 61 - train_acc: 0.9659367396593674 - train_loss: 0.08539474595476208 - val_acc: 0.7666666666666667 - val_loss: 1.0556764772661476\n",
      "EarlyStopping counter: 46 out of 50\n",
      "epoch: 62 - train_acc: 0.9781021897810219 - train_loss: 0.06585400893356641 - val_acc: 0.8333333333333334 - val_loss: 1.5260934570713656\n",
      "EarlyStopping counter: 47 out of 50\n",
      "epoch: 63 - train_acc: 0.9683698296836983 - train_loss: 0.0762395417073813 - val_acc: 0.7666666666666667 - val_loss: 1.2128160040154465\n",
      "EarlyStopping counter: 48 out of 50\n",
      "epoch: 64 - train_acc: 0.9805352798053528 - train_loss: 0.06986652092093394 - val_acc: 0.7333333333333333 - val_loss: 1.412490123614413\n",
      "EarlyStopping counter: 49 out of 50\n",
      "epoch: 65 - train_acc: 0.9781021897810219 - train_loss: 0.06357796553365822 - val_acc: 0.8 - val_loss: 0.7889893242628012\n",
      "EarlyStopping counter: 50 out of 50\n",
      "epoch: 66 - train_acc: 0.9805352798053528 - train_loss: 0.07044979378095101 - val_acc: 0.8333333333333334 - val_loss: 1.441521069846602\n",
      "=> Early stopped !!\n",
      "Accuracy: 0.7321428571428571\n",
      "Precision: 0.7357142857142858\n",
      "Recall: 0.8174603174603174\n",
      "F1: 0.774436090225564\n",
      "\n",
      "[Fold 1]: \n",
      "len(train_sampler)=411\n",
      "len(val_sampler)=30\n",
      "[  0  15  19  33  55  70  72  76  79  93 101 113 132 153 154 165 172 220\n",
      " 255 272 291 305 307 310 340 368 391 416 419 431]\n",
      "Validation loss decreased (inf --> 0.705791).  Saving model ...\n",
      "epoch: 0 - train_acc: 0.5985401459854015 - train_loss: 0.6661367199169583 - val_acc: 0.6 - val_loss: 0.7057905630567298\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 1 - train_acc: 0.6618004866180048 - train_loss: 0.615750978634069 - val_acc: 0.43333333333333335 - val_loss: 0.7581379014662015\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 2 - train_acc: 0.6690997566909975 - train_loss: 0.5901687269646004 - val_acc: 0.6333333333333333 - val_loss: 1.0806374879745617\n",
      "EarlyStopping counter: 3 out of 50\n",
      "epoch: 3 - train_acc: 0.6593673965936739 - train_loss: 0.630314954679758 - val_acc: 0.7333333333333333 - val_loss: 0.7309687242197465\n",
      "Validation loss decreased (0.705791 --> 0.588339).  Saving model ...\n",
      "epoch: 4 - train_acc: 0.7055961070559611 - train_loss: 0.5694662529373807 - val_acc: 0.6333333333333333 - val_loss: 0.5883392586231103\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 5 - train_acc: 0.7274939172749392 - train_loss: 0.5168316629150452 - val_acc: 0.6333333333333333 - val_loss: 0.6168902180235675\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 6 - train_acc: 0.7469586374695864 - train_loss: 0.47753366061003955 - val_acc: 0.7 - val_loss: 0.7124657742445586\n",
      "EarlyStopping counter: 3 out of 50\n",
      "epoch: 7 - train_acc: 0.7761557177615572 - train_loss: 0.44084439691471566 - val_acc: 0.7333333333333333 - val_loss: 0.7492404693510197\n",
      "Validation loss decreased (0.588339 --> 0.544456).  Saving model ...\n",
      "epoch: 8 - train_acc: 0.7591240875912408 - train_loss: 0.44872231397725026 - val_acc: 0.8333333333333334 - val_loss: 0.5444563716927941\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 9 - train_acc: 0.8029197080291971 - train_loss: 0.42929885759484937 - val_acc: 0.5666666666666667 - val_loss: 0.770538768754002\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 10 - train_acc: 0.829683698296837 - train_loss: 0.39253151645902135 - val_acc: 0.7 - val_loss: 0.6331958118415479\n",
      "EarlyStopping counter: 3 out of 50\n",
      "epoch: 11 - train_acc: 0.8272506082725061 - train_loss: 0.3585277643666958 - val_acc: 0.6333333333333333 - val_loss: 0.8592078599392066\n",
      "EarlyStopping counter: 4 out of 50\n",
      "epoch: 12 - train_acc: 0.805352798053528 - train_loss: 0.39530297738876563 - val_acc: 0.6666666666666666 - val_loss: 0.8439054450205252\n",
      "EarlyStopping counter: 5 out of 50\n",
      "epoch: 13 - train_acc: 0.805352798053528 - train_loss: 0.3830127844990723 - val_acc: 0.5666666666666667 - val_loss: 0.8722271605275331\n",
      "EarlyStopping counter: 6 out of 50\n",
      "epoch: 14 - train_acc: 0.8345498783454988 - train_loss: 0.3406719596421469 - val_acc: 0.7333333333333333 - val_loss: 0.6450052992815187\n",
      "EarlyStopping counter: 7 out of 50\n",
      "epoch: 15 - train_acc: 0.8491484184914841 - train_loss: 0.3093610889924598 - val_acc: 0.8 - val_loss: 0.6362590252830096\n",
      "EarlyStopping counter: 8 out of 50\n",
      "epoch: 16 - train_acc: 0.8637469586374696 - train_loss: 0.3043874339245148 - val_acc: 0.7333333333333333 - val_loss: 0.8420349899613757\n",
      "EarlyStopping counter: 9 out of 50\n",
      "epoch: 17 - train_acc: 0.878345498783455 - train_loss: 0.27760264320313266 - val_acc: 0.7333333333333333 - val_loss: 0.7731176553217365\n",
      "EarlyStopping counter: 10 out of 50\n",
      "epoch: 18 - train_acc: 0.8734793187347932 - train_loss: 0.2954335845223377 - val_acc: 0.8333333333333334 - val_loss: 0.6790557696441573\n",
      "EarlyStopping counter: 11 out of 50\n",
      "epoch: 19 - train_acc: 0.8807785888077859 - train_loss: 0.26966039023586497 - val_acc: 0.7 - val_loss: 0.6543874026080566\n",
      "EarlyStopping counter: 12 out of 50\n",
      "epoch: 20 - train_acc: 0.8661800486618005 - train_loss: 0.2795704596376818 - val_acc: 0.5666666666666667 - val_loss: 0.8610110639883011\n",
      "EarlyStopping counter: 13 out of 50\n",
      "epoch: 21 - train_acc: 0.8686131386861314 - train_loss: 0.28070935554176274 - val_acc: 0.7666666666666667 - val_loss: 0.7767297663677973\n",
      "EarlyStopping counter: 14 out of 50\n",
      "epoch: 22 - train_acc: 0.8880778588807786 - train_loss: 0.24901712105838422 - val_acc: 0.6333333333333333 - val_loss: 0.7209035590126832\n",
      "EarlyStopping counter: 15 out of 50\n",
      "epoch: 23 - train_acc: 0.8661800486618005 - train_loss: 0.2868448151213399 - val_acc: 0.4666666666666667 - val_loss: 1.528996313634258\n",
      "EarlyStopping counter: 16 out of 50\n",
      "epoch: 24 - train_acc: 0.8807785888077859 - train_loss: 0.2491597221320209 - val_acc: 0.8 - val_loss: 0.6719154554601622\n",
      "EarlyStopping counter: 17 out of 50\n",
      "epoch: 25 - train_acc: 0.878345498783455 - train_loss: 0.25008802894394105 - val_acc: 0.7666666666666667 - val_loss: 0.8474342981393989\n",
      "EarlyStopping counter: 18 out of 50\n",
      "epoch: 26 - train_acc: 0.9051094890510949 - train_loss: 0.22289685430228992 - val_acc: 0.7333333333333333 - val_loss: 0.750974392561119\n",
      "EarlyStopping counter: 19 out of 50\n",
      "epoch: 27 - train_acc: 0.9197080291970803 - train_loss: 0.2032296354627043 - val_acc: 0.8 - val_loss: 1.3280296885264247\n",
      "EarlyStopping counter: 20 out of 50\n",
      "epoch: 28 - train_acc: 0.902676399026764 - train_loss: 0.24656952590115602 - val_acc: 0.6666666666666666 - val_loss: 1.224793437305646\n",
      "EarlyStopping counter: 21 out of 50\n",
      "epoch: 29 - train_acc: 0.9075425790754258 - train_loss: 0.20519608831813882 - val_acc: 0.6666666666666666 - val_loss: 1.258576938329813\n",
      "EarlyStopping counter: 22 out of 50\n",
      "epoch: 30 - train_acc: 0.9172749391727494 - train_loss: 0.19905473624455278 - val_acc: 0.7333333333333333 - val_loss: 0.8495050756248366\n",
      "EarlyStopping counter: 23 out of 50\n",
      "epoch: 31 - train_acc: 0.9245742092457421 - train_loss: 0.20382222951025322 - val_acc: 0.8 - val_loss: 0.8685542830026737\n",
      "EarlyStopping counter: 24 out of 50\n",
      "epoch: 32 - train_acc: 0.8807785888077859 - train_loss: 0.2501782879347673 - val_acc: 0.6 - val_loss: 0.674020568498887\n",
      "EarlyStopping counter: 25 out of 50\n",
      "epoch: 33 - train_acc: 0.9245742092457421 - train_loss: 0.1717233276457869 - val_acc: 0.6666666666666666 - val_loss: 0.8950414921876825\n",
      "EarlyStopping counter: 26 out of 50\n",
      "epoch: 34 - train_acc: 0.9391727493917275 - train_loss: 0.16554392547262314 - val_acc: 0.6 - val_loss: 1.1372103184362785\n",
      "EarlyStopping counter: 27 out of 50\n",
      "epoch: 35 - train_acc: 0.9318734793187348 - train_loss: 0.16215486985541597 - val_acc: 0.7 - val_loss: 1.408494419341119\n",
      "EarlyStopping counter: 28 out of 50\n",
      "epoch: 36 - train_acc: 0.9294403892944039 - train_loss: 0.16040039351533755 - val_acc: 0.7333333333333333 - val_loss: 0.7890284535418327\n",
      "EarlyStopping counter: 29 out of 50\n",
      "epoch: 37 - train_acc: 0.9172749391727494 - train_loss: 0.18764911400312595 - val_acc: 0.7333333333333333 - val_loss: 1.366078183555604\n",
      "EarlyStopping counter: 30 out of 50\n",
      "epoch: 38 - train_acc: 0.9391727493917275 - train_loss: 0.15395350293931023 - val_acc: 0.7333333333333333 - val_loss: 1.0812744194538069\n",
      "EarlyStopping counter: 31 out of 50\n",
      "epoch: 39 - train_acc: 0.9440389294403893 - train_loss: 0.1486827711666313 - val_acc: 0.7 - val_loss: 1.1414004636647308\n",
      "EarlyStopping counter: 32 out of 50\n",
      "epoch: 40 - train_acc: 0.9367396593673966 - train_loss: 0.1512652260605009 - val_acc: 0.8 - val_loss: 1.1451669776955433\n",
      "EarlyStopping counter: 33 out of 50\n",
      "epoch: 41 - train_acc: 0.9537712895377128 - train_loss: 0.1373157607687062 - val_acc: 0.6666666666666666 - val_loss: 1.0688587994004783\n",
      "EarlyStopping counter: 34 out of 50\n",
      "epoch: 42 - train_acc: 0.9221411192214112 - train_loss: 0.18706267221694198 - val_acc: 0.7 - val_loss: 1.4363456241929555\n",
      "EarlyStopping counter: 35 out of 50\n",
      "epoch: 43 - train_acc: 0.9367396593673966 - train_loss: 0.16707622499340344 - val_acc: 0.7 - val_loss: 1.0238732725437856\n",
      "EarlyStopping counter: 36 out of 50\n",
      "epoch: 44 - train_acc: 0.9732360097323601 - train_loss: 0.09993229536842362 - val_acc: 0.8333333333333334 - val_loss: 1.0949761543758563\n",
      "EarlyStopping counter: 37 out of 50\n",
      "epoch: 45 - train_acc: 0.9683698296836983 - train_loss: 0.08437976421021046 - val_acc: 0.7666666666666667 - val_loss: 1.0739237920721865\n",
      "EarlyStopping counter: 38 out of 50\n",
      "epoch: 46 - train_acc: 0.9610705596107056 - train_loss: 0.10957508543717093 - val_acc: 0.6666666666666666 - val_loss: 0.7949486914548977\n",
      "EarlyStopping counter: 39 out of 50\n",
      "epoch: 47 - train_acc: 0.948905109489051 - train_loss: 0.12650265808129177 - val_acc: 0.7 - val_loss: 1.502213705396621\n",
      "EarlyStopping counter: 40 out of 50\n",
      "epoch: 48 - train_acc: 0.9562043795620438 - train_loss: 0.13557128678300467 - val_acc: 0.8666666666666667 - val_loss: 1.7634069599903157\n",
      "EarlyStopping counter: 41 out of 50\n",
      "epoch: 49 - train_acc: 0.9464720194647201 - train_loss: 0.13627237955355098 - val_acc: 0.7 - val_loss: 1.0600177746436001\n",
      "EarlyStopping counter: 42 out of 50\n",
      "epoch: 50 - train_acc: 0.9513381995133819 - train_loss: 0.13714303873733727 - val_acc: 0.7666666666666667 - val_loss: 0.8209929423774497\n",
      "EarlyStopping counter: 43 out of 50\n",
      "epoch: 51 - train_acc: 0.9245742092457421 - train_loss: 0.20348183942380665 - val_acc: 0.6333333333333333 - val_loss: 1.143770667913624\n",
      "EarlyStopping counter: 44 out of 50\n",
      "epoch: 52 - train_acc: 0.927007299270073 - train_loss: 0.1815789032166139 - val_acc: 0.7333333333333333 - val_loss: 1.1510187842714101\n",
      "EarlyStopping counter: 45 out of 50\n",
      "epoch: 53 - train_acc: 0.9562043795620438 - train_loss: 0.12045395790035746 - val_acc: 0.7666666666666667 - val_loss: 2.065409342626875\n",
      "EarlyStopping counter: 46 out of 50\n",
      "epoch: 54 - train_acc: 0.9586374695863747 - train_loss: 0.10986893796392158 - val_acc: 0.6333333333333333 - val_loss: 1.236767027299301\n",
      "EarlyStopping counter: 47 out of 50\n",
      "epoch: 55 - train_acc: 0.9635036496350365 - train_loss: 0.11112317374735264 - val_acc: 0.6666666666666666 - val_loss: 0.8145756167428845\n",
      "EarlyStopping counter: 48 out of 50\n",
      "epoch: 56 - train_acc: 0.9708029197080292 - train_loss: 0.09707909745077797 - val_acc: 0.8333333333333334 - val_loss: 0.6736283125592604\n",
      "EarlyStopping counter: 49 out of 50\n",
      "epoch: 57 - train_acc: 0.9537712895377128 - train_loss: 0.12892686981248444 - val_acc: 0.7 - val_loss: 1.0813682062886767\n",
      "EarlyStopping counter: 50 out of 50\n",
      "epoch: 58 - train_acc: 0.9513381995133819 - train_loss: 0.1276572952656706 - val_acc: 0.6666666666666666 - val_loss: 0.8559241538582448\n",
      "=> Early stopped !!\n",
      "Accuracy: 0.6607142857142857\n",
      "Precision: 0.6373626373626373\n",
      "Recall: 0.9206349206349206\n",
      "F1: 0.7532467532467532\n",
      "\n",
      "[Fold 2]: \n",
      "len(train_sampler)=411\n",
      "len(val_sampler)=30\n",
      "[ 17  22  24  25  42  46  56  57  66  82  90 126 157 173 181 185 192 195\n",
      " 249 284 286 290 332 361 373 378 422 426 428 432]\n",
      "Validation loss decreased (inf --> 0.657933).  Saving model ...\n",
      "epoch: 0 - train_acc: 0.5693430656934306 - train_loss: 0.6885604229077565 - val_acc: 0.6333333333333333 - val_loss: 0.6579329264954523\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 1 - train_acc: 0.6058394160583942 - train_loss: 0.6396393560142036 - val_acc: 0.5666666666666667 - val_loss: 0.702412922940027\n",
      "Validation loss decreased (0.657933 --> 0.521146).  Saving model ...\n",
      "epoch: 2 - train_acc: 0.6496350364963503 - train_loss: 0.6001380650352487 - val_acc: 0.8 - val_loss: 0.521146256093551\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 3 - train_acc: 0.6618004866180048 - train_loss: 0.5825418907378267 - val_acc: 0.7333333333333333 - val_loss: 0.5302091699484862\n",
      "Validation loss decreased (0.521146 --> 0.519186).  Saving model ...\n",
      "epoch: 4 - train_acc: 0.7420924574209246 - train_loss: 0.5259489346210544 - val_acc: 0.6333333333333333 - val_loss: 0.5191859983378934\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 5 - train_acc: 0.7469586374695864 - train_loss: 0.4816058914800102 - val_acc: 0.5333333333333333 - val_loss: 0.917346124466895\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 6 - train_acc: 0.7858880778588808 - train_loss: 0.470863520145062 - val_acc: 0.6666666666666666 - val_loss: 0.5304635491739849\n",
      "EarlyStopping counter: 3 out of 50\n",
      "epoch: 7 - train_acc: 0.781021897810219 - train_loss: 0.4575396379003181 - val_acc: 0.7 - val_loss: 0.6373232329032419\n",
      "EarlyStopping counter: 4 out of 50\n",
      "epoch: 8 - train_acc: 0.8004866180048662 - train_loss: 0.4251166596896712 - val_acc: 0.6666666666666666 - val_loss: 0.6382301830655164\n",
      "Validation loss decreased (0.519186 --> 0.481859).  Saving model ...\n",
      "epoch: 9 - train_acc: 0.805352798053528 - train_loss: 0.4037066094513013 - val_acc: 0.6666666666666666 - val_loss: 0.4818588709701138\n",
      "Validation loss decreased (0.481859 --> 0.445261).  Saving model ...\n",
      "epoch: 10 - train_acc: 0.8272506082725061 - train_loss: 0.37514761059148527 - val_acc: 0.8 - val_loss: 0.4452612778767737\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 11 - train_acc: 0.8369829683698297 - train_loss: 0.36872971845010366 - val_acc: 0.7 - val_loss: 0.5164027850858699\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 12 - train_acc: 0.8491484184914841 - train_loss: 0.35095028706812664 - val_acc: 0.7666666666666667 - val_loss: 0.5329326162864978\n",
      "EarlyStopping counter: 3 out of 50\n",
      "epoch: 13 - train_acc: 0.8613138686131386 - train_loss: 0.34159442661629413 - val_acc: 0.6666666666666666 - val_loss: 0.7710336147288882\n",
      "Validation loss decreased (0.445261 --> 0.411136).  Saving model ...\n",
      "epoch: 14 - train_acc: 0.8491484184914841 - train_loss: 0.3304883403803911 - val_acc: 0.7333333333333333 - val_loss: 0.4111355340049604\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 15 - train_acc: 0.8613138686131386 - train_loss: 0.30181356856365865 - val_acc: 0.7333333333333333 - val_loss: 0.4915644673404091\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 16 - train_acc: 0.8588807785888077 - train_loss: 0.2946745393105288 - val_acc: 0.8 - val_loss: 0.4715835675451554\n",
      "EarlyStopping counter: 3 out of 50\n",
      "epoch: 17 - train_acc: 0.8832116788321168 - train_loss: 0.2760558776718153 - val_acc: 0.8 - val_loss: 0.5105852077179922\n",
      "Validation loss decreased (0.411136 --> 0.397276).  Saving model ...\n",
      "epoch: 18 - train_acc: 0.8613138686131386 - train_loss: 0.2872771070772659 - val_acc: 0.8 - val_loss: 0.39727594287866264\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 19 - train_acc: 0.9124087591240876 - train_loss: 0.2120887461044652 - val_acc: 0.7666666666666667 - val_loss: 0.48281122307561014\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 20 - train_acc: 0.8734793187347932 - train_loss: 0.30504333657776245 - val_acc: 0.8666666666666667 - val_loss: 1.3714158047264684\n",
      "EarlyStopping counter: 3 out of 50\n",
      "epoch: 21 - train_acc: 0.8807785888077859 - train_loss: 0.27716334606384685 - val_acc: 0.7666666666666667 - val_loss: 0.7612980476366894\n",
      "EarlyStopping counter: 4 out of 50\n",
      "epoch: 22 - train_acc: 0.8978102189781022 - train_loss: 0.24470972018800283 - val_acc: 0.8666666666666667 - val_loss: 0.4161437660383377\n",
      "EarlyStopping counter: 5 out of 50\n",
      "epoch: 23 - train_acc: 0.8856447688564477 - train_loss: 0.23176255475239102 - val_acc: 0.7333333333333333 - val_loss: 0.550315114502249\n",
      "EarlyStopping counter: 6 out of 50\n",
      "epoch: 24 - train_acc: 0.9148418491484185 - train_loss: 0.2189706293337917 - val_acc: 0.7333333333333333 - val_loss: 0.5400053092991858\n",
      "EarlyStopping counter: 7 out of 50\n",
      "epoch: 25 - train_acc: 0.8953771289537713 - train_loss: 0.24200296858426548 - val_acc: 0.7 - val_loss: 0.45777728914617205\n",
      "EarlyStopping counter: 8 out of 50\n",
      "epoch: 26 - train_acc: 0.8978102189781022 - train_loss: 0.22792764075987124 - val_acc: 0.6666666666666666 - val_loss: 0.5494345944689587\n",
      "EarlyStopping counter: 9 out of 50\n",
      "epoch: 27 - train_acc: 0.9075425790754258 - train_loss: 0.22712828266385748 - val_acc: 0.8 - val_loss: 0.6116207598686669\n",
      "EarlyStopping counter: 10 out of 50\n",
      "epoch: 28 - train_acc: 0.9002433090024331 - train_loss: 0.22483031167377845 - val_acc: 0.7 - val_loss: 0.6643571110719154\n",
      "EarlyStopping counter: 11 out of 50\n",
      "epoch: 29 - train_acc: 0.9221411192214112 - train_loss: 0.20337344307448513 - val_acc: 0.7 - val_loss: 0.665102674714288\n",
      "EarlyStopping counter: 12 out of 50\n",
      "epoch: 30 - train_acc: 0.9562043795620438 - train_loss: 0.1399514305557647 - val_acc: 0.7 - val_loss: 0.4822292370363022\n",
      "EarlyStopping counter: 13 out of 50\n",
      "epoch: 31 - train_acc: 0.902676399026764 - train_loss: 0.20340808043014386 - val_acc: 0.7 - val_loss: 0.6658814791722625\n",
      "EarlyStopping counter: 14 out of 50\n",
      "epoch: 32 - train_acc: 0.8978102189781022 - train_loss: 0.23659842666514913 - val_acc: 0.8 - val_loss: 0.6019722315694309\n",
      "EarlyStopping counter: 15 out of 50\n",
      "epoch: 33 - train_acc: 0.9343065693430657 - train_loss: 0.16262821443953218 - val_acc: 0.7333333333333333 - val_loss: 0.5915944718717832\n",
      "EarlyStopping counter: 16 out of 50\n",
      "epoch: 34 - train_acc: 0.9440389294403893 - train_loss: 0.16225800705580187 - val_acc: 0.8 - val_loss: 0.713540083368575\n",
      "EarlyStopping counter: 17 out of 50\n",
      "epoch: 35 - train_acc: 0.9513381995133819 - train_loss: 0.13400908701701122 - val_acc: 0.7333333333333333 - val_loss: 0.6627661947221499\n",
      "EarlyStopping counter: 18 out of 50\n",
      "epoch: 36 - train_acc: 0.9124087591240876 - train_loss: 0.24436107416000566 - val_acc: 0.7 - val_loss: 1.1860334304367943\n",
      "EarlyStopping counter: 19 out of 50\n",
      "epoch: 37 - train_acc: 0.9416058394160584 - train_loss: 0.15277793049065058 - val_acc: 0.7333333333333333 - val_loss: 0.6707642947473438\n",
      "EarlyStopping counter: 20 out of 50\n",
      "epoch: 38 - train_acc: 0.9124087591240876 - train_loss: 0.1943918413281441 - val_acc: 0.7 - val_loss: 1.316290416482292\n",
      "EarlyStopping counter: 21 out of 50\n",
      "epoch: 39 - train_acc: 0.9318734793187348 - train_loss: 0.18879461761227 - val_acc: 0.7333333333333333 - val_loss: 0.728427236749212\n",
      "EarlyStopping counter: 22 out of 50\n",
      "epoch: 40 - train_acc: 0.9221411192214112 - train_loss: 0.1823437752679918 - val_acc: 0.7333333333333333 - val_loss: 0.8684636862046853\n",
      "EarlyStopping counter: 23 out of 50\n",
      "epoch: 41 - train_acc: 0.9197080291970803 - train_loss: 0.16256363583412822 - val_acc: 0.7666666666666667 - val_loss: 0.8158644198028511\n",
      "EarlyStopping counter: 24 out of 50\n",
      "epoch: 42 - train_acc: 0.9513381995133819 - train_loss: 0.1384234602464846 - val_acc: 0.7333333333333333 - val_loss: 0.8271047023220452\n",
      "EarlyStopping counter: 25 out of 50\n",
      "epoch: 43 - train_acc: 0.9391727493917275 - train_loss: 0.15736222339626976 - val_acc: 0.8 - val_loss: 0.5705239750235463\n",
      "EarlyStopping counter: 26 out of 50\n",
      "epoch: 44 - train_acc: 0.9464720194647201 - train_loss: 0.16313186902125057 - val_acc: 0.7333333333333333 - val_loss: 0.7262441973966689\n",
      "EarlyStopping counter: 27 out of 50\n",
      "epoch: 45 - train_acc: 0.9440389294403893 - train_loss: 0.12971650542890964 - val_acc: 0.8 - val_loss: 0.9394806926605965\n",
      "EarlyStopping counter: 28 out of 50\n",
      "epoch: 46 - train_acc: 0.9610705596107056 - train_loss: 0.10019418119068894 - val_acc: 0.7666666666666667 - val_loss: 0.925443164168936\n",
      "EarlyStopping counter: 29 out of 50\n",
      "epoch: 47 - train_acc: 0.9391727493917275 - train_loss: 0.1594582480479689 - val_acc: 0.7666666666666667 - val_loss: 0.6836023682781017\n",
      "EarlyStopping counter: 30 out of 50\n",
      "epoch: 48 - train_acc: 0.9586374695863747 - train_loss: 0.12999192786918984 - val_acc: 0.8333333333333334 - val_loss: 0.5947884775546094\n",
      "EarlyStopping counter: 31 out of 50\n",
      "epoch: 49 - train_acc: 0.9586374695863747 - train_loss: 0.10202251491031944 - val_acc: 0.7 - val_loss: 1.1250134566251073\n",
      "EarlyStopping counter: 32 out of 50\n",
      "epoch: 50 - train_acc: 0.9537712895377128 - train_loss: 0.1265482718559094 - val_acc: 0.7333333333333333 - val_loss: 1.2327868488756\n",
      "EarlyStopping counter: 33 out of 50\n",
      "epoch: 51 - train_acc: 0.9635036496350365 - train_loss: 0.10396056242898931 - val_acc: 0.7333333333333333 - val_loss: 0.6012318275620492\n",
      "EarlyStopping counter: 34 out of 50\n",
      "epoch: 52 - train_acc: 0.9586374695863747 - train_loss: 0.11040484662777839 - val_acc: 0.8666666666666667 - val_loss: 0.6717977095341644\n",
      "EarlyStopping counter: 35 out of 50\n",
      "epoch: 53 - train_acc: 0.9562043795620438 - train_loss: 0.1386253745172079 - val_acc: 0.8 - val_loss: 0.5657715580626125\n",
      "EarlyStopping counter: 36 out of 50\n",
      "epoch: 54 - train_acc: 0.9683698296836983 - train_loss: 0.09160220234017155 - val_acc: 0.7666666666666667 - val_loss: 0.9832133935044853\n",
      "EarlyStopping counter: 37 out of 50\n",
      "epoch: 55 - train_acc: 0.9805352798053528 - train_loss: 0.06916583016128061 - val_acc: 0.7333333333333333 - val_loss: 0.8897900217900027\n",
      "EarlyStopping counter: 38 out of 50\n",
      "epoch: 56 - train_acc: 0.9416058394160584 - train_loss: 0.12298659985312918 - val_acc: 0.7666666666666667 - val_loss: 1.0806944040585458\n",
      "EarlyStopping counter: 39 out of 50\n",
      "epoch: 57 - train_acc: 0.9440389294403893 - train_loss: 0.12981950738043777 - val_acc: 0.7666666666666667 - val_loss: 0.5663180822184457\n",
      "EarlyStopping counter: 40 out of 50\n",
      "epoch: 58 - train_acc: 0.9343065693430657 - train_loss: 0.1330427455551177 - val_acc: 0.7666666666666667 - val_loss: 0.6971875634894074\n",
      "EarlyStopping counter: 41 out of 50\n",
      "epoch: 59 - train_acc: 0.9610705596107056 - train_loss: 0.12045102603532079 - val_acc: 0.8 - val_loss: 0.7115077006456547\n",
      "EarlyStopping counter: 42 out of 50\n",
      "epoch: 60 - train_acc: 0.9878345498783455 - train_loss: 0.06926129431161736 - val_acc: 0.7 - val_loss: 1.1180627952441826\n",
      "EarlyStopping counter: 43 out of 50\n",
      "epoch: 61 - train_acc: 0.9537712895377128 - train_loss: 0.09525572232553564 - val_acc: 0.7333333333333333 - val_loss: 0.9160810618272625\n",
      "EarlyStopping counter: 44 out of 50\n",
      "epoch: 62 - train_acc: 0.9635036496350365 - train_loss: 0.08539572432116894 - val_acc: 0.7666666666666667 - val_loss: 0.645331304604033\n",
      "EarlyStopping counter: 45 out of 50\n",
      "epoch: 63 - train_acc: 0.9805352798053528 - train_loss: 0.06850612551260553 - val_acc: 0.7666666666666667 - val_loss: 1.4040875660418268\n",
      "EarlyStopping counter: 46 out of 50\n",
      "epoch: 64 - train_acc: 0.9878345498783455 - train_loss: 0.05584637352554927 - val_acc: 0.7 - val_loss: 0.8176032141761335\n",
      "EarlyStopping counter: 47 out of 50\n",
      "epoch: 65 - train_acc: 0.9732360097323601 - train_loss: 0.07657402646989861 - val_acc: 0.7666666666666667 - val_loss: 0.8515235989897512\n",
      "EarlyStopping counter: 48 out of 50\n",
      "epoch: 66 - train_acc: 0.9854014598540146 - train_loss: 0.06414670144697536 - val_acc: 0.7666666666666667 - val_loss: 2.3333749404550446\n",
      "EarlyStopping counter: 49 out of 50\n",
      "epoch: 67 - train_acc: 0.9805352798053528 - train_loss: 0.05633257968861423 - val_acc: 0.7666666666666667 - val_loss: 1.0211396615952526\n",
      "EarlyStopping counter: 50 out of 50\n",
      "epoch: 68 - train_acc: 0.9659367396593674 - train_loss: 0.09724754568755864 - val_acc: 0.7333333333333333 - val_loss: 1.057068016219039\n",
      "=> Early stopped !!\n",
      "Accuracy: 0.6830357142857143\n",
      "Precision: 0.6896551724137931\n",
      "Recall: 0.7936507936507936\n",
      "F1: 0.7380073800738007\n",
      "\n",
      "[Fold 3]: \n",
      "len(train_sampler)=411\n",
      "len(val_sampler)=30\n",
      "[  3   5  16  18  31  45  60  63  84  94 104 140 168 175 208 218 222 229\n",
      " 231 239 277 285 297 302 353 354 369 379 386 407]\n",
      "Validation loss decreased (inf --> 0.683582).  Saving model ...\n",
      "epoch: 0 - train_acc: 0.6009732360097324 - train_loss: 0.680082885742824 - val_acc: 0.6666666666666666 - val_loss: 0.6835824309799403\n",
      "Validation loss decreased (0.683582 --> 0.591042).  Saving model ...\n",
      "epoch: 1 - train_acc: 0.635036496350365 - train_loss: 0.649783115653205 - val_acc: 0.7 - val_loss: 0.591041787583038\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 2 - train_acc: 0.6715328467153284 - train_loss: 0.6093180847854731 - val_acc: 0.5 - val_loss: 0.6565900558953859\n",
      "Validation loss decreased (0.591042 --> 0.500239).  Saving model ...\n",
      "epoch: 3 - train_acc: 0.7299270072992701 - train_loss: 0.5607798259424279 - val_acc: 0.9 - val_loss: 0.5002388926285252\n",
      "Validation loss decreased (0.500239 --> 0.433404).  Saving model ...\n",
      "epoch: 4 - train_acc: 0.7372262773722628 - train_loss: 0.5149493639766058 - val_acc: 0.8 - val_loss: 0.43340415178489844\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 5 - train_acc: 0.7858880778588808 - train_loss: 0.46902814851233954 - val_acc: 0.6333333333333333 - val_loss: 0.6963315081586435\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 6 - train_acc: 0.7250608272506083 - train_loss: 0.5146630368756798 - val_acc: 0.8333333333333334 - val_loss: 0.4857033385791708\n",
      "EarlyStopping counter: 3 out of 50\n",
      "epoch: 7 - train_acc: 0.7834549878345499 - train_loss: 0.45798677918148295 - val_acc: 0.7 - val_loss: 0.4522599886738824\n",
      "EarlyStopping counter: 4 out of 50\n",
      "epoch: 8 - train_acc: 0.7834549878345499 - train_loss: 0.44916695611687485 - val_acc: 0.7333333333333333 - val_loss: 0.674764949847785\n",
      "Validation loss decreased (0.433404 --> 0.399659).  Saving model ...\n",
      "epoch: 9 - train_acc: 0.8029197080291971 - train_loss: 0.4007119598129825 - val_acc: 0.7 - val_loss: 0.39965937740519625\n",
      "Validation loss decreased (0.399659 --> 0.348954).  Saving model ...\n",
      "epoch: 10 - train_acc: 0.805352798053528 - train_loss: 0.38483235756248707 - val_acc: 0.8 - val_loss: 0.3489535051091441\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 11 - train_acc: 0.8150851581508516 - train_loss: 0.4080202047699713 - val_acc: 0.8666666666666667 - val_loss: 0.3595796142535393\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 12 - train_acc: 0.8394160583941606 - train_loss: 0.3613543742190451 - val_acc: 0.8333333333333334 - val_loss: 0.3816546030865188\n",
      "Validation loss decreased (0.348954 --> 0.329079).  Saving model ...\n",
      "epoch: 13 - train_acc: 0.8345498783454988 - train_loss: 0.36973428460237323 - val_acc: 0.8333333333333334 - val_loss: 0.32907935498432356\n",
      "Validation loss decreased (0.329079 --> 0.286037).  Saving model ...\n",
      "epoch: 14 - train_acc: 0.8199513381995134 - train_loss: 0.3706238138728919 - val_acc: 0.9 - val_loss: 0.2860373835635528\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 15 - train_acc: 0.8540145985401459 - train_loss: 0.31666486807250366 - val_acc: 0.8333333333333334 - val_loss: 0.5197193820748041\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 16 - train_acc: 0.851581508515815 - train_loss: 0.33865820947376096 - val_acc: 0.8666666666666667 - val_loss: 0.48534318550614475\n",
      "EarlyStopping counter: 3 out of 50\n",
      "epoch: 17 - train_acc: 0.8369829683698297 - train_loss: 0.34387973448734493 - val_acc: 0.8333333333333334 - val_loss: 0.44354726860256566\n",
      "EarlyStopping counter: 4 out of 50\n",
      "epoch: 18 - train_acc: 0.8613138686131386 - train_loss: 0.2977127246300186 - val_acc: 0.7333333333333333 - val_loss: 0.4180378522917565\n",
      "EarlyStopping counter: 5 out of 50\n",
      "epoch: 19 - train_acc: 0.8929440389294404 - train_loss: 0.24731074919800355 - val_acc: 0.8666666666666667 - val_loss: 0.29726747237769446\n",
      "Validation loss decreased (0.286037 --> 0.276219).  Saving model ...\n",
      "epoch: 20 - train_acc: 0.8637469586374696 - train_loss: 0.31484053721262384 - val_acc: 0.8666666666666667 - val_loss: 0.27621945882826293\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 21 - train_acc: 0.9002433090024331 - train_loss: 0.227131319836545 - val_acc: 0.9 - val_loss: 0.44143620078822565\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 22 - train_acc: 0.8856447688564477 - train_loss: 0.28284087913595946 - val_acc: 0.8333333333333334 - val_loss: 0.43473734597462355\n",
      "Validation loss decreased (0.276219 --> 0.248888).  Saving model ...\n",
      "epoch: 23 - train_acc: 0.8880778588807786 - train_loss: 0.26300147110101396 - val_acc: 0.8666666666666667 - val_loss: 0.24888837212687148\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 24 - train_acc: 0.9051094890510949 - train_loss: 0.2535949580758842 - val_acc: 0.9 - val_loss: 0.27938751072226103\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 25 - train_acc: 0.9197080291970803 - train_loss: 0.22398124934853247 - val_acc: 0.8666666666666667 - val_loss: 0.3701772056424703\n",
      "EarlyStopping counter: 3 out of 50\n",
      "epoch: 26 - train_acc: 0.8832116788321168 - train_loss: 0.2707777914654768 - val_acc: 0.8333333333333334 - val_loss: 0.48583328843675544\n",
      "EarlyStopping counter: 4 out of 50\n",
      "epoch: 27 - train_acc: 0.8856447688564477 - train_loss: 0.26635679117992095 - val_acc: 0.8 - val_loss: 0.4843177317160279\n",
      "EarlyStopping counter: 5 out of 50\n",
      "epoch: 28 - train_acc: 0.9002433090024331 - train_loss: 0.2246453251103828 - val_acc: 0.8333333333333334 - val_loss: 0.5293532831269863\n",
      "EarlyStopping counter: 6 out of 50\n",
      "epoch: 29 - train_acc: 0.9318734793187348 - train_loss: 0.19194450647780298 - val_acc: 0.8666666666666667 - val_loss: 0.37989828198183934\n",
      "EarlyStopping counter: 7 out of 50\n",
      "epoch: 30 - train_acc: 0.9440389294403893 - train_loss: 0.184661221867061 - val_acc: 0.8333333333333334 - val_loss: 0.30652874564265803\n",
      "EarlyStopping counter: 8 out of 50\n",
      "epoch: 31 - train_acc: 0.9172749391727494 - train_loss: 0.19649980494579378 - val_acc: 0.9 - val_loss: 0.5383651306847453\n",
      "EarlyStopping counter: 9 out of 50\n",
      "epoch: 32 - train_acc: 0.9221411192214112 - train_loss: 0.2083379827787565 - val_acc: 0.8666666666666667 - val_loss: 0.4855628843139316\n",
      "EarlyStopping counter: 10 out of 50\n",
      "epoch: 33 - train_acc: 0.9318734793187348 - train_loss: 0.1911583569527598 - val_acc: 0.8333333333333334 - val_loss: 0.7170385018893151\n",
      "EarlyStopping counter: 11 out of 50\n",
      "epoch: 34 - train_acc: 0.9391727493917275 - train_loss: 0.14438379785601435 - val_acc: 0.9 - val_loss: 0.47406010643405594\n",
      "EarlyStopping counter: 12 out of 50\n",
      "epoch: 35 - train_acc: 0.9367396593673966 - train_loss: 0.16816998882618264 - val_acc: 0.8333333333333334 - val_loss: 0.37387646068143054\n",
      "EarlyStopping counter: 13 out of 50\n",
      "epoch: 36 - train_acc: 0.9294403892944039 - train_loss: 0.17811624728311112 - val_acc: 0.8666666666666667 - val_loss: 0.4403422981073671\n",
      "EarlyStopping counter: 14 out of 50\n",
      "epoch: 37 - train_acc: 0.9343065693430657 - train_loss: 0.18203837288473865 - val_acc: 0.8666666666666667 - val_loss: 0.283235134203012\n",
      "EarlyStopping counter: 15 out of 50\n",
      "epoch: 38 - train_acc: 0.9513381995133819 - train_loss: 0.13870435867693576 - val_acc: 0.9 - val_loss: 0.3643292775886774\n",
      "EarlyStopping counter: 16 out of 50\n",
      "epoch: 39 - train_acc: 0.9416058394160584 - train_loss: 0.1598605895348245 - val_acc: 0.9 - val_loss: 0.3715162343777943\n",
      "EarlyStopping counter: 17 out of 50\n",
      "epoch: 40 - train_acc: 0.9610705596107056 - train_loss: 0.11398435928599175 - val_acc: 0.8666666666666667 - val_loss: 0.35434743030571075\n",
      "EarlyStopping counter: 18 out of 50\n",
      "epoch: 41 - train_acc: 0.9659367396593674 - train_loss: 0.09805117929677003 - val_acc: 0.9 - val_loss: 0.3140037064276098\n",
      "EarlyStopping counter: 19 out of 50\n",
      "epoch: 42 - train_acc: 0.9537712895377128 - train_loss: 0.11660630745431728 - val_acc: 0.9 - val_loss: 0.611288301606161\n",
      "EarlyStopping counter: 20 out of 50\n",
      "epoch: 43 - train_acc: 0.9391727493917275 - train_loss: 0.15421102366945105 - val_acc: 0.8333333333333334 - val_loss: 0.6749477922754288\n",
      "EarlyStopping counter: 21 out of 50\n",
      "epoch: 44 - train_acc: 0.9513381995133819 - train_loss: 0.12079961049285844 - val_acc: 0.8666666666666667 - val_loss: 0.3711616809306116\n",
      "EarlyStopping counter: 22 out of 50\n",
      "epoch: 45 - train_acc: 0.948905109489051 - train_loss: 0.11532279081652676 - val_acc: 0.8333333333333334 - val_loss: 0.47898145339524945\n",
      "EarlyStopping counter: 23 out of 50\n",
      "epoch: 46 - train_acc: 0.9562043795620438 - train_loss: 0.12582394947846523 - val_acc: 0.9 - val_loss: 0.7641172577525605\n",
      "EarlyStopping counter: 24 out of 50\n",
      "epoch: 47 - train_acc: 0.9367396593673966 - train_loss: 0.13672184914862845 - val_acc: 0.8666666666666667 - val_loss: 0.5553586298043185\n",
      "EarlyStopping counter: 25 out of 50\n",
      "epoch: 48 - train_acc: 0.948905109489051 - train_loss: 0.1405523094446037 - val_acc: 0.9 - val_loss: 0.406132518294563\n",
      "EarlyStopping counter: 26 out of 50\n",
      "epoch: 49 - train_acc: 0.9537712895377128 - train_loss: 0.1216111387268477 - val_acc: 0.9 - val_loss: 0.3225201530870674\n",
      "EarlyStopping counter: 27 out of 50\n",
      "epoch: 50 - train_acc: 0.9537712895377128 - train_loss: 0.1673795982898275 - val_acc: 0.8666666666666667 - val_loss: 0.4016484573783619\n",
      "EarlyStopping counter: 28 out of 50\n",
      "epoch: 51 - train_acc: 0.9464720194647201 - train_loss: 0.12938204823734925 - val_acc: 0.8666666666666667 - val_loss: 0.49692606276250106\n",
      "EarlyStopping counter: 29 out of 50\n",
      "epoch: 52 - train_acc: 0.9464720194647201 - train_loss: 0.13475535710090214 - val_acc: 0.8333333333333334 - val_loss: 0.9960076278288824\n",
      "EarlyStopping counter: 30 out of 50\n",
      "epoch: 53 - train_acc: 0.9610705596107056 - train_loss: 0.10141500795867782 - val_acc: 0.8666666666666667 - val_loss: 0.4262469596362916\n",
      "EarlyStopping counter: 31 out of 50\n",
      "epoch: 54 - train_acc: 0.9854014598540146 - train_loss: 0.0662313276189409 - val_acc: 0.9 - val_loss: 0.6297399380204162\n",
      "EarlyStopping counter: 32 out of 50\n",
      "epoch: 55 - train_acc: 0.9781021897810219 - train_loss: 0.06594393355757568 - val_acc: 0.8666666666666667 - val_loss: 0.5385981907071917\n",
      "EarlyStopping counter: 33 out of 50\n",
      "epoch: 56 - train_acc: 0.9732360097323601 - train_loss: 0.08347463938725828 - val_acc: 0.9 - val_loss: 0.5222672087804251\n",
      "EarlyStopping counter: 34 out of 50\n",
      "epoch: 57 - train_acc: 0.9683698296836983 - train_loss: 0.10889352094798281 - val_acc: 0.8666666666666667 - val_loss: 0.45206071652011\n",
      "EarlyStopping counter: 35 out of 50\n",
      "epoch: 58 - train_acc: 0.9781021897810219 - train_loss: 0.07403050754371268 - val_acc: 0.8333333333333334 - val_loss: 0.6402682167180215\n",
      "EarlyStopping counter: 36 out of 50\n",
      "epoch: 59 - train_acc: 0.9732360097323601 - train_loss: 0.07104162939099151 - val_acc: 0.8666666666666667 - val_loss: 0.5048749018847499\n",
      "EarlyStopping counter: 37 out of 50\n",
      "epoch: 60 - train_acc: 0.9683698296836983 - train_loss: 0.09516408657459227 - val_acc: 0.9 - val_loss: 0.9009180175224464\n",
      "EarlyStopping counter: 38 out of 50\n",
      "epoch: 61 - train_acc: 0.9440389294403893 - train_loss: 0.1526607504965818 - val_acc: 0.9 - val_loss: 1.184617061482692\n",
      "EarlyStopping counter: 39 out of 50\n",
      "epoch: 62 - train_acc: 0.9854014598540146 - train_loss: 0.05457164768553504 - val_acc: 0.8666666666666667 - val_loss: 0.34044037236694297\n",
      "EarlyStopping counter: 40 out of 50\n",
      "epoch: 63 - train_acc: 0.975669099756691 - train_loss: 0.0687818632822496 - val_acc: 0.9 - val_loss: 0.8615822602527862\n",
      "EarlyStopping counter: 41 out of 50\n",
      "epoch: 64 - train_acc: 0.9805352798053528 - train_loss: 0.07887400839163619 - val_acc: 0.9 - val_loss: 0.5709562022989377\n",
      "EarlyStopping counter: 42 out of 50\n",
      "epoch: 65 - train_acc: 0.9610705596107056 - train_loss: 0.11064473926440022 - val_acc: 0.8666666666666667 - val_loss: 0.47087337003350205\n",
      "EarlyStopping counter: 43 out of 50\n",
      "epoch: 66 - train_acc: 0.9732360097323601 - train_loss: 0.08203324471419567 - val_acc: 0.8333333333333334 - val_loss: 0.7256644153762504\n",
      "EarlyStopping counter: 44 out of 50\n",
      "epoch: 67 - train_acc: 0.9805352798053528 - train_loss: 0.055289792902755625 - val_acc: 0.9 - val_loss: 0.5154203697903794\n",
      "EarlyStopping counter: 45 out of 50\n",
      "epoch: 68 - train_acc: 0.9781021897810219 - train_loss: 0.07295136759910209 - val_acc: 0.9 - val_loss: 0.46882536527519436\n",
      "EarlyStopping counter: 46 out of 50\n",
      "epoch: 69 - train_acc: 0.9902676399026764 - train_loss: 0.049150793273731175 - val_acc: 0.9 - val_loss: 0.4093291102244066\n",
      "EarlyStopping counter: 47 out of 50\n",
      "epoch: 70 - train_acc: 0.9586374695863747 - train_loss: 0.10732723344064465 - val_acc: 0.8333333333333334 - val_loss: 0.6814986575217467\n",
      "EarlyStopping counter: 48 out of 50\n",
      "epoch: 71 - train_acc: 0.9586374695863747 - train_loss: 0.09254194132290776 - val_acc: 0.9 - val_loss: 0.41753612778856086\n",
      "EarlyStopping counter: 49 out of 50\n",
      "epoch: 72 - train_acc: 0.9562043795620438 - train_loss: 0.09962644755632488 - val_acc: 0.8666666666666667 - val_loss: 0.6115644251207112\n",
      "EarlyStopping counter: 50 out of 50\n",
      "epoch: 73 - train_acc: 0.9537712895377128 - train_loss: 0.14185401520679003 - val_acc: 0.8333333333333334 - val_loss: 1.0890895556635452\n",
      "=> Early stopped !!\n",
      "Accuracy: 0.71875\n",
      "Precision: 0.7058823529411765\n",
      "Recall: 0.8571428571428571\n",
      "F1: 0.7741935483870969\n",
      "\n",
      "[Fold 4]: \n",
      "len(train_sampler)=411\n",
      "len(val_sampler)=30\n",
      "[  7  26  29 108 109 110 114 118 141 145 148 193 210 225 266 274 294 320\n",
      " 323 324 336 380 384 392 400 411 415 417 427 438]\n",
      "Validation loss decreased (inf --> 0.718521).  Saving model ...\n",
      "epoch: 0 - train_acc: 0.5766423357664233 - train_loss: 0.6558532616079834 - val_acc: 0.36666666666666664 - val_loss: 0.718520581821019\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 1 - train_acc: 0.683698296836983 - train_loss: 0.607295559320938 - val_acc: 0.4666666666666667 - val_loss: 0.7345647813083278\n",
      "Validation loss decreased (0.718521 --> 0.624751).  Saving model ...\n",
      "epoch: 2 - train_acc: 0.6958637469586375 - train_loss: 0.5692683520118783 - val_acc: 0.5333333333333333 - val_loss: 0.6247513218548565\n",
      "Validation loss decreased (0.624751 --> 0.536287).  Saving model ...\n",
      "epoch: 3 - train_acc: 0.7372262773722628 - train_loss: 0.5289532628159666 - val_acc: 0.7 - val_loss: 0.5362871586140713\n",
      "Validation loss decreased (0.536287 --> 0.497760).  Saving model ...\n",
      "epoch: 4 - train_acc: 0.7785888077858881 - train_loss: 0.47972415299410803 - val_acc: 0.6666666666666666 - val_loss: 0.49775982348230746\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 5 - train_acc: 0.781021897810219 - train_loss: 0.4707540560583012 - val_acc: 0.6333333333333333 - val_loss: 0.6499662726620828\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 6 - train_acc: 0.7907542579075426 - train_loss: 0.4628814167055734 - val_acc: 0.7666666666666667 - val_loss: 0.8671520944981308\n",
      "EarlyStopping counter: 3 out of 50\n",
      "epoch: 7 - train_acc: 0.8126520681265207 - train_loss: 0.4036304405887036 - val_acc: 0.7333333333333333 - val_loss: 0.49828439752376275\n",
      "EarlyStopping counter: 4 out of 50\n",
      "epoch: 8 - train_acc: 0.8394160583941606 - train_loss: 0.3875742610055179 - val_acc: 0.6333333333333333 - val_loss: 0.5168981099793626\n",
      "EarlyStopping counter: 5 out of 50\n",
      "epoch: 9 - train_acc: 0.8418491484184915 - train_loss: 0.38865327882326073 - val_acc: 0.6666666666666666 - val_loss: 0.8302416093260698\n",
      "EarlyStopping counter: 6 out of 50\n",
      "epoch: 10 - train_acc: 0.8321167883211679 - train_loss: 0.37825157480701777 - val_acc: 0.7666666666666667 - val_loss: 0.5267635258237555\n",
      "Validation loss decreased (0.497760 --> 0.469597).  Saving model ...\n",
      "epoch: 11 - train_acc: 0.8467153284671532 - train_loss: 0.35818001488047463 - val_acc: 0.7666666666666667 - val_loss: 0.469597468195166\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 12 - train_acc: 0.8029197080291971 - train_loss: 0.3758505750350659 - val_acc: 0.6666666666666666 - val_loss: 0.5999600620215674\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 13 - train_acc: 0.8613138686131386 - train_loss: 0.3117272231182479 - val_acc: 0.7333333333333333 - val_loss: 0.6023470736701985\n",
      "Validation loss decreased (0.469597 --> 0.464670).  Saving model ...\n",
      "epoch: 14 - train_acc: 0.8637469586374696 - train_loss: 0.31474384293647045 - val_acc: 0.8333333333333334 - val_loss: 0.4646697198321919\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 15 - train_acc: 0.8661800486618005 - train_loss: 0.3246343453514965 - val_acc: 0.7333333333333333 - val_loss: 0.6417259418042499\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 16 - train_acc: 0.8734793187347932 - train_loss: 0.30539057596228186 - val_acc: 0.6333333333333333 - val_loss: 0.866886966028577\n",
      "EarlyStopping counter: 3 out of 50\n",
      "epoch: 17 - train_acc: 0.8637469586374696 - train_loss: 0.3353076122479807 - val_acc: 0.7 - val_loss: 0.6882159395110417\n",
      "EarlyStopping counter: 4 out of 50\n",
      "epoch: 18 - train_acc: 0.8540145985401459 - train_loss: 0.3273925129689675 - val_acc: 0.7333333333333333 - val_loss: 1.0052458747971877\n",
      "EarlyStopping counter: 5 out of 50\n",
      "epoch: 19 - train_acc: 0.8661800486618005 - train_loss: 0.3076626485282858 - val_acc: 0.7333333333333333 - val_loss: 0.7229462679663544\n",
      "EarlyStopping counter: 6 out of 50\n",
      "epoch: 20 - train_acc: 0.8807785888077859 - train_loss: 0.26302260337745975 - val_acc: 0.7 - val_loss: 0.6945523942592579\n",
      "EarlyStopping counter: 7 out of 50\n",
      "epoch: 21 - train_acc: 0.9002433090024331 - train_loss: 0.2320766438181344 - val_acc: 0.7333333333333333 - val_loss: 0.598013746841339\n",
      "EarlyStopping counter: 8 out of 50\n",
      "epoch: 22 - train_acc: 0.9002433090024331 - train_loss: 0.2368946166534217 - val_acc: 0.7666666666666667 - val_loss: 0.561962619222256\n",
      "EarlyStopping counter: 9 out of 50\n",
      "epoch: 23 - train_acc: 0.8734793187347932 - train_loss: 0.24414821377406057 - val_acc: 0.7 - val_loss: 0.6240906143182839\n",
      "EarlyStopping counter: 10 out of 50\n",
      "epoch: 24 - train_acc: 0.9099756690997567 - train_loss: 0.21566652312176457 - val_acc: 0.7666666666666667 - val_loss: 1.4337873819011464\n",
      "EarlyStopping counter: 11 out of 50\n",
      "epoch: 25 - train_acc: 0.9051094890510949 - train_loss: 0.22056287617980883 - val_acc: 0.8 - val_loss: 0.5437424227663729\n",
      "EarlyStopping counter: 12 out of 50\n",
      "epoch: 26 - train_acc: 0.8856447688564477 - train_loss: 0.233720057210489 - val_acc: 0.7 - val_loss: 0.7128807398825576\n",
      "EarlyStopping counter: 13 out of 50\n",
      "epoch: 27 - train_acc: 0.8880778588807786 - train_loss: 0.25508185928753413 - val_acc: 0.7 - val_loss: 0.6873848409111938\n",
      "EarlyStopping counter: 14 out of 50\n",
      "epoch: 28 - train_acc: 0.9148418491484185 - train_loss: 0.2075387769819408 - val_acc: 0.8333333333333334 - val_loss: 0.6375369455869734\n",
      "EarlyStopping counter: 15 out of 50\n",
      "epoch: 29 - train_acc: 0.9148418491484185 - train_loss: 0.20379269316627338 - val_acc: 0.8666666666666667 - val_loss: 0.4950929795597319\n",
      "EarlyStopping counter: 16 out of 50\n",
      "epoch: 30 - train_acc: 0.9197080291970803 - train_loss: 0.20617184907092295 - val_acc: 0.8 - val_loss: 0.5447213444686538\n",
      "EarlyStopping counter: 17 out of 50\n",
      "epoch: 31 - train_acc: 0.927007299270073 - train_loss: 0.20078108873181613 - val_acc: 0.7 - val_loss: 1.9146543130213345\n",
      "EarlyStopping counter: 18 out of 50\n",
      "epoch: 32 - train_acc: 0.927007299270073 - train_loss: 0.18325052019232393 - val_acc: 0.7333333333333333 - val_loss: 0.8643261861935536\n",
      "EarlyStopping counter: 19 out of 50\n",
      "epoch: 33 - train_acc: 0.9294403892944039 - train_loss: 0.15833564010487522 - val_acc: 0.7666666666666667 - val_loss: 0.5125799362655221\n",
      "EarlyStopping counter: 20 out of 50\n",
      "epoch: 34 - train_acc: 0.9075425790754258 - train_loss: 0.21894301094176716 - val_acc: 0.7666666666666667 - val_loss: 0.7817073034261269\n",
      "EarlyStopping counter: 21 out of 50\n",
      "epoch: 35 - train_acc: 0.9099756690997567 - train_loss: 0.23542040308113796 - val_acc: 0.8 - val_loss: 0.5270759359185304\n",
      "EarlyStopping counter: 22 out of 50\n",
      "epoch: 36 - train_acc: 0.9318734793187348 - train_loss: 0.14582796010277177 - val_acc: 0.7666666666666667 - val_loss: 0.6520976688824311\n",
      "EarlyStopping counter: 23 out of 50\n",
      "epoch: 37 - train_acc: 0.9245742092457421 - train_loss: 0.15549306434418145 - val_acc: 0.8 - val_loss: 0.6774550617616566\n",
      "EarlyStopping counter: 24 out of 50\n",
      "epoch: 38 - train_acc: 0.9294403892944039 - train_loss: 0.1862154321135879 - val_acc: 0.7666666666666667 - val_loss: 0.6052510487287214\n",
      "EarlyStopping counter: 25 out of 50\n",
      "epoch: 39 - train_acc: 0.9318734793187348 - train_loss: 0.17150666298673703 - val_acc: 0.6666666666666666 - val_loss: 0.9873802657727961\n",
      "EarlyStopping counter: 26 out of 50\n",
      "epoch: 40 - train_acc: 0.9245742092457421 - train_loss: 0.15912225718238407 - val_acc: 0.7666666666666667 - val_loss: 0.6469663460764905\n",
      "EarlyStopping counter: 27 out of 50\n",
      "epoch: 41 - train_acc: 0.9343065693430657 - train_loss: 0.14571045848675673 - val_acc: 0.7333333333333333 - val_loss: 0.6444230900339227\n",
      "EarlyStopping counter: 28 out of 50\n",
      "epoch: 42 - train_acc: 0.9440389294403893 - train_loss: 0.1299868602371819 - val_acc: 0.8 - val_loss: 1.426757052926012\n",
      "EarlyStopping counter: 29 out of 50\n",
      "epoch: 43 - train_acc: 0.9659367396593674 - train_loss: 0.1109401074971446 - val_acc: 0.8 - val_loss: 0.7142705253500713\n",
      "EarlyStopping counter: 30 out of 50\n",
      "epoch: 44 - train_acc: 0.9343065693430657 - train_loss: 0.15427411375745764 - val_acc: 0.7333333333333333 - val_loss: 1.68837343680502\n",
      "EarlyStopping counter: 31 out of 50\n",
      "epoch: 45 - train_acc: 0.9343065693430657 - train_loss: 0.15177683912478918 - val_acc: 0.8333333333333334 - val_loss: 0.4706181598985688\n",
      "EarlyStopping counter: 32 out of 50\n",
      "epoch: 46 - train_acc: 0.9416058394160584 - train_loss: 0.18003692801897672 - val_acc: 0.8 - val_loss: 1.4778618137033015\n",
      "EarlyStopping counter: 33 out of 50\n",
      "epoch: 47 - train_acc: 0.9343065693430657 - train_loss: 0.15739969592764688 - val_acc: 0.7666666666666667 - val_loss: 0.5722311367894556\n",
      "EarlyStopping counter: 34 out of 50\n",
      "epoch: 48 - train_acc: 0.9513381995133819 - train_loss: 0.15229430706561953 - val_acc: 0.8 - val_loss: 0.554534051977913\n",
      "EarlyStopping counter: 35 out of 50\n",
      "epoch: 49 - train_acc: 0.9367396593673966 - train_loss: 0.1495578321168981 - val_acc: 0.6666666666666666 - val_loss: 0.9812207328989265\n",
      "EarlyStopping counter: 36 out of 50\n",
      "epoch: 50 - train_acc: 0.9562043795620438 - train_loss: 0.11661323073699018 - val_acc: 0.8666666666666667 - val_loss: 0.7652328078623067\n",
      "EarlyStopping counter: 37 out of 50\n",
      "epoch: 51 - train_acc: 0.9635036496350365 - train_loss: 0.11003392621376097 - val_acc: 0.6666666666666666 - val_loss: 1.466986813738766\n",
      "EarlyStopping counter: 38 out of 50\n",
      "epoch: 52 - train_acc: 0.9513381995133819 - train_loss: 0.10611139516906656 - val_acc: 0.9 - val_loss: 0.6463790070474855\n",
      "EarlyStopping counter: 39 out of 50\n",
      "epoch: 53 - train_acc: 0.9683698296836983 - train_loss: 0.08070566801967942 - val_acc: 0.8 - val_loss: 0.6003193135908015\n",
      "EarlyStopping counter: 40 out of 50\n",
      "epoch: 54 - train_acc: 0.975669099756691 - train_loss: 0.07507621220895368 - val_acc: 0.8666666666666667 - val_loss: 0.49942494334098286\n",
      "EarlyStopping counter: 41 out of 50\n",
      "epoch: 55 - train_acc: 0.9440389294403893 - train_loss: 0.15633321334420172 - val_acc: 0.7333333333333333 - val_loss: 0.6631066644059779\n",
      "EarlyStopping counter: 42 out of 50\n",
      "epoch: 56 - train_acc: 0.9221411192214112 - train_loss: 0.21518992302933654 - val_acc: 0.7 - val_loss: 2.744003689531373\n",
      "EarlyStopping counter: 43 out of 50\n",
      "epoch: 57 - train_acc: 0.9172749391727494 - train_loss: 0.17994710811077766 - val_acc: 0.7 - val_loss: 1.3132375658115667\n",
      "EarlyStopping counter: 44 out of 50\n",
      "epoch: 58 - train_acc: 0.9391727493917275 - train_loss: 0.1442507296831576 - val_acc: 0.8333333333333334 - val_loss: 0.756418492470531\n",
      "EarlyStopping counter: 45 out of 50\n",
      "epoch: 59 - train_acc: 0.9878345498783455 - train_loss: 0.061634319310990573 - val_acc: 0.8666666666666667 - val_loss: 0.7447130740461384\n",
      "EarlyStopping counter: 46 out of 50\n",
      "epoch: 60 - train_acc: 0.9732360097323601 - train_loss: 0.07051714595184347 - val_acc: 0.7666666666666667 - val_loss: 0.8648678661619256\n",
      "EarlyStopping counter: 47 out of 50\n",
      "epoch: 61 - train_acc: 0.9464720194647201 - train_loss: 0.1484215233821972 - val_acc: 0.7666666666666667 - val_loss: 0.7328036559042534\n",
      "EarlyStopping counter: 48 out of 50\n",
      "epoch: 62 - train_acc: 0.9635036496350365 - train_loss: 0.11625081769620085 - val_acc: 0.8 - val_loss: 0.7800960197692287\n",
      "EarlyStopping counter: 49 out of 50\n",
      "epoch: 63 - train_acc: 0.948905109489051 - train_loss: 0.13202741908722002 - val_acc: 0.8 - val_loss: 2.321389567301084\n",
      "EarlyStopping counter: 50 out of 50\n",
      "epoch: 64 - train_acc: 0.948905109489051 - train_loss: 0.1571428509545455 - val_acc: 0.8333333333333334 - val_loss: 0.6990874150130546\n",
      "=> Early stopped !!\n",
      "Accuracy: 0.6785714285714286\n",
      "Precision: 0.68\n",
      "Recall: 0.8095238095238095\n",
      "F1: 0.7391304347826086\n",
      "\n",
      "[Fold 5]: \n",
      "len(train_sampler)=411\n",
      "len(val_sampler)=30\n",
      "[  6  36  59  74  83 111 119 139 176 180 196 199 211 244 262 275 314 322\n",
      " 338 341 346 347 357 375 382 390 397 399 401 434]\n",
      "Validation loss decreased (inf --> 0.722530).  Saving model ...\n",
      "epoch: 0 - train_acc: 0.5790754257907542 - train_loss: 0.6924711594593411 - val_acc: 0.4666666666666667 - val_loss: 0.7225297577775883\n",
      "Validation loss decreased (0.722530 --> 0.486439).  Saving model ...\n",
      "epoch: 1 - train_acc: 0.656934306569343 - train_loss: 0.6105796945499398 - val_acc: 0.7666666666666667 - val_loss: 0.4864393481113932\n",
      "Validation loss decreased (0.486439 --> 0.409692).  Saving model ...\n",
      "epoch: 2 - train_acc: 0.6763990267639902 - train_loss: 0.6013676683337021 - val_acc: 0.8666666666666667 - val_loss: 0.40969201080542766\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 3 - train_acc: 0.708029197080292 - train_loss: 0.549442236649312 - val_acc: 0.8 - val_loss: 0.46447571923972275\n",
      "Validation loss decreased (0.409692 --> 0.363065).  Saving model ...\n",
      "epoch: 4 - train_acc: 0.7493917274939172 - train_loss: 0.517886918012943 - val_acc: 0.8 - val_loss: 0.3630646277948808\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 5 - train_acc: 0.7566909975669099 - train_loss: 0.48313602744216666 - val_acc: 0.7333333333333333 - val_loss: 0.6115439418656622\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 6 - train_acc: 0.7761557177615572 - train_loss: 0.46036374509382894 - val_acc: 0.7333333333333333 - val_loss: 0.4784573339547079\n",
      "EarlyStopping counter: 3 out of 50\n",
      "epoch: 7 - train_acc: 0.7785888077858881 - train_loss: 0.4677655113238361 - val_acc: 0.8333333333333334 - val_loss: 0.4302497982142057\n",
      "EarlyStopping counter: 4 out of 50\n",
      "epoch: 8 - train_acc: 0.7785888077858881 - train_loss: 0.4449259208554534 - val_acc: 0.7 - val_loss: 0.5186884916777934\n",
      "Validation loss decreased (0.363065 --> 0.350808).  Saving model ...\n",
      "epoch: 9 - train_acc: 0.8369829683698297 - train_loss: 0.38445695240834876 - val_acc: 0.8 - val_loss: 0.3508078173935424\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 10 - train_acc: 0.8175182481751825 - train_loss: 0.36788021931989384 - val_acc: 0.7333333333333333 - val_loss: 0.5509156038915723\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 11 - train_acc: 0.8077858880778589 - train_loss: 0.3808621341378363 - val_acc: 0.7 - val_loss: 0.4883621879902549\n",
      "EarlyStopping counter: 3 out of 50\n",
      "epoch: 12 - train_acc: 0.8588807785888077 - train_loss: 0.33843613127681327 - val_acc: 0.7666666666666667 - val_loss: 0.4650458073592032\n",
      "EarlyStopping counter: 4 out of 50\n",
      "epoch: 13 - train_acc: 0.8345498783454988 - train_loss: 0.34834041886950695 - val_acc: 0.8333333333333334 - val_loss: 0.35964011072386975\n",
      "EarlyStopping counter: 5 out of 50\n",
      "epoch: 14 - train_acc: 0.8345498783454988 - train_loss: 0.32627231696931974 - val_acc: 0.7333333333333333 - val_loss: 0.5337769868084501\n",
      "EarlyStopping counter: 6 out of 50\n",
      "epoch: 15 - train_acc: 0.8832116788321168 - train_loss: 0.3063973090779957 - val_acc: 0.7666666666666667 - val_loss: 0.4082680155698565\n",
      "EarlyStopping counter: 7 out of 50\n",
      "epoch: 16 - train_acc: 0.8661800486618005 - train_loss: 0.28594066430439596 - val_acc: 0.8333333333333334 - val_loss: 0.4137850970321699\n",
      "EarlyStopping counter: 8 out of 50\n",
      "epoch: 17 - train_acc: 0.8686131386861314 - train_loss: 0.2754008724885581 - val_acc: 0.8333333333333334 - val_loss: 0.566198365333501\n",
      "EarlyStopping counter: 9 out of 50\n",
      "epoch: 18 - train_acc: 0.8734793187347932 - train_loss: 0.29341369934800926 - val_acc: 0.8333333333333334 - val_loss: 0.4341378602828706\n",
      "EarlyStopping counter: 10 out of 50\n",
      "epoch: 19 - train_acc: 0.9099756690997567 - train_loss: 0.22409226969418353 - val_acc: 0.8 - val_loss: 0.6532130062951145\n",
      "EarlyStopping counter: 11 out of 50\n",
      "epoch: 20 - train_acc: 0.8759124087591241 - train_loss: 0.2697559600143582 - val_acc: 0.8333333333333334 - val_loss: 0.46118322973423365\n",
      "EarlyStopping counter: 12 out of 50\n",
      "epoch: 21 - train_acc: 0.9172749391727494 - train_loss: 0.23753605806326283 - val_acc: 0.8 - val_loss: 0.39100562426753194\n",
      "EarlyStopping counter: 13 out of 50\n",
      "epoch: 22 - train_acc: 0.9221411192214112 - train_loss: 0.21440078473926288 - val_acc: 0.8666666666666667 - val_loss: 1.0313186739759712\n",
      "EarlyStopping counter: 14 out of 50\n",
      "epoch: 23 - train_acc: 0.9099756690997567 - train_loss: 0.2160606072706673 - val_acc: 0.7333333333333333 - val_loss: 0.8914571119601324\n",
      "EarlyStopping counter: 15 out of 50\n",
      "epoch: 24 - train_acc: 0.9075425790754258 - train_loss: 0.2343460785134372 - val_acc: 0.7666666666666667 - val_loss: 0.5193858488615509\n",
      "EarlyStopping counter: 16 out of 50\n",
      "epoch: 25 - train_acc: 0.9075425790754258 - train_loss: 0.25765064698828244 - val_acc: 0.7666666666666667 - val_loss: 0.5489647019418293\n",
      "EarlyStopping counter: 17 out of 50\n",
      "epoch: 26 - train_acc: 0.8978102189781022 - train_loss: 0.24081079237914377 - val_acc: 0.8 - val_loss: 0.4486192722990793\n",
      "EarlyStopping counter: 18 out of 50\n",
      "epoch: 27 - train_acc: 0.9002433090024331 - train_loss: 0.2834892500127776 - val_acc: 0.8333333333333334 - val_loss: 0.6615328750514974\n",
      "EarlyStopping counter: 19 out of 50\n",
      "epoch: 28 - train_acc: 0.8734793187347932 - train_loss: 0.29840379173467213 - val_acc: 0.7333333333333333 - val_loss: 0.6531803753322052\n",
      "EarlyStopping counter: 20 out of 50\n",
      "epoch: 29 - train_acc: 0.8929440389294404 - train_loss: 0.2540852024283292 - val_acc: 0.8333333333333334 - val_loss: 0.5424112128339699\n",
      "EarlyStopping counter: 21 out of 50\n",
      "epoch: 30 - train_acc: 0.9172749391727494 - train_loss: 0.20865869011148888 - val_acc: 0.8 - val_loss: 0.403958666684463\n",
      "EarlyStopping counter: 22 out of 50\n",
      "epoch: 31 - train_acc: 0.9148418491484185 - train_loss: 0.20723880962260938 - val_acc: 0.8666666666666667 - val_loss: 0.4924205364095521\n",
      "EarlyStopping counter: 23 out of 50\n",
      "epoch: 32 - train_acc: 0.9124087591240876 - train_loss: 0.20045079415260236 - val_acc: 0.8 - val_loss: 0.6827856911543015\n",
      "EarlyStopping counter: 24 out of 50\n",
      "epoch: 33 - train_acc: 0.9148418491484185 - train_loss: 0.2065302740063115 - val_acc: 0.8333333333333334 - val_loss: 0.8090830847319035\n",
      "EarlyStopping counter: 25 out of 50\n",
      "epoch: 34 - train_acc: 0.927007299270073 - train_loss: 0.1679816441464655 - val_acc: 0.8333333333333334 - val_loss: 0.7541590060602639\n",
      "EarlyStopping counter: 26 out of 50\n",
      "epoch: 35 - train_acc: 0.9416058394160584 - train_loss: 0.15285045004701012 - val_acc: 0.8333333333333334 - val_loss: 1.6649545910135546\n",
      "EarlyStopping counter: 27 out of 50\n",
      "epoch: 36 - train_acc: 0.9513381995133819 - train_loss: 0.1358487990324189 - val_acc: 0.8666666666666667 - val_loss: 0.5665417501705938\n",
      "EarlyStopping counter: 28 out of 50\n",
      "epoch: 37 - train_acc: 0.9318734793187348 - train_loss: 0.16458504521512837 - val_acc: 0.8333333333333334 - val_loss: 0.37184273277262553\n",
      "EarlyStopping counter: 29 out of 50\n",
      "epoch: 38 - train_acc: 0.9367396593673966 - train_loss: 0.15069193285472565 - val_acc: 0.8333333333333334 - val_loss: 0.6019839447590192\n",
      "EarlyStopping counter: 30 out of 50\n",
      "epoch: 39 - train_acc: 0.948905109489051 - train_loss: 0.13578743221252162 - val_acc: 0.8 - val_loss: 0.5184878442798659\n",
      "EarlyStopping counter: 31 out of 50\n",
      "epoch: 40 - train_acc: 0.9562043795620438 - train_loss: 0.13376512531949872 - val_acc: 0.8666666666666667 - val_loss: 0.5151889289596728\n",
      "EarlyStopping counter: 32 out of 50\n",
      "epoch: 41 - train_acc: 0.9610705596107056 - train_loss: 0.13411985523117542 - val_acc: 0.7666666666666667 - val_loss: 0.8524880024279773\n",
      "EarlyStopping counter: 33 out of 50\n",
      "epoch: 42 - train_acc: 0.9416058394160584 - train_loss: 0.163435937435085 - val_acc: 0.8666666666666667 - val_loss: 0.4814625605063488\n",
      "EarlyStopping counter: 34 out of 50\n",
      "epoch: 43 - train_acc: 0.9513381995133819 - train_loss: 0.12488705731790513 - val_acc: 0.8333333333333334 - val_loss: 0.6075840903800306\n",
      "EarlyStopping counter: 35 out of 50\n",
      "epoch: 44 - train_acc: 0.9537712895377128 - train_loss: 0.11907276111472039 - val_acc: 0.8666666666666667 - val_loss: 0.5704980573679522\n",
      "EarlyStopping counter: 36 out of 50\n",
      "epoch: 45 - train_acc: 0.9683698296836983 - train_loss: 0.10629004351389626 - val_acc: 0.8333333333333334 - val_loss: 0.7312939422223271\n",
      "EarlyStopping counter: 37 out of 50\n",
      "epoch: 46 - train_acc: 0.9635036496350365 - train_loss: 0.11032971339780594 - val_acc: 0.8666666666666667 - val_loss: 0.43184457409959864\n",
      "EarlyStopping counter: 38 out of 50\n",
      "epoch: 47 - train_acc: 0.9586374695863747 - train_loss: 0.10115760496273563 - val_acc: 0.9 - val_loss: 1.2433363225374285\n",
      "EarlyStopping counter: 39 out of 50\n",
      "epoch: 48 - train_acc: 0.9683698296836983 - train_loss: 0.09207873086573708 - val_acc: 0.8 - val_loss: 0.5652477474912156\n",
      "EarlyStopping counter: 40 out of 50\n",
      "epoch: 49 - train_acc: 0.9440389294403893 - train_loss: 0.12739089436950604 - val_acc: 0.8 - val_loss: 0.7226132639832379\n",
      "EarlyStopping counter: 41 out of 50\n",
      "epoch: 50 - train_acc: 0.948905109489051 - train_loss: 0.13412972364396067 - val_acc: 0.8333333333333334 - val_loss: 0.6025866739120357\n",
      "EarlyStopping counter: 42 out of 50\n",
      "epoch: 51 - train_acc: 0.948905109489051 - train_loss: 0.11281327324813596 - val_acc: 0.8333333333333334 - val_loss: 0.4841542075855153\n",
      "EarlyStopping counter: 43 out of 50\n",
      "epoch: 52 - train_acc: 0.948905109489051 - train_loss: 0.1509680912768163 - val_acc: 0.8666666666666667 - val_loss: 1.3546717971893147\n",
      "EarlyStopping counter: 44 out of 50\n",
      "epoch: 53 - train_acc: 0.9343065693430657 - train_loss: 0.15838425985737808 - val_acc: 0.6666666666666666 - val_loss: 0.8778741491862658\n",
      "EarlyStopping counter: 45 out of 50\n",
      "epoch: 54 - train_acc: 0.9464720194647201 - train_loss: 0.13456406378246322 - val_acc: 0.8333333333333334 - val_loss: 0.6312164445770384\n",
      "EarlyStopping counter: 46 out of 50\n",
      "epoch: 55 - train_acc: 0.9586374695863747 - train_loss: 0.09490790563111969 - val_acc: 0.8666666666666667 - val_loss: 0.5756670334228987\n",
      "EarlyStopping counter: 47 out of 50\n",
      "epoch: 56 - train_acc: 0.975669099756691 - train_loss: 0.08114953484340627 - val_acc: 0.8666666666666667 - val_loss: 0.7908950502057028\n",
      "EarlyStopping counter: 48 out of 50\n",
      "epoch: 57 - train_acc: 0.9829683698296837 - train_loss: 0.06478785565045543 - val_acc: 0.8666666666666667 - val_loss: 0.6614559515419449\n",
      "EarlyStopping counter: 49 out of 50\n",
      "epoch: 58 - train_acc: 0.9635036496350365 - train_loss: 0.08476659320289984 - val_acc: 0.7666666666666667 - val_loss: 0.6269492818628676\n",
      "EarlyStopping counter: 50 out of 50\n",
      "epoch: 59 - train_acc: 0.9781021897810219 - train_loss: 0.06027680895120897 - val_acc: 0.9 - val_loss: 0.5704470734244413\n",
      "=> Early stopped !!\n",
      "Accuracy: 0.7008928571428571\n",
      "Precision: 0.7153284671532847\n",
      "Recall: 0.7777777777777778\n",
      "F1: 0.7452471482889734\n",
      "\n",
      "[Fold 6]: \n",
      "len(train_sampler)=412\n",
      "len(val_sampler)=29\n",
      "[  2  10  69  81  89  92 103 123 147 150 158 163 167 177 184 194 203 209\n",
      " 234 245 304 311 318 352 409 414 418 421 429]\n",
      "Validation loss decreased (inf --> 0.538958).  Saving model ...\n",
      "epoch: 0 - train_acc: 0.5533980582524272 - train_loss: 0.6901633418931908 - val_acc: 0.6896551724137931 - val_loss: 0.5389579001097554\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 1 - train_acc: 0.6286407766990292 - train_loss: 0.6408612667306971 - val_acc: 0.6206896551724138 - val_loss: 0.6600772668381305\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 2 - train_acc: 0.6941747572815534 - train_loss: 0.6031104200989815 - val_acc: 0.7241379310344828 - val_loss: 0.568713871935842\n",
      "EarlyStopping counter: 3 out of 50\n",
      "epoch: 3 - train_acc: 0.7087378640776699 - train_loss: 0.5836282763369132 - val_acc: 0.5172413793103449 - val_loss: 0.71268693900354\n",
      "Validation loss decreased (0.538958 --> 0.513390).  Saving model ...\n",
      "epoch: 4 - train_acc: 0.7451456310679612 - train_loss: 0.4920746060576992 - val_acc: 0.7586206896551724 - val_loss: 0.5133895688393195\n",
      "Validation loss decreased (0.513390 --> 0.453200).  Saving model ...\n",
      "epoch: 5 - train_acc: 0.75 - train_loss: 0.4596727469978884 - val_acc: 0.6896551724137931 - val_loss: 0.45319959823759615\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 6 - train_acc: 0.7888349514563107 - train_loss: 0.4587649780398403 - val_acc: 0.6896551724137931 - val_loss: 0.5886450215240506\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 7 - train_acc: 0.7888349514563107 - train_loss: 0.4394541209533787 - val_acc: 0.7586206896551724 - val_loss: 0.5296439450116527\n",
      "Validation loss decreased (0.453200 --> 0.422932).  Saving model ...\n",
      "epoch: 8 - train_acc: 0.7912621359223301 - train_loss: 0.4233259504656477 - val_acc: 0.7586206896551724 - val_loss: 0.4229318555747185\n",
      "Validation loss decreased (0.422932 --> 0.305081).  Saving model ...\n",
      "epoch: 9 - train_acc: 0.8203883495145631 - train_loss: 0.3855938505782031 - val_acc: 0.8275862068965517 - val_loss: 0.30508071291211414\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 10 - train_acc: 0.8106796116504854 - train_loss: 0.38227313583693734 - val_acc: 0.8275862068965517 - val_loss: 0.4019993682432197\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 11 - train_acc: 0.8325242718446602 - train_loss: 0.3443341846940004 - val_acc: 0.7931034482758621 - val_loss: 0.3332574927012114\n",
      "EarlyStopping counter: 3 out of 50\n",
      "epoch: 12 - train_acc: 0.8179611650485437 - train_loss: 0.38169394677859725 - val_acc: 0.7931034482758621 - val_loss: 0.3542323227924713\n",
      "EarlyStopping counter: 4 out of 50\n",
      "epoch: 13 - train_acc: 0.8422330097087378 - train_loss: 0.35561336067247096 - val_acc: 0.8620689655172413 - val_loss: 0.39956513054798654\n",
      "EarlyStopping counter: 5 out of 50\n",
      "epoch: 14 - train_acc: 0.8470873786407767 - train_loss: 0.3290720937445328 - val_acc: 0.7241379310344828 - val_loss: 0.37142818493891355\n",
      "EarlyStopping counter: 6 out of 50\n",
      "epoch: 15 - train_acc: 0.8786407766990292 - train_loss: 0.30860827027383336 - val_acc: 0.7586206896551724 - val_loss: 0.46173122640865927\n",
      "Validation loss decreased (0.305081 --> 0.222827).  Saving model ...\n",
      "epoch: 16 - train_acc: 0.837378640776699 - train_loss: 0.35438857387224415 - val_acc: 0.8620689655172413 - val_loss: 0.22282683152575478\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 17 - train_acc: 0.883495145631068 - train_loss: 0.2972762124114918 - val_acc: 0.8275862068965517 - val_loss: 0.3956750693494958\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 18 - train_acc: 0.8810679611650486 - train_loss: 0.2842033843674347 - val_acc: 0.8275862068965517 - val_loss: 0.3425774086787179\n",
      "EarlyStopping counter: 3 out of 50\n",
      "epoch: 19 - train_acc: 0.8689320388349514 - train_loss: 0.27142002543702726 - val_acc: 0.8275862068965517 - val_loss: 0.27453315972194353\n",
      "EarlyStopping counter: 4 out of 50\n",
      "epoch: 20 - train_acc: 0.9004854368932039 - train_loss: 0.24477050352136526 - val_acc: 0.7586206896551724 - val_loss: 0.3628662425427966\n",
      "EarlyStopping counter: 5 out of 50\n",
      "epoch: 21 - train_acc: 0.8956310679611651 - train_loss: 0.25939926496890064 - val_acc: 0.8275862068965517 - val_loss: 0.31783048964076666\n",
      "EarlyStopping counter: 6 out of 50\n",
      "epoch: 22 - train_acc: 0.9004854368932039 - train_loss: 0.21959351655009382 - val_acc: 0.7931034482758621 - val_loss: 0.37462511464049586\n",
      "EarlyStopping counter: 7 out of 50\n",
      "epoch: 23 - train_acc: 0.8810679611650486 - train_loss: 0.3219635910228215 - val_acc: 0.7241379310344828 - val_loss: 0.40215928202843126\n",
      "EarlyStopping counter: 8 out of 50\n",
      "epoch: 24 - train_acc: 0.9077669902912622 - train_loss: 0.24053652604843898 - val_acc: 0.8620689655172413 - val_loss: 0.3664893174026165\n",
      "EarlyStopping counter: 9 out of 50\n",
      "epoch: 25 - train_acc: 0.9223300970873787 - train_loss: 0.21031631624434008 - val_acc: 0.7931034482758621 - val_loss: 0.3104843403448921\n",
      "EarlyStopping counter: 10 out of 50\n",
      "epoch: 26 - train_acc: 0.8859223300970874 - train_loss: 0.23932340598212148 - val_acc: 0.8275862068965517 - val_loss: 0.33334220143733584\n",
      "EarlyStopping counter: 11 out of 50\n",
      "epoch: 27 - train_acc: 0.9199029126213593 - train_loss: 0.22854047017010024 - val_acc: 0.8275862068965517 - val_loss: 0.30165959006038934\n",
      "EarlyStopping counter: 12 out of 50\n",
      "epoch: 28 - train_acc: 0.9441747572815534 - train_loss: 0.17287161784736002 - val_acc: 0.7586206896551724 - val_loss: 0.29094725624503787\n",
      "Validation loss decreased (0.222827 --> 0.211457).  Saving model ...\n",
      "epoch: 29 - train_acc: 0.9344660194174758 - train_loss: 0.16049070899989532 - val_acc: 0.8275862068965517 - val_loss: 0.21145739944794423\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 30 - train_acc: 0.9368932038834952 - train_loss: 0.18040468176368357 - val_acc: 0.7586206896551724 - val_loss: 0.4883646275740766\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 31 - train_acc: 0.9174757281553398 - train_loss: 0.18826841646754122 - val_acc: 0.7931034482758621 - val_loss: 0.48795159438446567\n",
      "EarlyStopping counter: 3 out of 50\n",
      "epoch: 32 - train_acc: 0.9393203883495146 - train_loss: 0.15125226553811225 - val_acc: 0.7931034482758621 - val_loss: 0.26849954167641116\n",
      "EarlyStopping counter: 4 out of 50\n",
      "epoch: 33 - train_acc: 0.9466019417475728 - train_loss: 0.14730831016066362 - val_acc: 0.7931034482758621 - val_loss: 0.33301580756380045\n",
      "EarlyStopping counter: 5 out of 50\n",
      "epoch: 34 - train_acc: 0.9296116504854369 - train_loss: 0.17647527792782094 - val_acc: 0.8275862068965517 - val_loss: 0.38536327432150486\n",
      "EarlyStopping counter: 6 out of 50\n",
      "epoch: 35 - train_acc: 0.9490291262135923 - train_loss: 0.1105855541733963 - val_acc: 0.8620689655172413 - val_loss: 0.34563960525518767\n",
      "EarlyStopping counter: 7 out of 50\n",
      "epoch: 36 - train_acc: 0.9538834951456311 - train_loss: 0.12199434631368805 - val_acc: 0.7931034482758621 - val_loss: 0.2843246765822328\n",
      "EarlyStopping counter: 8 out of 50\n",
      "epoch: 37 - train_acc: 0.9611650485436893 - train_loss: 0.11418998895417021 - val_acc: 0.9310344827586207 - val_loss: 0.22835600733526007\n",
      "Validation loss decreased (0.211457 --> 0.198929).  Saving model ...\n",
      "epoch: 38 - train_acc: 0.9320388349514563 - train_loss: 0.17274091851610143 - val_acc: 0.896551724137931 - val_loss: 0.19892931761704852\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 39 - train_acc: 0.9174757281553398 - train_loss: 0.20121764500873726 - val_acc: 0.8620689655172413 - val_loss: 0.6165737554078679\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 40 - train_acc: 0.9368932038834952 - train_loss: 0.14188744089614092 - val_acc: 0.8275862068965517 - val_loss: 0.39529738611971943\n",
      "EarlyStopping counter: 3 out of 50\n",
      "epoch: 41 - train_acc: 0.9174757281553398 - train_loss: 0.20212537613501838 - val_acc: 0.8620689655172413 - val_loss: 0.7248809737645329\n",
      "EarlyStopping counter: 4 out of 50\n",
      "epoch: 42 - train_acc: 0.9344660194174758 - train_loss: 0.16172283870656848 - val_acc: 0.7586206896551724 - val_loss: 0.4902165635228088\n",
      "EarlyStopping counter: 5 out of 50\n",
      "epoch: 43 - train_acc: 0.9538834951456311 - train_loss: 0.10711730867774803 - val_acc: 0.7931034482758621 - val_loss: 0.23914878544267953\n",
      "EarlyStopping counter: 6 out of 50\n",
      "epoch: 44 - train_acc: 0.9781553398058253 - train_loss: 0.07899618453820888 - val_acc: 0.7931034482758621 - val_loss: 0.4570938677140124\n",
      "EarlyStopping counter: 7 out of 50\n",
      "epoch: 45 - train_acc: 0.9587378640776699 - train_loss: 0.13025482396355703 - val_acc: 0.7586206896551724 - val_loss: 1.0370670666702904\n",
      "EarlyStopping counter: 8 out of 50\n",
      "epoch: 46 - train_acc: 0.9611650485436893 - train_loss: 0.11201102261180529 - val_acc: 0.8275862068965517 - val_loss: 0.20218237530170963\n",
      "EarlyStopping counter: 9 out of 50\n",
      "epoch: 47 - train_acc: 0.9635922330097088 - train_loss: 0.11479980204340395 - val_acc: 0.8275862068965517 - val_loss: 0.3202534066173663\n",
      "EarlyStopping counter: 10 out of 50\n",
      "epoch: 48 - train_acc: 0.9684466019417476 - train_loss: 0.10310931334697379 - val_acc: 0.8620689655172413 - val_loss: 0.31269541482431634\n",
      "EarlyStopping counter: 11 out of 50\n",
      "epoch: 49 - train_acc: 0.9660194174757282 - train_loss: 0.10180208617232686 - val_acc: 0.7586206896551724 - val_loss: 0.5158075746916522\n",
      "EarlyStopping counter: 12 out of 50\n",
      "epoch: 50 - train_acc: 0.9563106796116505 - train_loss: 0.10799398897890455 - val_acc: 0.8275862068965517 - val_loss: 0.43828975090427286\n",
      "EarlyStopping counter: 13 out of 50\n",
      "epoch: 51 - train_acc: 0.9660194174757282 - train_loss: 0.10804808456000069 - val_acc: 0.9310344827586207 - val_loss: 0.45285533096344716\n",
      "EarlyStopping counter: 14 out of 50\n",
      "epoch: 52 - train_acc: 0.9660194174757282 - train_loss: 0.08273011827956497 - val_acc: 0.896551724137931 - val_loss: 0.32280580579074764\n",
      "EarlyStopping counter: 15 out of 50\n",
      "epoch: 53 - train_acc: 0.9635922330097088 - train_loss: 0.08857702989050767 - val_acc: 0.8275862068965517 - val_loss: 0.37341899653417593\n",
      "EarlyStopping counter: 16 out of 50\n",
      "epoch: 54 - train_acc: 0.970873786407767 - train_loss: 0.08325451032268855 - val_acc: 0.8275862068965517 - val_loss: 0.5800534781104882\n",
      "EarlyStopping counter: 17 out of 50\n",
      "epoch: 55 - train_acc: 0.9635922330097088 - train_loss: 0.11006180021427094 - val_acc: 0.7241379310344828 - val_loss: 0.6052347977446895\n",
      "EarlyStopping counter: 18 out of 50\n",
      "epoch: 56 - train_acc: 0.9635922330097088 - train_loss: 0.11748703036531011 - val_acc: 0.7931034482758621 - val_loss: 0.5329465723999205\n",
      "EarlyStopping counter: 19 out of 50\n",
      "epoch: 57 - train_acc: 0.9781553398058253 - train_loss: 0.062436120869340826 - val_acc: 0.896551724137931 - val_loss: 0.2816889370635084\n",
      "EarlyStopping counter: 20 out of 50\n",
      "epoch: 58 - train_acc: 0.9684466019417476 - train_loss: 0.10566439201995594 - val_acc: 0.8620689655172413 - val_loss: 0.46034738419525656\n",
      "EarlyStopping counter: 21 out of 50\n",
      "epoch: 59 - train_acc: 0.9733009708737864 - train_loss: 0.0651642360823407 - val_acc: 0.8275862068965517 - val_loss: 0.5625085381341005\n",
      "EarlyStopping counter: 22 out of 50\n",
      "epoch: 60 - train_acc: 0.9902912621359223 - train_loss: 0.03532686657147607 - val_acc: 0.8620689655172413 - val_loss: 0.7702808458381154\n",
      "EarlyStopping counter: 23 out of 50\n",
      "epoch: 61 - train_acc: 0.9854368932038835 - train_loss: 0.045536851702373524 - val_acc: 0.8275862068965517 - val_loss: 0.28000998940909827\n",
      "EarlyStopping counter: 24 out of 50\n",
      "epoch: 62 - train_acc: 0.9902912621359223 - train_loss: 0.03795359093528774 - val_acc: 0.8620689655172413 - val_loss: 0.2721206794013306\n",
      "EarlyStopping counter: 25 out of 50\n",
      "epoch: 63 - train_acc: 0.970873786407767 - train_loss: 0.07581837507292387 - val_acc: 0.8275862068965517 - val_loss: 0.8245170099287122\n",
      "EarlyStopping counter: 26 out of 50\n",
      "epoch: 64 - train_acc: 0.9854368932038835 - train_loss: 0.06340784414213849 - val_acc: 0.7931034482758621 - val_loss: 0.4993940025073381\n",
      "EarlyStopping counter: 27 out of 50\n",
      "epoch: 65 - train_acc: 0.9635922330097088 - train_loss: 0.07962948979571642 - val_acc: 0.7931034482758621 - val_loss: 0.7367304663796853\n",
      "EarlyStopping counter: 28 out of 50\n",
      "epoch: 66 - train_acc: 0.9733009708737864 - train_loss: 0.05888201942764635 - val_acc: 0.8275862068965517 - val_loss: 0.2290530861221414\n",
      "EarlyStopping counter: 29 out of 50\n",
      "epoch: 67 - train_acc: 0.9733009708737864 - train_loss: 0.07213761762574794 - val_acc: 0.896551724137931 - val_loss: 0.20488500848072946\n",
      "EarlyStopping counter: 30 out of 50\n",
      "epoch: 68 - train_acc: 0.9757281553398058 - train_loss: 0.05422717776433522 - val_acc: 0.8275862068965517 - val_loss: 0.5160860768424945\n",
      "EarlyStopping counter: 31 out of 50\n",
      "epoch: 69 - train_acc: 0.9733009708737864 - train_loss: 0.07691430942599949 - val_acc: 0.7586206896551724 - val_loss: 0.41130631724551453\n",
      "EarlyStopping counter: 32 out of 50\n",
      "epoch: 70 - train_acc: 0.9635922330097088 - train_loss: 0.08162516629516268 - val_acc: 0.7931034482758621 - val_loss: 0.5489680459307948\n",
      "EarlyStopping counter: 33 out of 50\n",
      "epoch: 71 - train_acc: 0.9538834951456311 - train_loss: 0.12513473509919987 - val_acc: 0.8275862068965517 - val_loss: 0.4107221495478176\n",
      "EarlyStopping counter: 34 out of 50\n",
      "epoch: 72 - train_acc: 0.9174757281553398 - train_loss: 0.17185104056338565 - val_acc: 0.8620689655172413 - val_loss: 0.37831597418604773\n",
      "Validation loss decreased (0.198929 --> 0.186868).  Saving model ...\n",
      "epoch: 73 - train_acc: 0.9635922330097088 - train_loss: 0.11140169983740233 - val_acc: 0.896551724137931 - val_loss: 0.18686848041001183\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 74 - train_acc: 0.9805825242718447 - train_loss: 0.07221761597804842 - val_acc: 0.7931034482758621 - val_loss: 0.32250387260051827\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 75 - train_acc: 0.9684466019417476 - train_loss: 0.10624351690837266 - val_acc: 0.8275862068965517 - val_loss: 0.33870068413286336\n",
      "Validation loss decreased (0.186868 --> 0.136150).  Saving model ...\n",
      "epoch: 76 - train_acc: 0.9902912621359223 - train_loss: 0.04544714508126094 - val_acc: 0.8620689655172413 - val_loss: 0.1361495415318037\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 77 - train_acc: 0.9927184466019418 - train_loss: 0.037696097643695164 - val_acc: 0.8620689655172413 - val_loss: 0.2903779537622857\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 78 - train_acc: 0.9902912621359223 - train_loss: 0.030855050678023112 - val_acc: 0.9655172413793104 - val_loss: 0.14633857408013365\n",
      "EarlyStopping counter: 3 out of 50\n",
      "epoch: 79 - train_acc: 0.9781553398058253 - train_loss: 0.06921435224404307 - val_acc: 0.8275862068965517 - val_loss: 0.33628229338966437\n",
      "EarlyStopping counter: 4 out of 50\n",
      "epoch: 80 - train_acc: 0.9902912621359223 - train_loss: 0.039219376683719706 - val_acc: 0.7931034482758621 - val_loss: 0.3103262235669207\n",
      "EarlyStopping counter: 5 out of 50\n",
      "epoch: 81 - train_acc: 0.9878640776699029 - train_loss: 0.042736310271261096 - val_acc: 0.7586206896551724 - val_loss: 0.3238214373952516\n",
      "EarlyStopping counter: 6 out of 50\n",
      "epoch: 82 - train_acc: 0.9951456310679612 - train_loss: 0.023656535143593185 - val_acc: 0.8275862068965517 - val_loss: 0.2425043480888902\n",
      "EarlyStopping counter: 7 out of 50\n",
      "epoch: 83 - train_acc: 0.9951456310679612 - train_loss: 0.02800815048435827 - val_acc: 0.896551724137931 - val_loss: 0.2360701293440145\n",
      "EarlyStopping counter: 8 out of 50\n",
      "epoch: 84 - train_acc: 0.9733009708737864 - train_loss: 0.08076563238786515 - val_acc: 0.8275862068965517 - val_loss: 0.3469620626498841\n",
      "EarlyStopping counter: 9 out of 50\n",
      "epoch: 85 - train_acc: 0.9757281553398058 - train_loss: 0.062428796536743096 - val_acc: 0.896551724137931 - val_loss: 0.20463261039370517\n",
      "EarlyStopping counter: 10 out of 50\n",
      "epoch: 86 - train_acc: 0.9878640776699029 - train_loss: 0.04507446450905949 - val_acc: 0.8275862068965517 - val_loss: 0.6398091763644242\n",
      "EarlyStopping counter: 11 out of 50\n",
      "epoch: 87 - train_acc: 0.9878640776699029 - train_loss: 0.040304697153227216 - val_acc: 0.8620689655172413 - val_loss: 0.20277777919105888\n",
      "EarlyStopping counter: 12 out of 50\n",
      "epoch: 88 - train_acc: 0.9854368932038835 - train_loss: 0.05473056387758067 - val_acc: 0.8620689655172413 - val_loss: 0.18545612107456944\n",
      "EarlyStopping counter: 13 out of 50\n",
      "epoch: 89 - train_acc: 0.970873786407767 - train_loss: 0.08695818179912385 - val_acc: 0.896551724137931 - val_loss: 0.208131419404822\n",
      "EarlyStopping counter: 14 out of 50\n",
      "epoch: 90 - train_acc: 0.9660194174757282 - train_loss: 0.11007415381039329 - val_acc: 0.896551724137931 - val_loss: 0.4623009141844253\n",
      "EarlyStopping counter: 15 out of 50\n",
      "epoch: 91 - train_acc: 0.9830097087378641 - train_loss: 0.07025122683818942 - val_acc: 0.8620689655172413 - val_loss: 0.39089457344290496\n",
      "EarlyStopping counter: 16 out of 50\n",
      "epoch: 92 - train_acc: 0.9660194174757282 - train_loss: 0.08881941593118069 - val_acc: 0.896551724137931 - val_loss: 0.19326323581593843\n",
      "EarlyStopping counter: 17 out of 50\n",
      "epoch: 93 - train_acc: 0.9587378640776699 - train_loss: 0.10875195760455598 - val_acc: 0.8620689655172413 - val_loss: 0.32762925066309245\n",
      "EarlyStopping counter: 18 out of 50\n",
      "epoch: 94 - train_acc: 0.9878640776699029 - train_loss: 0.04919042770836145 - val_acc: 0.8620689655172413 - val_loss: 0.31982625769564976\n",
      "EarlyStopping counter: 19 out of 50\n",
      "epoch: 95 - train_acc: 0.9854368932038835 - train_loss: 0.03974718233258045 - val_acc: 0.9310344827586207 - val_loss: 0.4236317704474426\n",
      "EarlyStopping counter: 20 out of 50\n",
      "epoch: 96 - train_acc: 0.970873786407767 - train_loss: 0.06687028709458641 - val_acc: 0.8620689655172413 - val_loss: 0.3393105203105794\n",
      "EarlyStopping counter: 21 out of 50\n",
      "epoch: 97 - train_acc: 0.9611650485436893 - train_loss: 0.09391347323629448 - val_acc: 0.9310344827586207 - val_loss: 0.2300361197281592\n",
      "EarlyStopping counter: 22 out of 50\n",
      "epoch: 98 - train_acc: 0.9854368932038835 - train_loss: 0.03801766649488689 - val_acc: 0.896551724137931 - val_loss: 0.19717110591961073\n",
      "EarlyStopping counter: 23 out of 50\n",
      "epoch: 99 - train_acc: 0.9951456310679612 - train_loss: 0.02032284129870653 - val_acc: 0.896551724137931 - val_loss: 0.3542440368527485\n",
      "EarlyStopping counter: 24 out of 50\n",
      "epoch: 100 - train_acc: 0.9975728155339806 - train_loss: 0.027228837584840754 - val_acc: 0.8620689655172413 - val_loss: 0.16731998224244896\n",
      "EarlyStopping counter: 25 out of 50\n",
      "epoch: 101 - train_acc: 0.9975728155339806 - train_loss: 0.015355604385618761 - val_acc: 0.8275862068965517 - val_loss: 0.423598947523134\n",
      "EarlyStopping counter: 26 out of 50\n",
      "epoch: 102 - train_acc: 0.9733009708737864 - train_loss: 0.06578953908521919 - val_acc: 0.8275862068965517 - val_loss: 0.47515544155886796\n",
      "EarlyStopping counter: 27 out of 50\n",
      "epoch: 103 - train_acc: 0.9611650485436893 - train_loss: 0.08549866746970966 - val_acc: 0.9310344827586207 - val_loss: 0.15214718585416173\n",
      "EarlyStopping counter: 28 out of 50\n",
      "epoch: 104 - train_acc: 0.970873786407767 - train_loss: 0.07115274565551973 - val_acc: 0.8620689655172413 - val_loss: 0.205911935858313\n",
      "EarlyStopping counter: 29 out of 50\n",
      "epoch: 105 - train_acc: 0.9878640776699029 - train_loss: 0.03517141765323285 - val_acc: 0.896551724137931 - val_loss: 0.18161046775081088\n",
      "EarlyStopping counter: 30 out of 50\n",
      "epoch: 106 - train_acc: 0.9951456310679612 - train_loss: 0.03422645605124595 - val_acc: 0.8275862068965517 - val_loss: 0.19156428538448916\n",
      "EarlyStopping counter: 31 out of 50\n",
      "epoch: 107 - train_acc: 0.9878640776699029 - train_loss: 0.033912646960468744 - val_acc: 0.8275862068965517 - val_loss: 0.46596652886491663\n",
      "EarlyStopping counter: 32 out of 50\n",
      "epoch: 108 - train_acc: 0.9854368932038835 - train_loss: 0.042784879609167 - val_acc: 0.896551724137931 - val_loss: 0.33235661203665856\n",
      "EarlyStopping counter: 33 out of 50\n",
      "epoch: 109 - train_acc: 0.9902912621359223 - train_loss: 0.030104885353641912 - val_acc: 0.8620689655172413 - val_loss: 0.8941552323284636\n",
      "EarlyStopping counter: 34 out of 50\n",
      "epoch: 110 - train_acc: 0.9757281553398058 - train_loss: 0.08037301956021224 - val_acc: 0.7931034482758621 - val_loss: 0.2130636009279492\n",
      "EarlyStopping counter: 35 out of 50\n",
      "epoch: 111 - train_acc: 0.9830097087378641 - train_loss: 0.07313695820222021 - val_acc: 0.8275862068965517 - val_loss: 0.3741009770507281\n",
      "EarlyStopping counter: 36 out of 50\n",
      "epoch: 112 - train_acc: 0.9757281553398058 - train_loss: 0.062471670343077865 - val_acc: 0.8620689655172413 - val_loss: 0.22758481952570858\n",
      "EarlyStopping counter: 37 out of 50\n",
      "epoch: 113 - train_acc: 0.9830097087378641 - train_loss: 0.05011396082330716 - val_acc: 0.8275862068965517 - val_loss: 0.44911393216442225\n",
      "EarlyStopping counter: 38 out of 50\n",
      "epoch: 114 - train_acc: 0.9951456310679612 - train_loss: 0.0313733709276435 - val_acc: 0.7931034482758621 - val_loss: 0.33696546013151507\n",
      "EarlyStopping counter: 39 out of 50\n",
      "epoch: 115 - train_acc: 0.9951456310679612 - train_loss: 0.01868364741156781 - val_acc: 0.8275862068965517 - val_loss: 0.249026295643059\n",
      "EarlyStopping counter: 40 out of 50\n",
      "epoch: 116 - train_acc: 0.9781553398058253 - train_loss: 0.052153024455990064 - val_acc: 0.8620689655172413 - val_loss: 0.33188384828607015\n",
      "EarlyStopping counter: 41 out of 50\n",
      "epoch: 117 - train_acc: 0.9975728155339806 - train_loss: 0.021303935071278916 - val_acc: 0.8620689655172413 - val_loss: 0.37449216177083045\n",
      "EarlyStopping counter: 42 out of 50\n",
      "epoch: 118 - train_acc: 0.9927184466019418 - train_loss: 0.024801869664064533 - val_acc: 0.8620689655172413 - val_loss: 0.24320299828318082\n",
      "EarlyStopping counter: 43 out of 50\n",
      "epoch: 119 - train_acc: 0.9781553398058253 - train_loss: 0.04708969738126819 - val_acc: 0.7931034482758621 - val_loss: 0.3819316966792033\n",
      "EarlyStopping counter: 44 out of 50\n",
      "epoch: 120 - train_acc: 0.9830097087378641 - train_loss: 0.05204483880323935 - val_acc: 0.896551724137931 - val_loss: 0.43706850613786524\n",
      "EarlyStopping counter: 45 out of 50\n",
      "epoch: 121 - train_acc: 0.9902912621359223 - train_loss: 0.02796822344609551 - val_acc: 0.8275862068965517 - val_loss: 0.4237641473196337\n",
      "EarlyStopping counter: 46 out of 50\n",
      "epoch: 122 - train_acc: 0.9951456310679612 - train_loss: 0.016985664205193846 - val_acc: 0.7931034482758621 - val_loss: 0.6284463142338518\n",
      "EarlyStopping counter: 47 out of 50\n",
      "epoch: 123 - train_acc: 0.9975728155339806 - train_loss: 0.02006663397168063 - val_acc: 0.8275862068965517 - val_loss: 0.651086752686278\n",
      "EarlyStopping counter: 48 out of 50\n",
      "epoch: 124 - train_acc: 0.9611650485436893 - train_loss: 0.10192624449711829 - val_acc: 0.8620689655172413 - val_loss: 0.2736220436249381\n",
      "EarlyStopping counter: 49 out of 50\n",
      "epoch: 125 - train_acc: 0.9660194174757282 - train_loss: 0.09923490514483502 - val_acc: 0.7241379310344828 - val_loss: 1.405695825921109\n",
      "EarlyStopping counter: 50 out of 50\n",
      "epoch: 126 - train_acc: 0.970873786407767 - train_loss: 0.0895690031213526 - val_acc: 0.8620689655172413 - val_loss: 0.4868028210710622\n",
      "=> Early stopped !!\n",
      "Accuracy: 0.7321428571428571\n",
      "Precision: 0.7357142857142858\n",
      "Recall: 0.8174603174603174\n",
      "F1: 0.774436090225564\n",
      "\n",
      "[Fold 7]: \n",
      "len(train_sampler)=412\n",
      "len(val_sampler)=29\n",
      "[ 23  37  67  68  96  97 122 125 143 144 146 182 202 223 227 238 247 261\n",
      " 265 268 271 287 316 325 334 349 360 367 377]\n",
      "Validation loss decreased (inf --> 0.919742).  Saving model ...\n",
      "epoch: 0 - train_acc: 0.5558252427184466 - train_loss: 0.6728569020123677 - val_acc: 0.5862068965517241 - val_loss: 0.9197422881712829\n",
      "Validation loss decreased (0.919742 --> 0.531539).  Saving model ...\n",
      "epoch: 1 - train_acc: 0.6432038834951457 - train_loss: 0.6127705060444006 - val_acc: 0.6206896551724138 - val_loss: 0.5315387348657981\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 2 - train_acc: 0.6820388349514563 - train_loss: 0.5770462117816786 - val_acc: 0.5862068965517241 - val_loss: 0.7068857519858064\n",
      "Validation loss decreased (0.531539 --> 0.524101).  Saving model ...\n",
      "epoch: 3 - train_acc: 0.6577669902912622 - train_loss: 0.5752882437512534 - val_acc: 0.6896551724137931 - val_loss: 0.5241005189825872\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 4 - train_acc: 0.6868932038834952 - train_loss: 0.5643744544696788 - val_acc: 0.5172413793103449 - val_loss: 0.7922373917022663\n",
      "Validation loss decreased (0.524101 --> 0.492346).  Saving model ...\n",
      "epoch: 5 - train_acc: 0.7475728155339806 - train_loss: 0.5032153981397527 - val_acc: 0.7586206896551724 - val_loss: 0.49234642391339734\n",
      "Validation loss decreased (0.492346 --> 0.430592).  Saving model ...\n",
      "epoch: 6 - train_acc: 0.7548543689320388 - train_loss: 0.46125793824082995 - val_acc: 0.6551724137931034 - val_loss: 0.4305924676621152\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 7 - train_acc: 0.7572815533980582 - train_loss: 0.4803865872676898 - val_acc: 0.6551724137931034 - val_loss: 0.5986143004727023\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 8 - train_acc: 0.7839805825242718 - train_loss: 0.4475488777811731 - val_acc: 0.7931034482758621 - val_loss: 0.6104606611585763\n",
      "EarlyStopping counter: 3 out of 50\n",
      "epoch: 9 - train_acc: 0.7961165048543689 - train_loss: 0.4248542475158648 - val_acc: 0.7241379310344828 - val_loss: 0.7361473051736528\n",
      "EarlyStopping counter: 4 out of 50\n",
      "epoch: 10 - train_acc: 0.8033980582524272 - train_loss: 0.4210758577530965 - val_acc: 0.7586206896551724 - val_loss: 0.4543838496330346\n",
      "EarlyStopping counter: 5 out of 50\n",
      "epoch: 11 - train_acc: 0.8106796116504854 - train_loss: 0.3828675741298592 - val_acc: 0.5862068965517241 - val_loss: 0.6902726881584106\n",
      "EarlyStopping counter: 6 out of 50\n",
      "epoch: 12 - train_acc: 0.8300970873786407 - train_loss: 0.38082523500862986 - val_acc: 0.6896551724137931 - val_loss: 0.5762835693743505\n",
      "EarlyStopping counter: 7 out of 50\n",
      "epoch: 13 - train_acc: 0.8179611650485437 - train_loss: 0.38455705921100575 - val_acc: 0.7931034482758621 - val_loss: 0.5300300345487904\n",
      "Validation loss decreased (0.430592 --> 0.422897).  Saving model ...\n",
      "epoch: 14 - train_acc: 0.8276699029126213 - train_loss: 0.35140292114648414 - val_acc: 0.8620689655172413 - val_loss: 0.42289710194152735\n",
      "Validation loss decreased (0.422897 --> 0.392810).  Saving model ...\n",
      "epoch: 15 - train_acc: 0.866504854368932 - train_loss: 0.3194811281243623 - val_acc: 0.7931034482758621 - val_loss: 0.39280997608684853\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 16 - train_acc: 0.837378640776699 - train_loss: 0.33603983199282855 - val_acc: 0.6551724137931034 - val_loss: 0.3996625584025025\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 17 - train_acc: 0.866504854368932 - train_loss: 0.308414249051719 - val_acc: 0.8275862068965517 - val_loss: 0.41251534716584504\n",
      "EarlyStopping counter: 3 out of 50\n",
      "epoch: 18 - train_acc: 0.8519417475728155 - train_loss: 0.3058513997705247 - val_acc: 0.8620689655172413 - val_loss: 0.7505621601145522\n",
      "Validation loss decreased (0.392810 --> 0.315514).  Saving model ...\n",
      "epoch: 19 - train_acc: 0.9004854368932039 - train_loss: 0.2507480501198359 - val_acc: 0.7931034482758621 - val_loss: 0.3155140216627849\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 20 - train_acc: 0.8859223300970874 - train_loss: 0.2767313884275087 - val_acc: 0.8275862068965517 - val_loss: 0.38531669997459816\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 21 - train_acc: 0.8980582524271845 - train_loss: 0.2502636742688398 - val_acc: 0.6896551724137931 - val_loss: 0.515531775892357\n",
      "EarlyStopping counter: 3 out of 50\n",
      "epoch: 22 - train_acc: 0.8907766990291263 - train_loss: 0.2702653158667458 - val_acc: 0.8275862068965517 - val_loss: 0.34801491908470406\n",
      "EarlyStopping counter: 4 out of 50\n",
      "epoch: 23 - train_acc: 0.9029126213592233 - train_loss: 0.23338344209337172 - val_acc: 0.7931034482758621 - val_loss: 0.42574732763185086\n",
      "EarlyStopping counter: 5 out of 50\n",
      "epoch: 24 - train_acc: 0.8446601941747572 - train_loss: 0.32222036036979557 - val_acc: 0.7586206896551724 - val_loss: 0.45355109243754554\n",
      "EarlyStopping counter: 6 out of 50\n",
      "epoch: 25 - train_acc: 0.8859223300970874 - train_loss: 0.28904236841019654 - val_acc: 0.6896551724137931 - val_loss: 0.5014294023270924\n",
      "EarlyStopping counter: 7 out of 50\n",
      "epoch: 26 - train_acc: 0.9223300970873787 - train_loss: 0.2366440276753942 - val_acc: 0.7931034482758621 - val_loss: 0.3804812910408251\n",
      "EarlyStopping counter: 8 out of 50\n",
      "epoch: 27 - train_acc: 0.9271844660194175 - train_loss: 0.19869680701094575 - val_acc: 0.8275862068965517 - val_loss: 0.4009381485522337\n",
      "EarlyStopping counter: 9 out of 50\n",
      "epoch: 28 - train_acc: 0.9223300970873787 - train_loss: 0.19676390529416832 - val_acc: 0.8275862068965517 - val_loss: 0.4392265363404313\n",
      "EarlyStopping counter: 10 out of 50\n",
      "epoch: 29 - train_acc: 0.9150485436893204 - train_loss: 0.1974408090412113 - val_acc: 0.7931034482758621 - val_loss: 1.0524558373376627\n",
      "EarlyStopping counter: 11 out of 50\n",
      "epoch: 30 - train_acc: 0.9101941747572816 - train_loss: 0.21310160950680546 - val_acc: 0.7931034482758621 - val_loss: 0.43908258837912967\n",
      "EarlyStopping counter: 12 out of 50\n",
      "epoch: 31 - train_acc: 0.9271844660194175 - train_loss: 0.18999054448241318 - val_acc: 0.7931034482758621 - val_loss: 0.5085381687219271\n",
      "EarlyStopping counter: 13 out of 50\n",
      "epoch: 32 - train_acc: 0.9174757281553398 - train_loss: 0.19461170864767263 - val_acc: 0.896551724137931 - val_loss: 0.5313429145566005\n",
      "EarlyStopping counter: 14 out of 50\n",
      "epoch: 33 - train_acc: 0.9320388349514563 - train_loss: 0.18548890212526373 - val_acc: 0.7586206896551724 - val_loss: 1.2374090431790283\n",
      "EarlyStopping counter: 15 out of 50\n",
      "epoch: 34 - train_acc: 0.9077669902912622 - train_loss: 0.226855739861135 - val_acc: 0.7931034482758621 - val_loss: 0.3818198048847926\n",
      "EarlyStopping counter: 16 out of 50\n",
      "epoch: 35 - train_acc: 0.9271844660194175 - train_loss: 0.16683827763980677 - val_acc: 0.6896551724137931 - val_loss: 2.3510121313158985\n",
      "EarlyStopping counter: 17 out of 50\n",
      "epoch: 36 - train_acc: 0.9053398058252428 - train_loss: 0.2250055755412491 - val_acc: 0.6896551724137931 - val_loss: 0.6331183223466144\n",
      "EarlyStopping counter: 18 out of 50\n",
      "epoch: 37 - train_acc: 0.9466019417475728 - train_loss: 0.1460401487201314 - val_acc: 0.7241379310344828 - val_loss: 0.43110141267124213\n",
      "Validation loss decreased (0.315514 --> 0.221580).  Saving model ...\n",
      "epoch: 38 - train_acc: 0.9441747572815534 - train_loss: 0.13707344088994464 - val_acc: 0.8275862068965517 - val_loss: 0.22157968685913237\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 39 - train_acc: 0.912621359223301 - train_loss: 0.17144329078688225 - val_acc: 0.7241379310344828 - val_loss: 0.5356899237756034\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 40 - train_acc: 0.9344660194174758 - train_loss: 0.16628123746740384 - val_acc: 0.7931034482758621 - val_loss: 0.3702644052875685\n",
      "EarlyStopping counter: 3 out of 50\n",
      "epoch: 41 - train_acc: 0.9441747572815534 - train_loss: 0.18641586420991896 - val_acc: 0.7241379310344828 - val_loss: 0.9382039556293091\n",
      "EarlyStopping counter: 4 out of 50\n",
      "epoch: 42 - train_acc: 0.9271844660194175 - train_loss: 0.1654187239738606 - val_acc: 0.7586206896551724 - val_loss: 0.35922165267264616\n",
      "EarlyStopping counter: 5 out of 50\n",
      "epoch: 43 - train_acc: 0.9635922330097088 - train_loss: 0.11654688767340377 - val_acc: 0.7586206896551724 - val_loss: 1.3437149259716297\n",
      "EarlyStopping counter: 6 out of 50\n",
      "epoch: 44 - train_acc: 0.9441747572815534 - train_loss: 0.1562328478999639 - val_acc: 0.7586206896551724 - val_loss: 0.8449489021114877\n",
      "EarlyStopping counter: 7 out of 50\n",
      "epoch: 45 - train_acc: 0.9514563106796117 - train_loss: 0.12252245206732353 - val_acc: 0.8275862068965517 - val_loss: 0.7658629364275832\n",
      "EarlyStopping counter: 8 out of 50\n",
      "epoch: 46 - train_acc: 0.9587378640776699 - train_loss: 0.1216528502471545 - val_acc: 0.7586206896551724 - val_loss: 1.3084718704031595\n",
      "EarlyStopping counter: 9 out of 50\n",
      "epoch: 47 - train_acc: 0.9757281553398058 - train_loss: 0.089042921672459 - val_acc: 0.7931034482758621 - val_loss: 0.573220418070172\n",
      "EarlyStopping counter: 10 out of 50\n",
      "epoch: 48 - train_acc: 0.970873786407767 - train_loss: 0.08833105394937461 - val_acc: 0.7586206896551724 - val_loss: 0.6383448161966644\n",
      "EarlyStopping counter: 11 out of 50\n",
      "epoch: 49 - train_acc: 0.9757281553398058 - train_loss: 0.08622215120558165 - val_acc: 0.7586206896551724 - val_loss: 0.5325709346581998\n",
      "EarlyStopping counter: 12 out of 50\n",
      "epoch: 50 - train_acc: 0.9538834951456311 - train_loss: 0.12343305269505064 - val_acc: 0.6896551724137931 - val_loss: 1.3984928028153658\n",
      "EarlyStopping counter: 13 out of 50\n",
      "epoch: 51 - train_acc: 0.9660194174757282 - train_loss: 0.10535453592251012 - val_acc: 0.7586206896551724 - val_loss: 0.7103900568847732\n",
      "EarlyStopping counter: 14 out of 50\n",
      "epoch: 52 - train_acc: 0.970873786407767 - train_loss: 0.08980699982369658 - val_acc: 0.8275862068965517 - val_loss: 0.4930797697698415\n",
      "EarlyStopping counter: 15 out of 50\n",
      "epoch: 53 - train_acc: 0.9635922330097088 - train_loss: 0.09214520753703752 - val_acc: 0.8620689655172413 - val_loss: 0.4941448187675175\n",
      "EarlyStopping counter: 16 out of 50\n",
      "epoch: 54 - train_acc: 0.9757281553398058 - train_loss: 0.07439981228204476 - val_acc: 0.7586206896551724 - val_loss: 0.6503049013968111\n",
      "EarlyStopping counter: 17 out of 50\n",
      "epoch: 55 - train_acc: 0.9684466019417476 - train_loss: 0.08395418249843203 - val_acc: 0.7931034482758621 - val_loss: 0.5579884724934837\n",
      "EarlyStopping counter: 18 out of 50\n",
      "epoch: 56 - train_acc: 0.9563106796116505 - train_loss: 0.10668295879127154 - val_acc: 0.7931034482758621 - val_loss: 0.6773879332315451\n",
      "EarlyStopping counter: 19 out of 50\n",
      "epoch: 57 - train_acc: 0.9344660194174758 - train_loss: 0.13543431125012742 - val_acc: 0.6551724137931034 - val_loss: 1.0889432785974844\n",
      "EarlyStopping counter: 20 out of 50\n",
      "epoch: 58 - train_acc: 0.9635922330097088 - train_loss: 0.09307442569256896 - val_acc: 0.8275862068965517 - val_loss: 0.5627278500373257\n",
      "EarlyStopping counter: 21 out of 50\n",
      "epoch: 59 - train_acc: 0.9296116504854369 - train_loss: 0.1705000125458327 - val_acc: 0.8275862068965517 - val_loss: 0.6135166091539874\n",
      "EarlyStopping counter: 22 out of 50\n",
      "epoch: 60 - train_acc: 0.9393203883495146 - train_loss: 0.15023517714818335 - val_acc: 0.8275862068965517 - val_loss: 0.6605404997015827\n",
      "EarlyStopping counter: 23 out of 50\n",
      "epoch: 61 - train_acc: 0.9781553398058253 - train_loss: 0.08350251528137828 - val_acc: 0.7586206896551724 - val_loss: 0.6648820211222622\n",
      "EarlyStopping counter: 24 out of 50\n",
      "epoch: 62 - train_acc: 0.9757281553398058 - train_loss: 0.07241109027425296 - val_acc: 0.7586206896551724 - val_loss: 0.2913895777095729\n",
      "EarlyStopping counter: 25 out of 50\n",
      "epoch: 63 - train_acc: 0.9635922330097088 - train_loss: 0.09590445666510125 - val_acc: 0.8620689655172413 - val_loss: 0.9777334099740838\n",
      "EarlyStopping counter: 26 out of 50\n",
      "epoch: 64 - train_acc: 0.9490291262135923 - train_loss: 0.138734860484417 - val_acc: 0.7931034482758621 - val_loss: 0.42350559046595765\n",
      "EarlyStopping counter: 27 out of 50\n",
      "epoch: 65 - train_acc: 0.9902912621359223 - train_loss: 0.04869637361650846 - val_acc: 0.7586206896551724 - val_loss: 1.093185343934638\n",
      "EarlyStopping counter: 28 out of 50\n",
      "epoch: 66 - train_acc: 0.9514563106796117 - train_loss: 0.1527292536670223 - val_acc: 0.7241379310344828 - val_loss: 1.1611497142212976\n",
      "EarlyStopping counter: 29 out of 50\n",
      "epoch: 67 - train_acc: 0.9660194174757282 - train_loss: 0.08547054096252826 - val_acc: 0.7586206896551724 - val_loss: 0.5819354156865579\n",
      "EarlyStopping counter: 30 out of 50\n",
      "epoch: 68 - train_acc: 0.9902912621359223 - train_loss: 0.06533744215966757 - val_acc: 0.8275862068965517 - val_loss: 0.9325359247113376\n",
      "EarlyStopping counter: 31 out of 50\n",
      "epoch: 69 - train_acc: 0.9733009708737864 - train_loss: 0.0834648661766739 - val_acc: 0.7241379310344828 - val_loss: 0.8678470032041703\n",
      "EarlyStopping counter: 32 out of 50\n",
      "epoch: 70 - train_acc: 0.9781553398058253 - train_loss: 0.0786771796803777 - val_acc: 0.7931034482758621 - val_loss: 0.5891677803685136\n",
      "EarlyStopping counter: 33 out of 50\n",
      "epoch: 71 - train_acc: 0.9781553398058253 - train_loss: 0.07377082328834977 - val_acc: 0.7586206896551724 - val_loss: 0.6445224712394912\n",
      "EarlyStopping counter: 34 out of 50\n",
      "epoch: 72 - train_acc: 0.9975728155339806 - train_loss: 0.03435871619080494 - val_acc: 0.8620689655172413 - val_loss: 1.062800040853967\n",
      "EarlyStopping counter: 35 out of 50\n",
      "epoch: 73 - train_acc: 0.970873786407767 - train_loss: 0.08834463169124006 - val_acc: 0.6551724137931034 - val_loss: 1.141578096069605\n",
      "EarlyStopping counter: 36 out of 50\n",
      "epoch: 74 - train_acc: 0.9781553398058253 - train_loss: 0.05443945202274611 - val_acc: 0.7586206896551724 - val_loss: 0.7173756479798964\n",
      "EarlyStopping counter: 37 out of 50\n",
      "epoch: 75 - train_acc: 0.9854368932038835 - train_loss: 0.06641576816926499 - val_acc: 0.6551724137931034 - val_loss: 0.7505667461704002\n",
      "EarlyStopping counter: 38 out of 50\n",
      "epoch: 76 - train_acc: 0.9830097087378641 - train_loss: 0.04958279933558465 - val_acc: 0.7241379310344828 - val_loss: 0.6560603045316511\n",
      "EarlyStopping counter: 39 out of 50\n",
      "epoch: 77 - train_acc: 0.9902912621359223 - train_loss: 0.03590194575056239 - val_acc: 0.6896551724137931 - val_loss: 0.8126671557136194\n",
      "EarlyStopping counter: 40 out of 50\n",
      "epoch: 78 - train_acc: 0.9805825242718447 - train_loss: 0.05388456006882764 - val_acc: 0.7586206896551724 - val_loss: 1.8758957306138584\n",
      "EarlyStopping counter: 41 out of 50\n",
      "epoch: 79 - train_acc: 0.9951456310679612 - train_loss: 0.03782222610323739 - val_acc: 0.8275862068965517 - val_loss: 0.5914645863097716\n",
      "EarlyStopping counter: 42 out of 50\n",
      "epoch: 80 - train_acc: 0.970873786407767 - train_loss: 0.0703542578918089 - val_acc: 0.8275862068965517 - val_loss: 0.6451584781130945\n",
      "EarlyStopping counter: 43 out of 50\n",
      "epoch: 81 - train_acc: 0.9660194174757282 - train_loss: 0.11065365064493361 - val_acc: 0.7241379310344828 - val_loss: 1.4018078304336057\n",
      "EarlyStopping counter: 44 out of 50\n",
      "epoch: 82 - train_acc: 0.9223300970873787 - train_loss: 0.2315387606208125 - val_acc: 0.8275862068965517 - val_loss: 2.58305096646331\n",
      "EarlyStopping counter: 45 out of 50\n",
      "epoch: 83 - train_acc: 0.9805825242718447 - train_loss: 0.07531185961152365 - val_acc: 0.8275862068965517 - val_loss: 1.4749650279548392\n",
      "EarlyStopping counter: 46 out of 50\n",
      "epoch: 84 - train_acc: 0.9805825242718447 - train_loss: 0.07345741061093966 - val_acc: 0.7931034482758621 - val_loss: 1.1343607849942292\n",
      "EarlyStopping counter: 47 out of 50\n",
      "epoch: 85 - train_acc: 0.9733009708737864 - train_loss: 0.07653067435502593 - val_acc: 0.6896551724137931 - val_loss: 1.0128109892505883\n",
      "EarlyStopping counter: 48 out of 50\n",
      "epoch: 86 - train_acc: 0.9733009708737864 - train_loss: 0.07703404827711938 - val_acc: 0.7931034482758621 - val_loss: 0.6873207313602222\n",
      "EarlyStopping counter: 49 out of 50\n",
      "epoch: 87 - train_acc: 0.9757281553398058 - train_loss: 0.07305596853029311 - val_acc: 0.7241379310344828 - val_loss: 0.7789940309216393\n",
      "EarlyStopping counter: 50 out of 50\n",
      "epoch: 88 - train_acc: 0.9830097087378641 - train_loss: 0.048278155219080514 - val_acc: 0.8275862068965517 - val_loss: 0.7696479773836613\n",
      "=> Early stopped !!\n",
      "Accuracy: 0.7232142857142857\n",
      "Precision: 0.7758620689655172\n",
      "Recall: 0.7142857142857143\n",
      "F1: 0.743801652892562\n",
      "\n",
      "[Fold 8]: \n",
      "len(train_sampler)=412\n",
      "len(val_sampler)=29\n",
      "[ 11  38  86 112 129 164 179 183 219 228 233 248 250 253 289 298 299 312\n",
      " 317 331 333 358 370 371 389 410 412 420 424]\n",
      "Validation loss decreased (inf --> 0.853506).  Saving model ...\n",
      "epoch: 0 - train_acc: 0.5946601941747572 - train_loss: 0.6745496414727289 - val_acc: 0.4482758620689655 - val_loss: 0.8535059462330461\n",
      "Validation loss decreased (0.853506 --> 0.696775).  Saving model ...\n",
      "epoch: 1 - train_acc: 0.691747572815534 - train_loss: 0.5974496762780751 - val_acc: 0.5862068965517241 - val_loss: 0.6967747070170462\n",
      "Validation loss decreased (0.696775 --> 0.542141).  Saving model ...\n",
      "epoch: 2 - train_acc: 0.6771844660194175 - train_loss: 0.5674231488445531 - val_acc: 0.6206896551724138 - val_loss: 0.5421414248430501\n",
      "Validation loss decreased (0.542141 --> 0.474384).  Saving model ...\n",
      "epoch: 3 - train_acc: 0.7233009708737864 - train_loss: 0.5224976121390118 - val_acc: 0.6551724137931034 - val_loss: 0.4743835766760663\n",
      "Validation loss decreased (0.474384 --> 0.417011).  Saving model ...\n",
      "epoch: 4 - train_acc: 0.7184466019417476 - train_loss: 0.5319125916424383 - val_acc: 0.7586206896551724 - val_loss: 0.4170112235236955\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 5 - train_acc: 0.7548543689320388 - train_loss: 0.49127874174254627 - val_acc: 0.5862068965517241 - val_loss: 0.688950809803801\n",
      "Validation loss decreased (0.417011 --> 0.369711).  Saving model ...\n",
      "epoch: 6 - train_acc: 0.7572815533980582 - train_loss: 0.4874194086077414 - val_acc: 0.8275862068965517 - val_loss: 0.3697114416443422\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 7 - train_acc: 0.7766990291262136 - train_loss: 0.44454515336755684 - val_acc: 0.7586206896551724 - val_loss: 0.4534165676349081\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 8 - train_acc: 0.7936893203883495 - train_loss: 0.41688775455393173 - val_acc: 0.6896551724137931 - val_loss: 0.5984916090110275\n",
      "EarlyStopping counter: 3 out of 50\n",
      "epoch: 9 - train_acc: 0.7815533980582524 - train_loss: 0.4500907079106452 - val_acc: 0.7931034482758621 - val_loss: 0.4986485202666824\n",
      "EarlyStopping counter: 4 out of 50\n",
      "epoch: 10 - train_acc: 0.7864077669902912 - train_loss: 0.4226767762461412 - val_acc: 0.6206896551724138 - val_loss: 0.5407743862650755\n",
      "EarlyStopping counter: 5 out of 50\n",
      "epoch: 11 - train_acc: 0.8276699029126213 - train_loss: 0.37309874845480123 - val_acc: 0.7241379310344828 - val_loss: 0.6118290893564975\n",
      "EarlyStopping counter: 6 out of 50\n",
      "epoch: 12 - train_acc: 0.8325242718446602 - train_loss: 0.3575969782009973 - val_acc: 0.6896551724137931 - val_loss: 0.4679035368990686\n",
      "EarlyStopping counter: 7 out of 50\n",
      "epoch: 13 - train_acc: 0.8567961165048543 - train_loss: 0.32446364149338563 - val_acc: 0.7241379310344828 - val_loss: 0.43693711059513346\n",
      "EarlyStopping counter: 8 out of 50\n",
      "epoch: 14 - train_acc: 0.8495145631067961 - train_loss: 0.3223009967553784 - val_acc: 0.5862068965517241 - val_loss: 0.5445641698580622\n",
      "EarlyStopping counter: 9 out of 50\n",
      "epoch: 15 - train_acc: 0.8446601941747572 - train_loss: 0.3520755709261939 - val_acc: 0.7931034482758621 - val_loss: 0.8929566135667079\n",
      "EarlyStopping counter: 10 out of 50\n",
      "epoch: 16 - train_acc: 0.837378640776699 - train_loss: 0.3814792860046735 - val_acc: 0.7931034482758621 - val_loss: 0.4515987932545859\n",
      "EarlyStopping counter: 11 out of 50\n",
      "epoch: 17 - train_acc: 0.8349514563106796 - train_loss: 0.3739230112107305 - val_acc: 0.8620689655172413 - val_loss: 0.3708401550952899\n",
      "EarlyStopping counter: 12 out of 50\n",
      "epoch: 18 - train_acc: 0.8737864077669902 - train_loss: 0.289719372103436 - val_acc: 0.6896551724137931 - val_loss: 0.515781732580419\n",
      "EarlyStopping counter: 13 out of 50\n",
      "epoch: 19 - train_acc: 0.8713592233009708 - train_loss: 0.2917176044653277 - val_acc: 0.6896551724137931 - val_loss: 0.46814699776572005\n",
      "EarlyStopping counter: 14 out of 50\n",
      "epoch: 20 - train_acc: 0.8859223300970874 - train_loss: 0.2759213582258766 - val_acc: 0.7931034482758621 - val_loss: 0.8552467500360035\n",
      "Validation loss decreased (0.369711 --> 0.240663).  Saving model ...\n",
      "epoch: 21 - train_acc: 0.8956310679611651 - train_loss: 0.23209232292590448 - val_acc: 0.7931034482758621 - val_loss: 0.2406628253653397\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 22 - train_acc: 0.8907766990291263 - train_loss: 0.23803816043452705 - val_acc: 0.7241379310344828 - val_loss: 0.3457711682833178\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 23 - train_acc: 0.883495145631068 - train_loss: 0.28105862853776004 - val_acc: 0.8275862068965517 - val_loss: 0.502558822909589\n",
      "EarlyStopping counter: 3 out of 50\n",
      "epoch: 24 - train_acc: 0.8956310679611651 - train_loss: 0.2588839971797986 - val_acc: 0.7586206896551724 - val_loss: 0.437662629480763\n",
      "EarlyStopping counter: 4 out of 50\n",
      "epoch: 25 - train_acc: 0.883495145631068 - train_loss: 0.26849847549291767 - val_acc: 0.7241379310344828 - val_loss: 0.5701161606147467\n",
      "EarlyStopping counter: 5 out of 50\n",
      "epoch: 26 - train_acc: 0.8883495145631068 - train_loss: 0.2500410513392555 - val_acc: 0.8275862068965517 - val_loss: 0.5065971972735147\n",
      "EarlyStopping counter: 6 out of 50\n",
      "epoch: 27 - train_acc: 0.9247572815533981 - train_loss: 0.21252648370539493 - val_acc: 0.8275862068965517 - val_loss: 0.43669145778067703\n",
      "EarlyStopping counter: 7 out of 50\n",
      "epoch: 28 - train_acc: 0.9174757281553398 - train_loss: 0.2239258617902258 - val_acc: 0.8275862068965517 - val_loss: 0.40736004222091976\n",
      "EarlyStopping counter: 8 out of 50\n",
      "epoch: 29 - train_acc: 0.9029126213592233 - train_loss: 0.2849360441420748 - val_acc: 0.8620689655172413 - val_loss: 0.35511405511302674\n",
      "EarlyStopping counter: 9 out of 50\n",
      "epoch: 30 - train_acc: 0.9004854368932039 - train_loss: 0.23714169445412486 - val_acc: 0.7241379310344828 - val_loss: 0.35386251224821114\n",
      "EarlyStopping counter: 10 out of 50\n",
      "epoch: 31 - train_acc: 0.9053398058252428 - train_loss: 0.22664549616768848 - val_acc: 0.7586206896551724 - val_loss: 0.6076757722298094\n",
      "EarlyStopping counter: 11 out of 50\n",
      "epoch: 32 - train_acc: 0.9199029126213593 - train_loss: 0.18526340556635415 - val_acc: 0.7241379310344828 - val_loss: 0.5087361508031503\n",
      "EarlyStopping counter: 12 out of 50\n",
      "epoch: 33 - train_acc: 0.9393203883495146 - train_loss: 0.15276206242323132 - val_acc: 0.7931034482758621 - val_loss: 0.3031062193357572\n",
      "EarlyStopping counter: 13 out of 50\n",
      "epoch: 34 - train_acc: 0.9393203883495146 - train_loss: 0.13838586691835988 - val_acc: 0.7586206896551724 - val_loss: 0.5423596667972563\n",
      "EarlyStopping counter: 14 out of 50\n",
      "epoch: 35 - train_acc: 0.9320388349514563 - train_loss: 0.16829450283039518 - val_acc: 0.7586206896551724 - val_loss: 0.6045742429516265\n",
      "EarlyStopping counter: 15 out of 50\n",
      "epoch: 36 - train_acc: 0.9223300970873787 - train_loss: 0.2436666094422876 - val_acc: 0.7586206896551724 - val_loss: 0.9889690930390108\n",
      "EarlyStopping counter: 16 out of 50\n",
      "epoch: 37 - train_acc: 0.9077669902912622 - train_loss: 0.20346205463366723 - val_acc: 0.8275862068965517 - val_loss: 0.38973665255580897\n",
      "EarlyStopping counter: 17 out of 50\n",
      "epoch: 38 - train_acc: 0.9271844660194175 - train_loss: 0.18189951148361075 - val_acc: 0.7241379310344828 - val_loss: 0.48604174097634906\n",
      "EarlyStopping counter: 18 out of 50\n",
      "epoch: 39 - train_acc: 0.941747572815534 - train_loss: 0.14551863102249893 - val_acc: 0.7241379310344828 - val_loss: 0.4022163109283362\n",
      "Validation loss decreased (0.240663 --> 0.212418).  Saving model ...\n",
      "epoch: 40 - train_acc: 0.9441747572815534 - train_loss: 0.13901726643837664 - val_acc: 0.8620689655172413 - val_loss: 0.21241791215842482\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 41 - train_acc: 0.970873786407767 - train_loss: 0.10600734537968381 - val_acc: 0.8275862068965517 - val_loss: 0.3341433951911027\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 42 - train_acc: 0.9538834951456311 - train_loss: 0.12068930962048381 - val_acc: 0.8275862068965517 - val_loss: 0.6614259425854867\n",
      "EarlyStopping counter: 3 out of 50\n",
      "epoch: 43 - train_acc: 0.9466019417475728 - train_loss: 0.1212394939174769 - val_acc: 0.8275862068965517 - val_loss: 0.40678928335315034\n",
      "EarlyStopping counter: 4 out of 50\n",
      "epoch: 44 - train_acc: 0.9393203883495146 - train_loss: 0.14466560263651765 - val_acc: 0.7586206896551724 - val_loss: 0.35587200855262197\n",
      "EarlyStopping counter: 5 out of 50\n",
      "epoch: 45 - train_acc: 0.9514563106796117 - train_loss: 0.11064470290269178 - val_acc: 0.7931034482758621 - val_loss: 0.6605807147690199\n",
      "EarlyStopping counter: 6 out of 50\n",
      "epoch: 46 - train_acc: 0.9490291262135923 - train_loss: 0.1419386890717407 - val_acc: 0.7586206896551724 - val_loss: 1.1416785150889068\n",
      "EarlyStopping counter: 7 out of 50\n",
      "epoch: 47 - train_acc: 0.9199029126213593 - train_loss: 0.19442869153601552 - val_acc: 0.7931034482758621 - val_loss: 0.5341287218996744\n",
      "EarlyStopping counter: 8 out of 50\n",
      "epoch: 48 - train_acc: 0.9150485436893204 - train_loss: 0.21507074265267961 - val_acc: 0.8620689655172413 - val_loss: 0.6678523735426066\n",
      "EarlyStopping counter: 9 out of 50\n",
      "epoch: 49 - train_acc: 0.9174757281553398 - train_loss: 0.17091435185479606 - val_acc: 0.8275862068965517 - val_loss: 0.36289272314934035\n",
      "EarlyStopping counter: 10 out of 50\n",
      "epoch: 50 - train_acc: 0.9587378640776699 - train_loss: 0.11738589462464938 - val_acc: 0.7241379310344828 - val_loss: 0.49930783639922727\n",
      "EarlyStopping counter: 11 out of 50\n",
      "epoch: 51 - train_acc: 0.9660194174757282 - train_loss: 0.09649661311154899 - val_acc: 0.8620689655172413 - val_loss: 0.33906582152862663\n",
      "EarlyStopping counter: 12 out of 50\n",
      "epoch: 52 - train_acc: 0.9344660194174758 - train_loss: 0.14980662607673303 - val_acc: 0.8620689655172413 - val_loss: 0.5467636737772776\n",
      "EarlyStopping counter: 13 out of 50\n",
      "epoch: 53 - train_acc: 0.9660194174757282 - train_loss: 0.11480932789922102 - val_acc: 0.8275862068965517 - val_loss: 0.39885079127142675\n",
      "EarlyStopping counter: 14 out of 50\n",
      "epoch: 54 - train_acc: 0.9368932038834952 - train_loss: 0.16997058134423865 - val_acc: 0.7931034482758621 - val_loss: 0.801367471638415\n",
      "EarlyStopping counter: 15 out of 50\n",
      "epoch: 55 - train_acc: 0.9393203883495146 - train_loss: 0.15309154712985193 - val_acc: 0.7931034482758621 - val_loss: 0.5049298877502854\n",
      "EarlyStopping counter: 16 out of 50\n",
      "epoch: 56 - train_acc: 0.9441747572815534 - train_loss: 0.13501755552112624 - val_acc: 0.8620689655172413 - val_loss: 0.3304737343013076\n",
      "EarlyStopping counter: 17 out of 50\n",
      "epoch: 57 - train_acc: 0.941747572815534 - train_loss: 0.14543369892859334 - val_acc: 0.6896551724137931 - val_loss: 0.9353736334905747\n",
      "EarlyStopping counter: 18 out of 50\n",
      "epoch: 58 - train_acc: 0.9490291262135923 - train_loss: 0.1290270384009862 - val_acc: 0.8275862068965517 - val_loss: 1.1865980141574448\n",
      "EarlyStopping counter: 19 out of 50\n",
      "epoch: 59 - train_acc: 0.9684466019417476 - train_loss: 0.10556685673471751 - val_acc: 0.8620689655172413 - val_loss: 0.8822644145805298\n",
      "EarlyStopping counter: 20 out of 50\n",
      "epoch: 60 - train_acc: 0.9757281553398058 - train_loss: 0.07918157142345639 - val_acc: 0.7931034482758621 - val_loss: 0.2593995538691519\n",
      "EarlyStopping counter: 21 out of 50\n",
      "epoch: 61 - train_acc: 0.9854368932038835 - train_loss: 0.04956277445857146 - val_acc: 0.8275862068965517 - val_loss: 0.2687666971382881\n",
      "EarlyStopping counter: 22 out of 50\n",
      "epoch: 62 - train_acc: 0.9854368932038835 - train_loss: 0.045562577932365664 - val_acc: 0.8275862068965517 - val_loss: 0.47351779668901445\n",
      "EarlyStopping counter: 23 out of 50\n",
      "epoch: 63 - train_acc: 0.9781553398058253 - train_loss: 0.06818029479625541 - val_acc: 0.8275862068965517 - val_loss: 1.1308833783931558\n",
      "EarlyStopping counter: 24 out of 50\n",
      "epoch: 64 - train_acc: 0.970873786407767 - train_loss: 0.06863999049460151 - val_acc: 0.7931034482758621 - val_loss: 0.502122989055355\n",
      "EarlyStopping counter: 25 out of 50\n",
      "epoch: 65 - train_acc: 0.9660194174757282 - train_loss: 0.10265948209346867 - val_acc: 0.896551724137931 - val_loss: 0.5158216567229454\n",
      "EarlyStopping counter: 26 out of 50\n",
      "epoch: 66 - train_acc: 0.9441747572815534 - train_loss: 0.141459849990071 - val_acc: 0.7241379310344828 - val_loss: 0.4277906218813077\n",
      "EarlyStopping counter: 27 out of 50\n",
      "epoch: 67 - train_acc: 0.9757281553398058 - train_loss: 0.06528963074086774 - val_acc: 0.8620689655172413 - val_loss: 0.38548299482367676\n",
      "EarlyStopping counter: 28 out of 50\n",
      "epoch: 68 - train_acc: 0.9757281553398058 - train_loss: 0.06201862576004953 - val_acc: 0.8620689655172413 - val_loss: 0.4034004356567548\n",
      "EarlyStopping counter: 29 out of 50\n",
      "epoch: 69 - train_acc: 0.9805825242718447 - train_loss: 0.056244248724715944 - val_acc: 0.8275862068965517 - val_loss: 0.31927700967502454\n",
      "EarlyStopping counter: 30 out of 50\n",
      "epoch: 70 - train_acc: 0.9854368932038835 - train_loss: 0.05462314688740888 - val_acc: 0.7931034482758621 - val_loss: 0.41165492463083625\n",
      "EarlyStopping counter: 31 out of 50\n",
      "epoch: 71 - train_acc: 0.9660194174757282 - train_loss: 0.07275271604862218 - val_acc: 0.9310344827586207 - val_loss: 0.2764942877014369\n",
      "EarlyStopping counter: 32 out of 50\n",
      "epoch: 72 - train_acc: 0.9757281553398058 - train_loss: 0.0666707850751216 - val_acc: 0.896551724137931 - val_loss: 0.4684491781404049\n",
      "EarlyStopping counter: 33 out of 50\n",
      "epoch: 73 - train_acc: 0.9878640776699029 - train_loss: 0.04286432956296013 - val_acc: 0.9310344827586207 - val_loss: 0.27190482969081764\n",
      "EarlyStopping counter: 34 out of 50\n",
      "epoch: 74 - train_acc: 0.9660194174757282 - train_loss: 0.1014594946984297 - val_acc: 0.7586206896551724 - val_loss: 0.49271094626585743\n",
      "EarlyStopping counter: 35 out of 50\n",
      "epoch: 75 - train_acc: 0.9538834951456311 - train_loss: 0.0993190349399946 - val_acc: 0.7931034482758621 - val_loss: 0.4554848999364343\n",
      "EarlyStopping counter: 36 out of 50\n",
      "epoch: 76 - train_acc: 0.9635922330097088 - train_loss: 0.08996340041051687 - val_acc: 0.8275862068965517 - val_loss: 0.377977725561752\n",
      "EarlyStopping counter: 37 out of 50\n",
      "epoch: 77 - train_acc: 0.9757281553398058 - train_loss: 0.06620093701708352 - val_acc: 0.8275862068965517 - val_loss: 0.5406558562098017\n",
      "EarlyStopping counter: 38 out of 50\n",
      "epoch: 78 - train_acc: 0.9684466019417476 - train_loss: 0.11275338127065936 - val_acc: 0.7931034482758621 - val_loss: 0.8115653139508384\n",
      "EarlyStopping counter: 39 out of 50\n",
      "epoch: 79 - train_acc: 0.9757281553398058 - train_loss: 0.06108647432208943 - val_acc: 0.7586206896551724 - val_loss: 0.6008354273776131\n",
      "EarlyStopping counter: 40 out of 50\n",
      "epoch: 80 - train_acc: 0.9878640776699029 - train_loss: 0.04138574802839865 - val_acc: 0.8275862068965517 - val_loss: 0.6531597673991268\n",
      "EarlyStopping counter: 41 out of 50\n",
      "epoch: 81 - train_acc: 0.9854368932038835 - train_loss: 0.05897255669951722 - val_acc: 0.7931034482758621 - val_loss: 1.3859758607846917\n",
      "EarlyStopping counter: 42 out of 50\n",
      "epoch: 82 - train_acc: 0.9805825242718447 - train_loss: 0.07348852486259334 - val_acc: 0.8275862068965517 - val_loss: 0.43982992672130516\n",
      "EarlyStopping counter: 43 out of 50\n",
      "epoch: 83 - train_acc: 0.9296116504854369 - train_loss: 0.20136973712058412 - val_acc: 0.7586206896551724 - val_loss: 1.0492650607164522\n",
      "EarlyStopping counter: 44 out of 50\n",
      "epoch: 84 - train_acc: 0.9223300970873787 - train_loss: 0.17767016188344786 - val_acc: 0.8275862068965517 - val_loss: 0.4180005662181253\n",
      "EarlyStopping counter: 45 out of 50\n",
      "epoch: 85 - train_acc: 0.9320388349514563 - train_loss: 0.20397464423515427 - val_acc: 0.896551724137931 - val_loss: 0.36857047881039146\n",
      "EarlyStopping counter: 46 out of 50\n",
      "epoch: 86 - train_acc: 0.9611650485436893 - train_loss: 0.11377951440118898 - val_acc: 0.8620689655172413 - val_loss: 0.2259290363797512\n",
      "EarlyStopping counter: 47 out of 50\n",
      "epoch: 87 - train_acc: 0.9805825242718447 - train_loss: 0.07483816722776146 - val_acc: 0.8620689655172413 - val_loss: 0.9148201520649424\n",
      "EarlyStopping counter: 48 out of 50\n",
      "epoch: 88 - train_acc: 0.9951456310679612 - train_loss: 0.0378730241525073 - val_acc: 0.8275862068965517 - val_loss: 0.2677477796558145\n",
      "EarlyStopping counter: 49 out of 50\n",
      "epoch: 89 - train_acc: 0.9830097087378641 - train_loss: 0.04688097464825673 - val_acc: 0.8620689655172413 - val_loss: 0.5865377431050801\n",
      "EarlyStopping counter: 50 out of 50\n",
      "epoch: 90 - train_acc: 0.9878640776699029 - train_loss: 0.043633991808331114 - val_acc: 0.8275862068965517 - val_loss: 0.6013483566481311\n",
      "=> Early stopped !!\n",
      "Accuracy: 0.7366071428571429\n",
      "Precision: 0.7557251908396947\n",
      "Recall: 0.7857142857142857\n",
      "F1: 0.7704280155642023\n",
      "\n",
      "[Fold 9]: \n",
      "len(train_sampler)=412\n",
      "len(val_sampler)=29\n",
      "[ 12  28  35  44  65  85 107 115 120 127 133 136 142 159 170 186 197 224\n",
      " 232 242 258 283 296 321 327 351 355 364 405]\n",
      "Validation loss decreased (inf --> 0.640580).  Saving model ...\n",
      "epoch: 0 - train_acc: 0.6213592233009708 - train_loss: 0.6638899738570856 - val_acc: 0.6896551724137931 - val_loss: 0.6405800934343419\n",
      "Validation loss decreased (0.640580 --> 0.570530).  Saving model ...\n",
      "epoch: 1 - train_acc: 0.6456310679611651 - train_loss: 0.6338654646314738 - val_acc: 0.6896551724137931 - val_loss: 0.5705296278976505\n",
      "Validation loss decreased (0.570530 --> 0.509530).  Saving model ...\n",
      "epoch: 2 - train_acc: 0.6456310679611651 - train_loss: 0.6018148819496378 - val_acc: 0.6551724137931034 - val_loss: 0.50953024182344\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 3 - train_acc: 0.6941747572815534 - train_loss: 0.5689360004573019 - val_acc: 0.6206896551724138 - val_loss: 0.5144203956305833\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 4 - train_acc: 0.7621359223300971 - train_loss: 0.4866799853865092 - val_acc: 0.6551724137931034 - val_loss: 0.7231560944481932\n",
      "Validation loss decreased (0.509530 --> 0.506765).  Saving model ...\n",
      "epoch: 5 - train_acc: 0.7572815533980582 - train_loss: 0.49186159299328797 - val_acc: 0.6896551724137931 - val_loss: 0.506765393925344\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 6 - train_acc: 0.7912621359223301 - train_loss: 0.465185948695698 - val_acc: 0.7586206896551724 - val_loss: 0.5741839597640817\n",
      "Validation loss decreased (0.506765 --> 0.400643).  Saving model ...\n",
      "epoch: 7 - train_acc: 0.7669902912621359 - train_loss: 0.49167861899611237 - val_acc: 0.7241379310344828 - val_loss: 0.4006434845365076\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 8 - train_acc: 0.7985436893203883 - train_loss: 0.4218303910069929 - val_acc: 0.7241379310344828 - val_loss: 0.4794640650651013\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 9 - train_acc: 0.8058252427184466 - train_loss: 0.4111165797990133 - val_acc: 0.7241379310344828 - val_loss: 0.555508866954098\n",
      "EarlyStopping counter: 3 out of 50\n",
      "epoch: 10 - train_acc: 0.8300970873786407 - train_loss: 0.3727427190835052 - val_acc: 0.7586206896551724 - val_loss: 0.49719259132851135\n",
      "Validation loss decreased (0.400643 --> 0.396616).  Saving model ...\n",
      "epoch: 11 - train_acc: 0.8422330097087378 - train_loss: 0.3523471850895259 - val_acc: 0.8620689655172413 - val_loss: 0.39661621493053056\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 12 - train_acc: 0.8349514563106796 - train_loss: 0.33236649136704216 - val_acc: 0.8620689655172413 - val_loss: 0.41572182286743825\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 13 - train_acc: 0.8349514563106796 - train_loss: 0.3447114757123192 - val_acc: 0.7241379310344828 - val_loss: 0.4022548573111768\n",
      "Validation loss decreased (0.396616 --> 0.389027).  Saving model ...\n",
      "epoch: 14 - train_acc: 0.8325242718446602 - train_loss: 0.37163373020820106 - val_acc: 0.7586206896551724 - val_loss: 0.38902722158240377\n",
      "Validation loss decreased (0.389027 --> 0.227894).  Saving model ...\n",
      "epoch: 15 - train_acc: 0.8398058252427184 - train_loss: 0.3292244056584507 - val_acc: 0.8275862068965517 - val_loss: 0.2278942331334992\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 16 - train_acc: 0.8956310679611651 - train_loss: 0.27229445583964645 - val_acc: 0.8620689655172413 - val_loss: 0.24585532347028563\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 17 - train_acc: 0.8640776699029126 - train_loss: 0.2981851106427347 - val_acc: 0.7931034482758621 - val_loss: 0.674042271707912\n",
      "EarlyStopping counter: 3 out of 50\n",
      "epoch: 18 - train_acc: 0.9029126213592233 - train_loss: 0.25336735916901276 - val_acc: 0.8620689655172413 - val_loss: 0.26666200443705573\n",
      "EarlyStopping counter: 4 out of 50\n",
      "epoch: 19 - train_acc: 0.9077669902912622 - train_loss: 0.22985248608637712 - val_acc: 0.896551724137931 - val_loss: 0.289736338429291\n",
      "EarlyStopping counter: 5 out of 50\n",
      "epoch: 20 - train_acc: 0.8640776699029126 - train_loss: 0.24919395604372535 - val_acc: 0.8620689655172413 - val_loss: 0.44919277689394255\n",
      "EarlyStopping counter: 6 out of 50\n",
      "epoch: 21 - train_acc: 0.8956310679611651 - train_loss: 0.23231611622538953 - val_acc: 0.8620689655172413 - val_loss: 0.26292796645734845\n",
      "EarlyStopping counter: 7 out of 50\n",
      "epoch: 22 - train_acc: 0.8810679611650486 - train_loss: 0.2533082595779009 - val_acc: 0.7931034482758621 - val_loss: 0.2468300106666756\n",
      "EarlyStopping counter: 8 out of 50\n",
      "epoch: 23 - train_acc: 0.8980582524271845 - train_loss: 0.2392761627320984 - val_acc: 0.896551724137931 - val_loss: 0.2826884819101423\n",
      "EarlyStopping counter: 9 out of 50\n",
      "epoch: 24 - train_acc: 0.8980582524271845 - train_loss: 0.21161087154040042 - val_acc: 0.8275862068965517 - val_loss: 0.35920948553519216\n",
      "EarlyStopping counter: 10 out of 50\n",
      "epoch: 25 - train_acc: 0.912621359223301 - train_loss: 0.22248936093980615 - val_acc: 0.7931034482758621 - val_loss: 0.2575939375748531\n",
      "EarlyStopping counter: 11 out of 50\n",
      "epoch: 26 - train_acc: 0.9174757281553398 - train_loss: 0.23146649093031352 - val_acc: 0.896551724137931 - val_loss: 0.2435927944183\n",
      "EarlyStopping counter: 12 out of 50\n",
      "epoch: 27 - train_acc: 0.9393203883495146 - train_loss: 0.18098112786758233 - val_acc: 0.8275862068965517 - val_loss: 0.2556022865662639\n",
      "EarlyStopping counter: 13 out of 50\n",
      "epoch: 28 - train_acc: 0.9320388349514563 - train_loss: 0.16406792640081097 - val_acc: 0.7931034482758621 - val_loss: 0.6940202254124836\n",
      "EarlyStopping counter: 14 out of 50\n",
      "epoch: 29 - train_acc: 0.9247572815533981 - train_loss: 0.16856361248997137 - val_acc: 0.7241379310344828 - val_loss: 0.6084434597499231\n",
      "EarlyStopping counter: 15 out of 50\n",
      "epoch: 30 - train_acc: 0.8980582524271845 - train_loss: 0.2365228439818845 - val_acc: 0.896551724137931 - val_loss: 0.2691951218601387\n",
      "EarlyStopping counter: 16 out of 50\n",
      "epoch: 31 - train_acc: 0.9344660194174758 - train_loss: 0.18266612250166675 - val_acc: 0.896551724137931 - val_loss: 0.2615210212373626\n",
      "EarlyStopping counter: 17 out of 50\n",
      "epoch: 32 - train_acc: 0.9296116504854369 - train_loss: 0.18589829666946883 - val_acc: 0.8620689655172413 - val_loss: 0.31997062696266254\n",
      "EarlyStopping counter: 18 out of 50\n",
      "epoch: 33 - train_acc: 0.9393203883495146 - train_loss: 0.16230929975899705 - val_acc: 0.896551724137931 - val_loss: 0.8218572030958778\n",
      "Validation loss decreased (0.227894 --> 0.169134).  Saving model ...\n",
      "epoch: 34 - train_acc: 0.9271844660194175 - train_loss: 0.1587258730004862 - val_acc: 0.8620689655172413 - val_loss: 0.16913437529820632\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 35 - train_acc: 0.9514563106796117 - train_loss: 0.1287685203068598 - val_acc: 0.9310344827586207 - val_loss: 0.3145518863485357\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 36 - train_acc: 0.9538834951456311 - train_loss: 0.1380886292303198 - val_acc: 0.8620689655172413 - val_loss: 0.3644569764275869\n",
      "EarlyStopping counter: 3 out of 50\n",
      "epoch: 37 - train_acc: 0.9223300970873787 - train_loss: 0.19016073995808883 - val_acc: 0.8620689655172413 - val_loss: 0.30814350906264265\n",
      "EarlyStopping counter: 4 out of 50\n",
      "epoch: 38 - train_acc: 0.941747572815534 - train_loss: 0.15582980934396984 - val_acc: 0.896551724137931 - val_loss: 0.26445606443462305\n",
      "EarlyStopping counter: 5 out of 50\n",
      "epoch: 39 - train_acc: 0.9441747572815534 - train_loss: 0.13708722095988976 - val_acc: 0.7586206896551724 - val_loss: 0.4563067268109262\n",
      "EarlyStopping counter: 6 out of 50\n",
      "epoch: 40 - train_acc: 0.9344660194174758 - train_loss: 0.17020486639357277 - val_acc: 0.8275862068965517 - val_loss: 0.2573796538984998\n",
      "EarlyStopping counter: 7 out of 50\n",
      "epoch: 41 - train_acc: 0.9563106796116505 - train_loss: 0.12222466324401908 - val_acc: 0.7586206896551724 - val_loss: 0.41743948417583343\n",
      "EarlyStopping counter: 8 out of 50\n",
      "epoch: 42 - train_acc: 0.9781553398058253 - train_loss: 0.09591715824937663 - val_acc: 0.8620689655172413 - val_loss: 0.5628569868661529\n",
      "EarlyStopping counter: 9 out of 50\n",
      "epoch: 43 - train_acc: 0.9611650485436893 - train_loss: 0.11018655996425473 - val_acc: 0.896551724137931 - val_loss: 0.35198682806944404\n",
      "EarlyStopping counter: 10 out of 50\n",
      "epoch: 44 - train_acc: 0.970873786407767 - train_loss: 0.09536493298613527 - val_acc: 0.8620689655172413 - val_loss: 0.6445284053290938\n",
      "EarlyStopping counter: 11 out of 50\n",
      "epoch: 45 - train_acc: 0.941747572815534 - train_loss: 0.15799713670776794 - val_acc: 0.896551724137931 - val_loss: 0.5856150223578773\n",
      "EarlyStopping counter: 12 out of 50\n",
      "epoch: 46 - train_acc: 0.9490291262135923 - train_loss: 0.15742824388192725 - val_acc: 0.8275862068965517 - val_loss: 0.584359604782036\n",
      "EarlyStopping counter: 13 out of 50\n",
      "epoch: 47 - train_acc: 0.9393203883495146 - train_loss: 0.14050892261561854 - val_acc: 0.8275862068965517 - val_loss: 0.4561220173965975\n",
      "EarlyStopping counter: 14 out of 50\n",
      "epoch: 48 - train_acc: 0.9490291262135923 - train_loss: 0.11531698068192935 - val_acc: 0.8620689655172413 - val_loss: 0.30602505800430263\n",
      "EarlyStopping counter: 15 out of 50\n",
      "epoch: 49 - train_acc: 0.9393203883495146 - train_loss: 0.15566603748080154 - val_acc: 0.7931034482758621 - val_loss: 0.4296780650265116\n",
      "EarlyStopping counter: 16 out of 50\n",
      "epoch: 50 - train_acc: 0.941747572815534 - train_loss: 0.14951794041517952 - val_acc: 0.896551724137931 - val_loss: 0.439976775414921\n",
      "EarlyStopping counter: 17 out of 50\n",
      "epoch: 51 - train_acc: 0.9733009708737864 - train_loss: 0.08408076104174539 - val_acc: 0.896551724137931 - val_loss: 0.23239365963876585\n",
      "EarlyStopping counter: 18 out of 50\n",
      "epoch: 52 - train_acc: 0.9805825242718447 - train_loss: 0.06892351795357678 - val_acc: 0.9310344827586207 - val_loss: 0.2393165646800687\n",
      "EarlyStopping counter: 19 out of 50\n",
      "epoch: 53 - train_acc: 0.9757281553398058 - train_loss: 0.0748409364728112 - val_acc: 0.896551724137931 - val_loss: 0.6050444115339066\n",
      "EarlyStopping counter: 20 out of 50\n",
      "epoch: 54 - train_acc: 0.9538834951456311 - train_loss: 0.13228041357599804 - val_acc: 0.8275862068965517 - val_loss: 0.27702756435748543\n",
      "EarlyStopping counter: 21 out of 50\n",
      "epoch: 55 - train_acc: 0.9368932038834952 - train_loss: 0.1842642061279003 - val_acc: 0.896551724137931 - val_loss: 0.3553847617958209\n",
      "EarlyStopping counter: 22 out of 50\n",
      "epoch: 56 - train_acc: 0.9635922330097088 - train_loss: 0.09731987717380026 - val_acc: 0.8620689655172413 - val_loss: 0.38912618811833766\n",
      "EarlyStopping counter: 23 out of 50\n",
      "epoch: 57 - train_acc: 0.9296116504854369 - train_loss: 0.15684466808734474 - val_acc: 0.896551724137931 - val_loss: 0.43442955864596466\n",
      "EarlyStopping counter: 24 out of 50\n",
      "epoch: 58 - train_acc: 0.9635922330097088 - train_loss: 0.1313603487187914 - val_acc: 0.896551724137931 - val_loss: 0.2796508245655328\n",
      "EarlyStopping counter: 25 out of 50\n",
      "epoch: 59 - train_acc: 0.9660194174757282 - train_loss: 0.09524126904602978 - val_acc: 0.7931034482758621 - val_loss: 0.4617674467246863\n",
      "EarlyStopping counter: 26 out of 50\n",
      "epoch: 60 - train_acc: 0.9757281553398058 - train_loss: 0.07721697911336581 - val_acc: 0.896551724137931 - val_loss: 0.22931682797328826\n",
      "EarlyStopping counter: 27 out of 50\n",
      "epoch: 61 - train_acc: 0.9878640776699029 - train_loss: 0.051767561205063296 - val_acc: 0.896551724137931 - val_loss: 0.26659418541803026\n",
      "EarlyStopping counter: 28 out of 50\n",
      "epoch: 62 - train_acc: 0.9902912621359223 - train_loss: 0.04656656003825846 - val_acc: 0.8275862068965517 - val_loss: 0.6301075794343963\n",
      "EarlyStopping counter: 29 out of 50\n",
      "epoch: 63 - train_acc: 0.9733009708737864 - train_loss: 0.08854706057161188 - val_acc: 0.9310344827586207 - val_loss: 0.17052796791687094\n",
      "EarlyStopping counter: 30 out of 50\n",
      "epoch: 64 - train_acc: 0.9635922330097088 - train_loss: 0.11093762580453795 - val_acc: 0.8620689655172413 - val_loss: 0.25025423124173485\n",
      "Validation loss decreased (0.169134 --> 0.140043).  Saving model ...\n",
      "epoch: 65 - train_acc: 0.941747572815534 - train_loss: 0.10190842231685586 - val_acc: 0.896551724137931 - val_loss: 0.14004252612634152\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 66 - train_acc: 0.9757281553398058 - train_loss: 0.106961469639987 - val_acc: 0.8275862068965517 - val_loss: 0.38807242210265613\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 67 - train_acc: 0.9660194174757282 - train_loss: 0.10046184546360915 - val_acc: 0.896551724137931 - val_loss: 0.22807526518059665\n",
      "EarlyStopping counter: 3 out of 50\n",
      "epoch: 68 - train_acc: 0.9563106796116505 - train_loss: 0.10950445248992303 - val_acc: 0.8620689655172413 - val_loss: 0.32403395109178323\n",
      "EarlyStopping counter: 4 out of 50\n",
      "epoch: 69 - train_acc: 0.9878640776699029 - train_loss: 0.0529120619820842 - val_acc: 0.9310344827586207 - val_loss: 0.19097761078604208\n",
      "EarlyStopping counter: 5 out of 50\n",
      "epoch: 70 - train_acc: 0.9854368932038835 - train_loss: 0.05554403694243726 - val_acc: 0.9310344827586207 - val_loss: 0.24983654085975865\n",
      "EarlyStopping counter: 6 out of 50\n",
      "epoch: 71 - train_acc: 0.9878640776699029 - train_loss: 0.0462485802941546 - val_acc: 0.9310344827586207 - val_loss: 0.6753843304392783\n",
      "EarlyStopping counter: 7 out of 50\n",
      "epoch: 72 - train_acc: 0.9878640776699029 - train_loss: 0.038475626318356076 - val_acc: 0.9310344827586207 - val_loss: 0.2578697263023094\n",
      "EarlyStopping counter: 8 out of 50\n",
      "epoch: 73 - train_acc: 0.9611650485436893 - train_loss: 0.07557840226664278 - val_acc: 0.896551724137931 - val_loss: 0.8521168220153656\n",
      "EarlyStopping counter: 9 out of 50\n",
      "epoch: 74 - train_acc: 0.9757281553398058 - train_loss: 0.08024543021517597 - val_acc: 0.8620689655172413 - val_loss: 0.2793394600379893\n",
      "EarlyStopping counter: 10 out of 50\n",
      "epoch: 75 - train_acc: 0.9684466019417476 - train_loss: 0.07426690965495879 - val_acc: 0.896551724137931 - val_loss: 0.39744122137838345\n",
      "EarlyStopping counter: 11 out of 50\n",
      "epoch: 76 - train_acc: 0.9684466019417476 - train_loss: 0.09032375197266326 - val_acc: 0.896551724137931 - val_loss: 0.1649451863166345\n",
      "EarlyStopping counter: 12 out of 50\n",
      "epoch: 77 - train_acc: 0.9781553398058253 - train_loss: 0.053858348809638135 - val_acc: 0.9310344827586207 - val_loss: 0.2019485357844433\n",
      "EarlyStopping counter: 13 out of 50\n",
      "epoch: 78 - train_acc: 0.9805825242718447 - train_loss: 0.05646685722974615 - val_acc: 0.896551724137931 - val_loss: 0.3103728798396036\n",
      "EarlyStopping counter: 14 out of 50\n",
      "epoch: 79 - train_acc: 0.9902912621359223 - train_loss: 0.04078885821121742 - val_acc: 0.9655172413793104 - val_loss: 0.18395706981444007\n",
      "EarlyStopping counter: 15 out of 50\n",
      "epoch: 80 - train_acc: 0.9878640776699029 - train_loss: 0.05345575769733471 - val_acc: 0.9310344827586207 - val_loss: 0.1968701699762172\n",
      "EarlyStopping counter: 16 out of 50\n",
      "epoch: 81 - train_acc: 0.9757281553398058 - train_loss: 0.06611168777264798 - val_acc: 0.896551724137931 - val_loss: 0.24198155636648022\n",
      "EarlyStopping counter: 17 out of 50\n",
      "epoch: 82 - train_acc: 0.9878640776699029 - train_loss: 0.034836403197108186 - val_acc: 0.896551724137931 - val_loss: 0.3370110912816614\n",
      "EarlyStopping counter: 18 out of 50\n",
      "epoch: 83 - train_acc: 0.9927184466019418 - train_loss: 0.0337357504000271 - val_acc: 0.9310344827586207 - val_loss: 0.3095321704894125\n",
      "EarlyStopping counter: 19 out of 50\n",
      "epoch: 84 - train_acc: 0.9781553398058253 - train_loss: 0.06872422055333022 - val_acc: 0.7931034482758621 - val_loss: 0.46074136496139617\n",
      "EarlyStopping counter: 20 out of 50\n",
      "epoch: 85 - train_acc: 0.9368932038834952 - train_loss: 0.1613056569992338 - val_acc: 0.896551724137931 - val_loss: 0.1507497977777499\n",
      "EarlyStopping counter: 21 out of 50\n",
      "epoch: 86 - train_acc: 0.9635922330097088 - train_loss: 0.1065099493002175 - val_acc: 0.8275862068965517 - val_loss: 0.6306254723032114\n",
      "EarlyStopping counter: 22 out of 50\n",
      "epoch: 87 - train_acc: 0.9854368932038835 - train_loss: 0.05392572604167534 - val_acc: 0.9310344827586207 - val_loss: 0.23149975983877574\n",
      "EarlyStopping counter: 23 out of 50\n",
      "epoch: 88 - train_acc: 0.9854368932038835 - train_loss: 0.05147547209760691 - val_acc: 0.896551724137931 - val_loss: 0.4944453558465901\n",
      "EarlyStopping counter: 24 out of 50\n",
      "epoch: 89 - train_acc: 0.9854368932038835 - train_loss: 0.05689612439761074 - val_acc: 0.9310344827586207 - val_loss: 0.3161821634796252\n",
      "EarlyStopping counter: 25 out of 50\n",
      "epoch: 90 - train_acc: 0.9854368932038835 - train_loss: 0.04337929221179395 - val_acc: 0.896551724137931 - val_loss: 0.3145092991871967\n",
      "EarlyStopping counter: 26 out of 50\n",
      "epoch: 91 - train_acc: 0.9781553398058253 - train_loss: 0.05686565221140071 - val_acc: 0.9310344827586207 - val_loss: 0.2878318320363832\n",
      "EarlyStopping counter: 27 out of 50\n",
      "epoch: 92 - train_acc: 0.9830097087378641 - train_loss: 0.041960779264033285 - val_acc: 0.9310344827586207 - val_loss: 0.2105274438807593\n",
      "EarlyStopping counter: 28 out of 50\n",
      "epoch: 93 - train_acc: 0.9830097087378641 - train_loss: 0.05933362583562892 - val_acc: 0.8275862068965517 - val_loss: 0.7451758266259028\n",
      "EarlyStopping counter: 29 out of 50\n",
      "epoch: 94 - train_acc: 0.9733009708737864 - train_loss: 0.08624798154881119 - val_acc: 0.896551724137931 - val_loss: 0.5029319289120634\n",
      "EarlyStopping counter: 30 out of 50\n",
      "epoch: 95 - train_acc: 0.9684466019417476 - train_loss: 0.09413011630808113 - val_acc: 0.8620689655172413 - val_loss: 0.38258838327328343\n",
      "EarlyStopping counter: 31 out of 50\n",
      "epoch: 96 - train_acc: 0.970873786407767 - train_loss: 0.07445789847404126 - val_acc: 0.896551724137931 - val_loss: 1.867316077349362\n",
      "EarlyStopping counter: 32 out of 50\n",
      "epoch: 97 - train_acc: 0.9878640776699029 - train_loss: 0.048345987997510845 - val_acc: 0.896551724137931 - val_loss: 0.2798972627044868\n",
      "EarlyStopping counter: 33 out of 50\n",
      "epoch: 98 - train_acc: 0.9733009708737864 - train_loss: 0.06607036171651368 - val_acc: 0.9310344827586207 - val_loss: 0.25185684259322677\n",
      "EarlyStopping counter: 34 out of 50\n",
      "epoch: 99 - train_acc: 0.9635922330097088 - train_loss: 0.13621280771865193 - val_acc: 0.8620689655172413 - val_loss: 0.4150642890914782\n",
      "EarlyStopping counter: 35 out of 50\n",
      "epoch: 100 - train_acc: 0.9975728155339806 - train_loss: 0.030826506305555817 - val_acc: 0.8620689655172413 - val_loss: 0.2601224826367131\n",
      "EarlyStopping counter: 36 out of 50\n",
      "epoch: 101 - train_acc: 0.9927184466019418 - train_loss: 0.032790260567196756 - val_acc: 0.9310344827586207 - val_loss: 0.29198060599348746\n",
      "EarlyStopping counter: 37 out of 50\n",
      "epoch: 102 - train_acc: 0.9830097087378641 - train_loss: 0.04568529599966769 - val_acc: 0.896551724137931 - val_loss: 0.43848785164645193\n",
      "EarlyStopping counter: 38 out of 50\n",
      "epoch: 103 - train_acc: 0.9805825242718447 - train_loss: 0.051504528126491665 - val_acc: 0.9310344827586207 - val_loss: 0.46340116857397473\n",
      "EarlyStopping counter: 39 out of 50\n",
      "epoch: 104 - train_acc: 0.9854368932038835 - train_loss: 0.0466501188165425 - val_acc: 0.896551724137931 - val_loss: 0.26484340254446703\n",
      "EarlyStopping counter: 40 out of 50\n",
      "epoch: 105 - train_acc: 0.9902912621359223 - train_loss: 0.03782273959992934 - val_acc: 0.896551724137931 - val_loss: 1.360387481122664\n",
      "EarlyStopping counter: 41 out of 50\n",
      "epoch: 106 - train_acc: 0.9951456310679612 - train_loss: 0.020709987990905034 - val_acc: 0.9310344827586207 - val_loss: 0.2159757478827779\n",
      "EarlyStopping counter: 42 out of 50\n",
      "epoch: 107 - train_acc: 0.9927184466019418 - train_loss: 0.025031346755605442 - val_acc: 0.9655172413793104 - val_loss: 0.21602719684272242\n",
      "EarlyStopping counter: 43 out of 50\n",
      "epoch: 108 - train_acc: 1.0 - train_loss: 0.009531793748378647 - val_acc: 0.9655172413793104 - val_loss: 0.8904874384928224\n",
      "EarlyStopping counter: 44 out of 50\n",
      "epoch: 109 - train_acc: 0.9951456310679612 - train_loss: 0.023570927198146276 - val_acc: 0.8620689655172413 - val_loss: 0.3806791869228364\n",
      "EarlyStopping counter: 45 out of 50\n",
      "epoch: 110 - train_acc: 0.9951456310679612 - train_loss: 0.0242759870014133 - val_acc: 0.9310344827586207 - val_loss: 0.25407369726470225\n",
      "EarlyStopping counter: 46 out of 50\n",
      "epoch: 111 - train_acc: 0.9975728155339806 - train_loss: 0.01538590521096836 - val_acc: 0.896551724137931 - val_loss: 0.2963960760140042\n",
      "EarlyStopping counter: 47 out of 50\n",
      "epoch: 112 - train_acc: 0.9927184466019418 - train_loss: 0.02213844202653526 - val_acc: 0.8275862068965517 - val_loss: 0.5908044432921957\n",
      "EarlyStopping counter: 48 out of 50\n",
      "epoch: 113 - train_acc: 0.970873786407767 - train_loss: 0.07967622487454087 - val_acc: 0.896551724137931 - val_loss: 0.23474141557021724\n",
      "EarlyStopping counter: 49 out of 50\n",
      "epoch: 114 - train_acc: 0.9805825242718447 - train_loss: 0.04287284188574516 - val_acc: 0.8620689655172413 - val_loss: 0.32260541825998984\n",
      "EarlyStopping counter: 50 out of 50\n",
      "epoch: 115 - train_acc: 0.9830097087378641 - train_loss: 0.04466973815451203 - val_acc: 0.8620689655172413 - val_loss: 0.7056430682513971\n",
      "=> Early stopped !!\n",
      "Accuracy: 0.7410714285714286\n",
      "Precision: 0.7266666666666667\n",
      "Recall: 0.8650793650793651\n",
      "F1: 0.7898550724637682\n",
      "\n",
      "[Fold 10]: \n",
      "len(train_sampler)=412\n",
      "len(val_sampler)=29\n",
      "[  4  32  41  47  51  61  95  98 100 171 178 206 213 215 221 226 240 254\n",
      " 256 267 282 292 301 329 335 342 365 376 395]\n",
      "Validation loss decreased (inf --> 0.682747).  Saving model ...\n",
      "epoch: 0 - train_acc: 0.5776699029126213 - train_loss: 0.6818883048860856 - val_acc: 0.5862068965517241 - val_loss: 0.6827468626564095\n",
      "Validation loss decreased (0.682747 --> 0.622406).  Saving model ...\n",
      "epoch: 1 - train_acc: 0.662621359223301 - train_loss: 0.6123518716889789 - val_acc: 0.6551724137931034 - val_loss: 0.6224064787442896\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 2 - train_acc: 0.7184466019417476 - train_loss: 0.570667238648036 - val_acc: 0.6551724137931034 - val_loss: 0.6981722412128135\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 3 - train_acc: 0.7354368932038835 - train_loss: 0.5320423952719263 - val_acc: 0.5517241379310345 - val_loss: 0.8594937218173115\n",
      "Validation loss decreased (0.622406 --> 0.535664).  Saving model ...\n",
      "epoch: 4 - train_acc: 0.7548543689320388 - train_loss: 0.527084052961293 - val_acc: 0.6551724137931034 - val_loss: 0.5356642675149317\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 5 - train_acc: 0.7864077669902912 - train_loss: 0.45001608891036515 - val_acc: 0.6206896551724138 - val_loss: 0.8957857565083714\n",
      "Validation loss decreased (0.535664 --> 0.521446).  Saving model ...\n",
      "epoch: 6 - train_acc: 0.7864077669902912 - train_loss: 0.4420698161863028 - val_acc: 0.6206896551724138 - val_loss: 0.5214458143329941\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 7 - train_acc: 0.779126213592233 - train_loss: 0.4208503906542562 - val_acc: 0.6206896551724138 - val_loss: 0.6032962224149266\n",
      "Validation loss decreased (0.521446 --> 0.430702).  Saving model ...\n",
      "epoch: 8 - train_acc: 0.8009708737864077 - train_loss: 0.4265560711984065 - val_acc: 0.6896551724137931 - val_loss: 0.43070211388461577\n",
      "Validation loss decreased (0.430702 --> 0.426952).  Saving model ...\n",
      "epoch: 9 - train_acc: 0.7961165048543689 - train_loss: 0.42507959331358136 - val_acc: 0.7241379310344828 - val_loss: 0.42695221743142775\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 10 - train_acc: 0.8203883495145631 - train_loss: 0.3834892954664238 - val_acc: 0.6551724137931034 - val_loss: 0.5496660697488742\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 11 - train_acc: 0.8422330097087378 - train_loss: 0.3635472003215237 - val_acc: 0.7241379310344828 - val_loss: 0.6661111356015788\n",
      "EarlyStopping counter: 3 out of 50\n",
      "epoch: 12 - train_acc: 0.866504854368932 - train_loss: 0.3263981006769509 - val_acc: 0.5517241379310345 - val_loss: 0.5182867242679715\n",
      "EarlyStopping counter: 4 out of 50\n",
      "epoch: 13 - train_acc: 0.8689320388349514 - train_loss: 0.30040308361152457 - val_acc: 0.7241379310344828 - val_loss: 0.6879728418076831\n",
      "EarlyStopping counter: 5 out of 50\n",
      "epoch: 14 - train_acc: 0.8737864077669902 - train_loss: 0.2786372232295 - val_acc: 0.6551724137931034 - val_loss: 0.6983239204703107\n",
      "Validation loss decreased (0.426952 --> 0.389628).  Saving model ...\n",
      "epoch: 15 - train_acc: 0.8762135922330098 - train_loss: 0.28476062018588233 - val_acc: 0.8275862068965517 - val_loss: 0.3896280433891082\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 16 - train_acc: 0.8810679611650486 - train_loss: 0.28706193357033943 - val_acc: 0.7586206896551724 - val_loss: 0.6930495951824296\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 17 - train_acc: 0.8762135922330098 - train_loss: 0.2920867604999368 - val_acc: 0.7241379310344828 - val_loss: 0.7313468892897743\n",
      "EarlyStopping counter: 3 out of 50\n",
      "epoch: 18 - train_acc: 0.8567961165048543 - train_loss: 0.3073950893339309 - val_acc: 0.6896551724137931 - val_loss: 0.5401849854798706\n",
      "EarlyStopping counter: 4 out of 50\n",
      "epoch: 19 - train_acc: 0.8980582524271845 - train_loss: 0.2476188564384132 - val_acc: 0.6896551724137931 - val_loss: 0.6378384556157445\n",
      "EarlyStopping counter: 5 out of 50\n",
      "epoch: 20 - train_acc: 0.9053398058252428 - train_loss: 0.21870725542327554 - val_acc: 0.7931034482758621 - val_loss: 0.6514259130690957\n",
      "EarlyStopping counter: 6 out of 50\n",
      "epoch: 21 - train_acc: 0.8907766990291263 - train_loss: 0.24470469226657265 - val_acc: 0.8275862068965517 - val_loss: 0.43508763604188694\n",
      "EarlyStopping counter: 7 out of 50\n",
      "epoch: 22 - train_acc: 0.8883495145631068 - train_loss: 0.2567954157895198 - val_acc: 0.7241379310344828 - val_loss: 0.44631746821596824\n",
      "EarlyStopping counter: 8 out of 50\n",
      "epoch: 23 - train_acc: 0.9004854368932039 - train_loss: 0.20324550614991718 - val_acc: 0.6551724137931034 - val_loss: 0.8276547868215249\n",
      "EarlyStopping counter: 9 out of 50\n",
      "epoch: 24 - train_acc: 0.9150485436893204 - train_loss: 0.20341429933753624 - val_acc: 0.5517241379310345 - val_loss: 0.8036799937445311\n",
      "EarlyStopping counter: 10 out of 50\n",
      "epoch: 25 - train_acc: 0.9247572815533981 - train_loss: 0.18599148932423312 - val_acc: 0.7931034482758621 - val_loss: 0.5233980717387874\n",
      "EarlyStopping counter: 11 out of 50\n",
      "epoch: 26 - train_acc: 0.912621359223301 - train_loss: 0.19868569128429658 - val_acc: 0.7241379310344828 - val_loss: 0.7131444642481904\n",
      "EarlyStopping counter: 12 out of 50\n",
      "epoch: 27 - train_acc: 0.9344660194174758 - train_loss: 0.16316068665811098 - val_acc: 0.6551724137931034 - val_loss: 0.5833757739232966\n",
      "EarlyStopping counter: 13 out of 50\n",
      "epoch: 28 - train_acc: 0.9223300970873787 - train_loss: 0.20154945747642822 - val_acc: 0.7241379310344828 - val_loss: 1.054194048510786\n",
      "EarlyStopping counter: 14 out of 50\n",
      "epoch: 29 - train_acc: 0.9077669902912622 - train_loss: 0.20989435216604607 - val_acc: 0.7586206896551724 - val_loss: 0.4983660088141968\n",
      "EarlyStopping counter: 15 out of 50\n",
      "epoch: 30 - train_acc: 0.9174757281553398 - train_loss: 0.1878243014081878 - val_acc: 0.6551724137931034 - val_loss: 0.5015763934948794\n",
      "EarlyStopping counter: 16 out of 50\n",
      "epoch: 31 - train_acc: 0.9344660194174758 - train_loss: 0.14742517917839076 - val_acc: 0.7586206896551724 - val_loss: 0.6945385635581581\n",
      "EarlyStopping counter: 17 out of 50\n",
      "epoch: 32 - train_acc: 0.9271844660194175 - train_loss: 0.1919839853436271 - val_acc: 0.7241379310344828 - val_loss: 0.9181215478324021\n",
      "EarlyStopping counter: 18 out of 50\n",
      "epoch: 33 - train_acc: 0.9587378640776699 - train_loss: 0.13749734916548295 - val_acc: 0.7241379310344828 - val_loss: 1.3295970625136975\n",
      "EarlyStopping counter: 19 out of 50\n",
      "epoch: 34 - train_acc: 0.9441747572815534 - train_loss: 0.1490894763505359 - val_acc: 0.7241379310344828 - val_loss: 0.810963487929849\n",
      "EarlyStopping counter: 20 out of 50\n",
      "epoch: 35 - train_acc: 0.9490291262135923 - train_loss: 0.1427589813064704 - val_acc: 0.6551724137931034 - val_loss: 0.734789815233459\n",
      "EarlyStopping counter: 21 out of 50\n",
      "epoch: 36 - train_acc: 0.9563106796116505 - train_loss: 0.14745225088960623 - val_acc: 0.7241379310344828 - val_loss: 0.6771401759212874\n",
      "EarlyStopping counter: 22 out of 50\n",
      "epoch: 37 - train_acc: 0.9150485436893204 - train_loss: 0.16990844878025166 - val_acc: 0.7931034482758621 - val_loss: 0.44057281906034707\n",
      "EarlyStopping counter: 23 out of 50\n",
      "epoch: 38 - train_acc: 0.9611650485436893 - train_loss: 0.12982134350686741 - val_acc: 0.6206896551724138 - val_loss: 0.6879422377198146\n",
      "EarlyStopping counter: 24 out of 50\n",
      "epoch: 39 - train_acc: 0.9368932038834952 - train_loss: 0.1535963769391862 - val_acc: 0.6896551724137931 - val_loss: 0.7778058982774811\n",
      "EarlyStopping counter: 25 out of 50\n",
      "epoch: 40 - train_acc: 0.941747572815534 - train_loss: 0.15612704678430103 - val_acc: 0.6551724137931034 - val_loss: 0.8910449826554507\n",
      "EarlyStopping counter: 26 out of 50\n",
      "epoch: 41 - train_acc: 0.9223300970873787 - train_loss: 0.17308370978920806 - val_acc: 0.6896551724137931 - val_loss: 0.7536581306832448\n",
      "EarlyStopping counter: 27 out of 50\n",
      "epoch: 42 - train_acc: 0.9441747572815534 - train_loss: 0.1439697372800398 - val_acc: 0.6551724137931034 - val_loss: 0.6789248340097944\n",
      "EarlyStopping counter: 28 out of 50\n",
      "epoch: 43 - train_acc: 0.9441747572815534 - train_loss: 0.14677275386029137 - val_acc: 0.8275862068965517 - val_loss: 0.6727270803538202\n",
      "EarlyStopping counter: 29 out of 50\n",
      "epoch: 44 - train_acc: 0.9368932038834952 - train_loss: 0.1670204346889682 - val_acc: 0.6206896551724138 - val_loss: 0.6654853522248683\n",
      "EarlyStopping counter: 30 out of 50\n",
      "epoch: 45 - train_acc: 0.9538834951456311 - train_loss: 0.11861975632801472 - val_acc: 0.7241379310344828 - val_loss: 0.8094624879388255\n",
      "EarlyStopping counter: 31 out of 50\n",
      "epoch: 46 - train_acc: 0.9684466019417476 - train_loss: 0.09477540822226943 - val_acc: 0.7931034482758621 - val_loss: 0.5809786688250491\n",
      "EarlyStopping counter: 32 out of 50\n",
      "epoch: 47 - train_acc: 0.9684466019417476 - train_loss: 0.09679901735857874 - val_acc: 0.7586206896551724 - val_loss: 1.0298569555762123\n",
      "EarlyStopping counter: 33 out of 50\n",
      "epoch: 48 - train_acc: 0.9878640776699029 - train_loss: 0.06326720904798501 - val_acc: 0.7241379310344828 - val_loss: 0.4363812866574045\n",
      "EarlyStopping counter: 34 out of 50\n",
      "epoch: 49 - train_acc: 0.9805825242718447 - train_loss: 0.08004158836828598 - val_acc: 0.7931034482758621 - val_loss: 0.6065462232310462\n",
      "EarlyStopping counter: 35 out of 50\n",
      "epoch: 50 - train_acc: 0.9830097087378641 - train_loss: 0.05673975666705331 - val_acc: 0.6896551724137931 - val_loss: 1.22433631044992\n",
      "EarlyStopping counter: 36 out of 50\n",
      "epoch: 51 - train_acc: 0.9611650485436893 - train_loss: 0.10827955536114696 - val_acc: 0.7241379310344828 - val_loss: 0.8083877461324873\n",
      "EarlyStopping counter: 37 out of 50\n",
      "epoch: 52 - train_acc: 0.9660194174757282 - train_loss: 0.10132556353806209 - val_acc: 0.6206896551724138 - val_loss: 0.8644922822927632\n",
      "EarlyStopping counter: 38 out of 50\n",
      "epoch: 53 - train_acc: 0.9781553398058253 - train_loss: 0.07979030990087099 - val_acc: 0.7931034482758621 - val_loss: 0.7215300581277322\n",
      "EarlyStopping counter: 39 out of 50\n",
      "epoch: 54 - train_acc: 0.9635922330097088 - train_loss: 0.10532690487744692 - val_acc: 0.7241379310344828 - val_loss: 0.8534903713022003\n",
      "EarlyStopping counter: 40 out of 50\n",
      "epoch: 55 - train_acc: 0.9563106796116505 - train_loss: 0.12254627660488203 - val_acc: 0.6206896551724138 - val_loss: 0.6082097525270316\n",
      "EarlyStopping counter: 41 out of 50\n",
      "epoch: 56 - train_acc: 0.970873786407767 - train_loss: 0.08913727132164072 - val_acc: 0.6551724137931034 - val_loss: 0.706089497711013\n",
      "EarlyStopping counter: 42 out of 50\n",
      "epoch: 57 - train_acc: 0.9854368932038835 - train_loss: 0.06374591692089092 - val_acc: 0.7241379310344828 - val_loss: 1.0834118603209777\n",
      "EarlyStopping counter: 43 out of 50\n",
      "epoch: 58 - train_acc: 0.9611650485436893 - train_loss: 0.12540999469788924 - val_acc: 0.7241379310344828 - val_loss: 0.6093291548777068\n",
      "EarlyStopping counter: 44 out of 50\n",
      "epoch: 59 - train_acc: 0.9684466019417476 - train_loss: 0.08121623675487034 - val_acc: 0.7586206896551724 - val_loss: 0.5808141262339388\n",
      "EarlyStopping counter: 45 out of 50\n",
      "epoch: 60 - train_acc: 0.9490291262135923 - train_loss: 0.1523868795557612 - val_acc: 0.7586206896551724 - val_loss: 0.7847371748829495\n",
      "EarlyStopping counter: 46 out of 50\n",
      "epoch: 61 - train_acc: 0.9757281553398058 - train_loss: 0.07886899272461793 - val_acc: 0.6896551724137931 - val_loss: 0.7575608007906536\n",
      "EarlyStopping counter: 47 out of 50\n",
      "epoch: 62 - train_acc: 0.9781553398058253 - train_loss: 0.08208242127876 - val_acc: 0.7586206896551724 - val_loss: 0.6442752511069357\n",
      "EarlyStopping counter: 48 out of 50\n",
      "epoch: 63 - train_acc: 0.9902912621359223 - train_loss: 0.04760729393667381 - val_acc: 0.7241379310344828 - val_loss: 0.6197147310873033\n",
      "EarlyStopping counter: 49 out of 50\n",
      "epoch: 64 - train_acc: 0.9684466019417476 - train_loss: 0.07106999775331051 - val_acc: 0.6896551724137931 - val_loss: 0.7619445962892994\n",
      "EarlyStopping counter: 50 out of 50\n",
      "epoch: 65 - train_acc: 0.970873786407767 - train_loss: 0.08123749355438363 - val_acc: 0.6896551724137931 - val_loss: 0.6285917989719043\n",
      "=> Early stopped !!\n",
      "Accuracy: 0.75\n",
      "Precision: 0.8125\n",
      "Recall: 0.7222222222222222\n",
      "F1: 0.7647058823529411\n",
      "\n",
      "[Fold 11]: \n",
      "len(train_sampler)=412\n",
      "len(val_sampler)=29\n",
      "[  8  14  27  40  62  64 128 135 138 156 162 200 207 212 216 230 236 260\n",
      " 279 288 300 326 337 356 374 381 393 404 408]\n",
      "Validation loss decreased (inf --> 0.607646).  Saving model ...\n",
      "epoch: 0 - train_acc: 0.6092233009708737 - train_loss: 0.6607647342794112 - val_acc: 0.5862068965517241 - val_loss: 0.607645857782443\n",
      "Validation loss decreased (0.607646 --> 0.594354).  Saving model ...\n",
      "epoch: 1 - train_acc: 0.6432038834951457 - train_loss: 0.6133764439770244 - val_acc: 0.7241379310344828 - val_loss: 0.5943535083195903\n",
      "Validation loss decreased (0.594354 --> 0.548402).  Saving model ...\n",
      "epoch: 2 - train_acc: 0.7063106796116505 - train_loss: 0.5539474451332332 - val_acc: 0.6206896551724138 - val_loss: 0.5484019063451268\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 3 - train_acc: 0.6771844660194175 - train_loss: 0.5434584817316795 - val_acc: 0.5172413793103449 - val_loss: 0.6111347827622092\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 4 - train_acc: 0.7475728155339806 - train_loss: 0.49907084177753264 - val_acc: 0.6206896551724138 - val_loss: 0.7056475461744411\n",
      "EarlyStopping counter: 3 out of 50\n",
      "epoch: 5 - train_acc: 0.7378640776699029 - train_loss: 0.5035699115862087 - val_acc: 0.7241379310344828 - val_loss: 0.6609444942506952\n",
      "Validation loss decreased (0.548402 --> 0.425007).  Saving model ...\n",
      "epoch: 6 - train_acc: 0.7815533980582524 - train_loss: 0.4549629811297475 - val_acc: 0.7931034482758621 - val_loss: 0.42500716334911176\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 7 - train_acc: 0.7621359223300971 - train_loss: 0.46747780976256126 - val_acc: 0.7931034482758621 - val_loss: 0.7207281371935301\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 8 - train_acc: 0.779126213592233 - train_loss: 0.4189386981315203 - val_acc: 0.6896551724137931 - val_loss: 0.5180746957352437\n",
      "Validation loss decreased (0.425007 --> 0.409124).  Saving model ...\n",
      "epoch: 9 - train_acc: 0.7936893203883495 - train_loss: 0.4286198171067453 - val_acc: 0.7586206896551724 - val_loss: 0.409123858921118\n",
      "Validation loss decreased (0.409124 --> 0.351092).  Saving model ...\n",
      "epoch: 10 - train_acc: 0.8155339805825242 - train_loss: 0.36314594225332064 - val_acc: 0.8275862068965517 - val_loss: 0.3510921119144462\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 11 - train_acc: 0.8252427184466019 - train_loss: 0.3749210835533397 - val_acc: 0.7586206896551724 - val_loss: 0.4315396096309434\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 12 - train_acc: 0.8325242718446602 - train_loss: 0.35498209372743933 - val_acc: 0.8275862068965517 - val_loss: 0.6864888115153833\n",
      "EarlyStopping counter: 3 out of 50\n",
      "epoch: 13 - train_acc: 0.8106796116504854 - train_loss: 0.41800711029309545 - val_acc: 0.6896551724137931 - val_loss: 0.5118759422968575\n",
      "Validation loss decreased (0.351092 --> 0.346354).  Saving model ...\n",
      "epoch: 14 - train_acc: 0.8519417475728155 - train_loss: 0.32082594968402744 - val_acc: 0.8620689655172413 - val_loss: 0.3463542216068419\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 15 - train_acc: 0.8567961165048543 - train_loss: 0.33117965533599153 - val_acc: 0.7931034482758621 - val_loss: 0.3846215956585625\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 16 - train_acc: 0.8616504854368932 - train_loss: 0.29262983425890077 - val_acc: 0.8275862068965517 - val_loss: 0.8013934577657744\n",
      "EarlyStopping counter: 3 out of 50\n",
      "epoch: 17 - train_acc: 0.866504854368932 - train_loss: 0.3038402217217928 - val_acc: 0.896551724137931 - val_loss: 0.4043486809485075\n",
      "EarlyStopping counter: 4 out of 50\n",
      "epoch: 18 - train_acc: 0.8932038834951457 - train_loss: 0.27146352018820935 - val_acc: 0.7586206896551724 - val_loss: 0.5082851832220132\n",
      "EarlyStopping counter: 5 out of 50\n",
      "epoch: 19 - train_acc: 0.8713592233009708 - train_loss: 0.2797805575582243 - val_acc: 0.7931034482758621 - val_loss: 0.42037234096448683\n",
      "EarlyStopping counter: 6 out of 50\n",
      "epoch: 20 - train_acc: 0.8640776699029126 - train_loss: 0.29456338345138666 - val_acc: 0.7586206896551724 - val_loss: 0.38228720661801735\n",
      "EarlyStopping counter: 7 out of 50\n",
      "epoch: 21 - train_acc: 0.9223300970873787 - train_loss: 0.20584804297805734 - val_acc: 0.7586206896551724 - val_loss: 0.50034761219628\n",
      "EarlyStopping counter: 8 out of 50\n",
      "epoch: 22 - train_acc: 0.8907766990291263 - train_loss: 0.24192082002184218 - val_acc: 0.7931034482758621 - val_loss: 0.5212444439079302\n",
      "EarlyStopping counter: 9 out of 50\n",
      "epoch: 23 - train_acc: 0.8786407766990292 - train_loss: 0.2795111333597466 - val_acc: 0.7241379310344828 - val_loss: 0.7723296648201373\n",
      "EarlyStopping counter: 10 out of 50\n",
      "epoch: 24 - train_acc: 0.8762135922330098 - train_loss: 0.2847038586244553 - val_acc: 0.7931034482758621 - val_loss: 0.6911257114575631\n",
      "EarlyStopping counter: 11 out of 50\n",
      "epoch: 25 - train_acc: 0.8907766990291263 - train_loss: 0.23630781269621914 - val_acc: 0.7241379310344828 - val_loss: 0.5092996711410925\n",
      "EarlyStopping counter: 12 out of 50\n",
      "epoch: 26 - train_acc: 0.8956310679611651 - train_loss: 0.2989469292728403 - val_acc: 0.7931034482758621 - val_loss: 0.4539060540676707\n",
      "EarlyStopping counter: 13 out of 50\n",
      "epoch: 27 - train_acc: 0.9053398058252428 - train_loss: 0.23411989670639333 - val_acc: 0.7931034482758621 - val_loss: 0.9040605444521169\n",
      "EarlyStopping counter: 14 out of 50\n",
      "epoch: 28 - train_acc: 0.8786407766990292 - train_loss: 0.25185424986860283 - val_acc: 0.7586206896551724 - val_loss: 0.5850767816317697\n",
      "EarlyStopping counter: 15 out of 50\n",
      "epoch: 29 - train_acc: 0.912621359223301 - train_loss: 0.21692638768517156 - val_acc: 0.7931034482758621 - val_loss: 0.5483190646415487\n",
      "EarlyStopping counter: 16 out of 50\n",
      "epoch: 30 - train_acc: 0.883495145631068 - train_loss: 0.26018901052389387 - val_acc: 0.7241379310344828 - val_loss: 0.4274645846249117\n",
      "EarlyStopping counter: 17 out of 50\n",
      "epoch: 31 - train_acc: 0.9174757281553398 - train_loss: 0.19176363521124942 - val_acc: 0.7931034482758621 - val_loss: 0.3845646270794552\n",
      "EarlyStopping counter: 18 out of 50\n",
      "epoch: 32 - train_acc: 0.8980582524271845 - train_loss: 0.21459495962598296 - val_acc: 0.6896551724137931 - val_loss: 0.7982999327826099\n",
      "EarlyStopping counter: 19 out of 50\n",
      "epoch: 33 - train_acc: 0.9199029126213593 - train_loss: 0.19261504281487596 - val_acc: 0.7586206896551724 - val_loss: 0.6960074728351953\n",
      "EarlyStopping counter: 20 out of 50\n",
      "epoch: 34 - train_acc: 0.9271844660194175 - train_loss: 0.17471947338172783 - val_acc: 0.896551724137931 - val_loss: 0.4415599051389321\n",
      "EarlyStopping counter: 21 out of 50\n",
      "epoch: 35 - train_acc: 0.9393203883495146 - train_loss: 0.1750263314946412 - val_acc: 0.7931034482758621 - val_loss: 0.5560502277896429\n",
      "EarlyStopping counter: 22 out of 50\n",
      "epoch: 36 - train_acc: 0.9563106796116505 - train_loss: 0.15176195106629542 - val_acc: 0.8275862068965517 - val_loss: 0.889424328070424\n",
      "EarlyStopping counter: 23 out of 50\n",
      "epoch: 37 - train_acc: 0.9393203883495146 - train_loss: 0.14388176649047885 - val_acc: 0.7586206896551724 - val_loss: 0.7562523255325094\n",
      "EarlyStopping counter: 24 out of 50\n",
      "epoch: 38 - train_acc: 0.9441747572815534 - train_loss: 0.1310362733275139 - val_acc: 0.7586206896551724 - val_loss: 0.9051139034973807\n",
      "EarlyStopping counter: 25 out of 50\n",
      "epoch: 39 - train_acc: 0.9611650485436893 - train_loss: 0.11003785847248881 - val_acc: 0.8275862068965517 - val_loss: 0.7499353959013408\n",
      "EarlyStopping counter: 26 out of 50\n",
      "epoch: 40 - train_acc: 0.9150485436893204 - train_loss: 0.19366045295922146 - val_acc: 0.7586206896551724 - val_loss: 0.7980779985016583\n",
      "EarlyStopping counter: 27 out of 50\n",
      "epoch: 41 - train_acc: 0.912621359223301 - train_loss: 0.1839288685853887 - val_acc: 0.8275862068965517 - val_loss: 0.5281373270219953\n",
      "EarlyStopping counter: 28 out of 50\n",
      "epoch: 42 - train_acc: 0.9174757281553398 - train_loss: 0.17110551760082685 - val_acc: 0.8275862068965517 - val_loss: 0.8951490636250975\n",
      "EarlyStopping counter: 29 out of 50\n",
      "epoch: 43 - train_acc: 0.9441747572815534 - train_loss: 0.15268243485945107 - val_acc: 0.7931034482758621 - val_loss: 0.5935614335678936\n",
      "EarlyStopping counter: 30 out of 50\n",
      "epoch: 44 - train_acc: 0.9368932038834952 - train_loss: 0.1509394470818473 - val_acc: 0.7586206896551724 - val_loss: 0.8229695669341751\n",
      "EarlyStopping counter: 31 out of 50\n",
      "epoch: 45 - train_acc: 0.9393203883495146 - train_loss: 0.13414378123860557 - val_acc: 0.9310344827586207 - val_loss: 0.7851844481509272\n",
      "EarlyStopping counter: 32 out of 50\n",
      "epoch: 46 - train_acc: 0.941747572815534 - train_loss: 0.1347487366378244 - val_acc: 0.7931034482758621 - val_loss: 0.35190968688692015\n",
      "EarlyStopping counter: 33 out of 50\n",
      "epoch: 47 - train_acc: 0.9490291262135923 - train_loss: 0.12518407521086097 - val_acc: 0.7241379310344828 - val_loss: 0.860543364514352\n",
      "EarlyStopping counter: 34 out of 50\n",
      "epoch: 48 - train_acc: 0.9587378640776699 - train_loss: 0.11448149626277342 - val_acc: 0.8275862068965517 - val_loss: 0.8349805684405445\n",
      "EarlyStopping counter: 35 out of 50\n",
      "epoch: 49 - train_acc: 0.9441747572815534 - train_loss: 0.13970824648830005 - val_acc: 0.7241379310344828 - val_loss: 0.7473980137006163\n",
      "EarlyStopping counter: 36 out of 50\n",
      "epoch: 50 - train_acc: 0.9344660194174758 - train_loss: 0.16389625376990913 - val_acc: 0.7931034482758621 - val_loss: 1.155668707515483\n",
      "EarlyStopping counter: 37 out of 50\n",
      "epoch: 51 - train_acc: 0.9635922330097088 - train_loss: 0.12312971258731271 - val_acc: 0.7931034482758621 - val_loss: 0.7332339748059975\n",
      "EarlyStopping counter: 38 out of 50\n",
      "epoch: 52 - train_acc: 0.9611650485436893 - train_loss: 0.10464699038259478 - val_acc: 0.7931034482758621 - val_loss: 0.8074084186870834\n",
      "EarlyStopping counter: 39 out of 50\n",
      "epoch: 53 - train_acc: 0.9587378640776699 - train_loss: 0.1159085567175715 - val_acc: 0.7931034482758621 - val_loss: 1.455500438241213\n",
      "EarlyStopping counter: 40 out of 50\n",
      "epoch: 54 - train_acc: 0.9611650485436893 - train_loss: 0.12342498916835384 - val_acc: 0.8275862068965517 - val_loss: 0.5098869178794122\n",
      "EarlyStopping counter: 41 out of 50\n",
      "epoch: 55 - train_acc: 0.9587378640776699 - train_loss: 0.11092505193665116 - val_acc: 0.7931034482758621 - val_loss: 1.027303375833307\n",
      "EarlyStopping counter: 42 out of 50\n",
      "epoch: 56 - train_acc: 0.9611650485436893 - train_loss: 0.12491778251849389 - val_acc: 0.7241379310344828 - val_loss: 1.3438404143727767\n",
      "EarlyStopping counter: 43 out of 50\n",
      "epoch: 57 - train_acc: 0.9757281553398058 - train_loss: 0.08145089573985839 - val_acc: 0.8620689655172413 - val_loss: 0.7336468822928384\n",
      "EarlyStopping counter: 44 out of 50\n",
      "epoch: 58 - train_acc: 0.9757281553398058 - train_loss: 0.08018483091349739 - val_acc: 0.8275862068965517 - val_loss: 0.6222830908020871\n",
      "EarlyStopping counter: 45 out of 50\n",
      "epoch: 59 - train_acc: 0.9635922330097088 - train_loss: 0.10306942918774037 - val_acc: 0.7931034482758621 - val_loss: 0.7582782036527181\n",
      "EarlyStopping counter: 46 out of 50\n",
      "epoch: 60 - train_acc: 0.9635922330097088 - train_loss: 0.09753090701275316 - val_acc: 0.8275862068965517 - val_loss: 0.7303431640922908\n",
      "EarlyStopping counter: 47 out of 50\n",
      "epoch: 61 - train_acc: 0.9805825242718447 - train_loss: 0.06093923418868575 - val_acc: 0.7241379310344828 - val_loss: 0.8274117430749683\n",
      "EarlyStopping counter: 48 out of 50\n",
      "epoch: 62 - train_acc: 0.9660194174757282 - train_loss: 0.08704885687129711 - val_acc: 0.7931034482758621 - val_loss: 0.6670204430461737\n",
      "EarlyStopping counter: 49 out of 50\n",
      "epoch: 63 - train_acc: 0.9854368932038835 - train_loss: 0.05777970028649833 - val_acc: 0.7241379310344828 - val_loss: 0.6908645318073529\n",
      "EarlyStopping counter: 50 out of 50\n",
      "epoch: 64 - train_acc: 0.9660194174757282 - train_loss: 0.08937010947734936 - val_acc: 0.7586206896551724 - val_loss: 1.5205497695073331\n",
      "=> Early stopped !!\n",
      "Accuracy: 0.6964285714285714\n",
      "Precision: 0.676829268292683\n",
      "Recall: 0.8809523809523809\n",
      "F1: 0.7655172413793103\n",
      "\n",
      "[Fold 12]: \n",
      "len(train_sampler)=412\n",
      "len(val_sampler)=29\n",
      "[  1  34  43  49  52  53  80  91 105 161 190 201 205 217 251 259 263 269\n",
      " 295 303 309 339 345 350 366 402 403 425 430]\n",
      "Validation loss decreased (inf --> 0.786301).  Saving model ...\n",
      "epoch: 0 - train_acc: 0.5849514563106796 - train_loss: 0.6723782817823823 - val_acc: 0.5172413793103449 - val_loss: 0.7863008212773275\n",
      "Validation loss decreased (0.786301 --> 0.777636).  Saving model ...\n",
      "epoch: 1 - train_acc: 0.6553398058252428 - train_loss: 0.6066176041289062 - val_acc: 0.5862068965517241 - val_loss: 0.7776362869889991\n",
      "Validation loss decreased (0.777636 --> 0.533230).  Saving model ...\n",
      "epoch: 2 - train_acc: 0.6796116504854369 - train_loss: 0.6049351985656137 - val_acc: 0.6206896551724138 - val_loss: 0.5332303369709932\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 3 - train_acc: 0.6990291262135923 - train_loss: 0.5778103158726428 - val_acc: 0.7586206896551724 - val_loss: 0.5507051505074156\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 4 - train_acc: 0.7378640776699029 - train_loss: 0.5190445000329768 - val_acc: 0.6551724137931034 - val_loss: 0.6094855625119182\n",
      "EarlyStopping counter: 3 out of 50\n",
      "epoch: 5 - train_acc: 0.7597087378640777 - train_loss: 0.5042525477366984 - val_acc: 0.5517241379310345 - val_loss: 0.7108490977629252\n",
      "EarlyStopping counter: 4 out of 50\n",
      "epoch: 6 - train_acc: 0.7766990291262136 - train_loss: 0.4328842747531775 - val_acc: 0.7241379310344828 - val_loss: 0.6020989323549862\n",
      "EarlyStopping counter: 5 out of 50\n",
      "epoch: 7 - train_acc: 0.7718446601941747 - train_loss: 0.4348699895461306 - val_acc: 0.6551724137931034 - val_loss: 0.9709060979853565\n",
      "EarlyStopping counter: 6 out of 50\n",
      "epoch: 8 - train_acc: 0.8009708737864077 - train_loss: 0.4122374230570963 - val_acc: 0.6551724137931034 - val_loss: 0.5567161985512538\n",
      "Validation loss decreased (0.533230 --> 0.381495).  Saving model ...\n",
      "epoch: 9 - train_acc: 0.8276699029126213 - train_loss: 0.3862526981715611 - val_acc: 0.7241379310344828 - val_loss: 0.3814951800239023\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 10 - train_acc: 0.8422330097087378 - train_loss: 0.36159640996200226 - val_acc: 0.7241379310344828 - val_loss: 0.5756945890660596\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 11 - train_acc: 0.8228155339805825 - train_loss: 0.356677653833359 - val_acc: 0.6551724137931034 - val_loss: 0.669397896472927\n",
      "EarlyStopping counter: 3 out of 50\n",
      "epoch: 12 - train_acc: 0.8252427184466019 - train_loss: 0.3609259716993526 - val_acc: 0.7241379310344828 - val_loss: 0.5303507859407499\n",
      "EarlyStopping counter: 4 out of 50\n",
      "epoch: 13 - train_acc: 0.8422330097087378 - train_loss: 0.33294140818020884 - val_acc: 0.6551724137931034 - val_loss: 0.3816315324495506\n",
      "EarlyStopping counter: 5 out of 50\n",
      "epoch: 14 - train_acc: 0.8179611650485437 - train_loss: 0.3753593815340411 - val_acc: 0.6206896551724138 - val_loss: 0.5053578176614868\n",
      "EarlyStopping counter: 6 out of 50\n",
      "epoch: 15 - train_acc: 0.8640776699029126 - train_loss: 0.322072548737357 - val_acc: 0.6896551724137931 - val_loss: 0.4794084365461785\n",
      "EarlyStopping counter: 7 out of 50\n",
      "epoch: 16 - train_acc: 0.8883495145631068 - train_loss: 0.2964615100282348 - val_acc: 0.7586206896551724 - val_loss: 0.5985527930701351\n",
      "EarlyStopping counter: 8 out of 50\n",
      "epoch: 17 - train_acc: 0.8810679611650486 - train_loss: 0.2890587078630861 - val_acc: 0.7241379310344828 - val_loss: 0.40858090013836923\n",
      "EarlyStopping counter: 9 out of 50\n",
      "epoch: 18 - train_acc: 0.8786407766990292 - train_loss: 0.2767320250986727 - val_acc: 0.7586206896551724 - val_loss: 0.51111217391432\n",
      "EarlyStopping counter: 10 out of 50\n",
      "epoch: 19 - train_acc: 0.9053398058252428 - train_loss: 0.2516502681308049 - val_acc: 0.6896551724137931 - val_loss: 0.6475380931747375\n",
      "Validation loss decreased (0.381495 --> 0.330528).  Saving model ...\n",
      "epoch: 20 - train_acc: 0.912621359223301 - train_loss: 0.2364593548702912 - val_acc: 0.7931034482758621 - val_loss: 0.33052827582823896\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 21 - train_acc: 0.883495145631068 - train_loss: 0.23969035302363556 - val_acc: 0.6896551724137931 - val_loss: 0.5248830063799631\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 22 - train_acc: 0.9199029126213593 - train_loss: 0.222222117843386 - val_acc: 0.6896551724137931 - val_loss: 1.1352593945530522\n",
      "EarlyStopping counter: 3 out of 50\n",
      "epoch: 23 - train_acc: 0.9029126213592233 - train_loss: 0.24452410596705798 - val_acc: 0.7586206896551724 - val_loss: 0.6706935355378001\n",
      "EarlyStopping counter: 4 out of 50\n",
      "epoch: 24 - train_acc: 0.8956310679611651 - train_loss: 0.22941343714490023 - val_acc: 0.6551724137931034 - val_loss: 0.8574275076246786\n",
      "Validation loss decreased (0.330528 --> 0.301237).  Saving model ...\n",
      "epoch: 25 - train_acc: 0.9077669902912622 - train_loss: 0.2295538305968677 - val_acc: 0.7931034482758621 - val_loss: 0.3012368133621179\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 26 - train_acc: 0.9004854368932039 - train_loss: 0.22110476607706553 - val_acc: 0.7241379310344828 - val_loss: 0.7252749963332724\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 27 - train_acc: 0.912621359223301 - train_loss: 0.22199784390030736 - val_acc: 0.7586206896551724 - val_loss: 0.45720900742935516\n",
      "EarlyStopping counter: 3 out of 50\n",
      "epoch: 28 - train_acc: 0.9271844660194175 - train_loss: 0.17190152945267526 - val_acc: 0.8275862068965517 - val_loss: 0.5058287274788718\n",
      "EarlyStopping counter: 4 out of 50\n",
      "epoch: 29 - train_acc: 0.9393203883495146 - train_loss: 0.1463850602036757 - val_acc: 0.6896551724137931 - val_loss: 0.4655687877324445\n",
      "EarlyStopping counter: 5 out of 50\n",
      "epoch: 30 - train_acc: 0.941747572815534 - train_loss: 0.1559232028475975 - val_acc: 0.6551724137931034 - val_loss: 0.6386456653853166\n",
      "EarlyStopping counter: 6 out of 50\n",
      "epoch: 31 - train_acc: 0.9101941747572816 - train_loss: 0.220354865968163 - val_acc: 0.7586206896551724 - val_loss: 0.8004770089237981\n",
      "EarlyStopping counter: 7 out of 50\n",
      "epoch: 32 - train_acc: 0.9077669902912622 - train_loss: 0.21748499457675025 - val_acc: 0.7586206896551724 - val_loss: 0.5021703969497431\n",
      "EarlyStopping counter: 8 out of 50\n",
      "epoch: 33 - train_acc: 0.9101941747572816 - train_loss: 0.21641866814712457 - val_acc: 0.7931034482758621 - val_loss: 1.0307231057514197\n",
      "EarlyStopping counter: 9 out of 50\n",
      "epoch: 34 - train_acc: 0.8883495145631068 - train_loss: 0.28029805419336323 - val_acc: 0.6896551724137931 - val_loss: 0.6344263394936377\n",
      "EarlyStopping counter: 10 out of 50\n",
      "epoch: 35 - train_acc: 0.9174757281553398 - train_loss: 0.1947737233680362 - val_acc: 0.7241379310344828 - val_loss: 1.0119294632241718\n",
      "EarlyStopping counter: 11 out of 50\n",
      "epoch: 36 - train_acc: 0.9538834951456311 - train_loss: 0.16214021712803006 - val_acc: 0.7241379310344828 - val_loss: 0.6561593319017094\n",
      "EarlyStopping counter: 12 out of 50\n",
      "epoch: 37 - train_acc: 0.9393203883495146 - train_loss: 0.16197443053922955 - val_acc: 0.7241379310344828 - val_loss: 0.7743575498571609\n",
      "EarlyStopping counter: 13 out of 50\n",
      "epoch: 38 - train_acc: 0.9466019417475728 - train_loss: 0.12117952312892445 - val_acc: 0.7931034482758621 - val_loss: 0.6776898087463734\n",
      "EarlyStopping counter: 14 out of 50\n",
      "epoch: 39 - train_acc: 0.9563106796116505 - train_loss: 0.11511678664427008 - val_acc: 0.7586206896551724 - val_loss: 0.41647203846970043\n",
      "EarlyStopping counter: 15 out of 50\n",
      "epoch: 40 - train_acc: 0.941747572815534 - train_loss: 0.15048938740192092 - val_acc: 0.7586206896551724 - val_loss: 0.9098158000449021\n",
      "EarlyStopping counter: 16 out of 50\n",
      "epoch: 41 - train_acc: 0.9757281553398058 - train_loss: 0.0915499556170514 - val_acc: 0.7586206896551724 - val_loss: 0.6761827970287962\n",
      "EarlyStopping counter: 17 out of 50\n",
      "epoch: 42 - train_acc: 0.9344660194174758 - train_loss: 0.15575832515720278 - val_acc: 0.6896551724137931 - val_loss: 1.614953762892838\n",
      "EarlyStopping counter: 18 out of 50\n",
      "epoch: 43 - train_acc: 0.941747572815534 - train_loss: 0.16373803240250162 - val_acc: 0.7241379310344828 - val_loss: 0.5497248511670713\n",
      "EarlyStopping counter: 19 out of 50\n",
      "epoch: 44 - train_acc: 0.9223300970873787 - train_loss: 0.172772386210503 - val_acc: 0.7241379310344828 - val_loss: 0.45220727463760746\n",
      "Validation loss decreased (0.301237 --> 0.273996).  Saving model ...\n",
      "epoch: 45 - train_acc: 0.9296116504854369 - train_loss: 0.15772063682341855 - val_acc: 0.8620689655172413 - val_loss: 0.2739962666660897\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 46 - train_acc: 0.941747572815534 - train_loss: 0.14116141045701716 - val_acc: 0.7586206896551724 - val_loss: 0.7722392863685823\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 47 - train_acc: 0.9174757281553398 - train_loss: 0.17056013663081854 - val_acc: 0.7241379310344828 - val_loss: 0.8345589396559852\n",
      "EarlyStopping counter: 3 out of 50\n",
      "epoch: 48 - train_acc: 0.9563106796116505 - train_loss: 0.1748038623711052 - val_acc: 0.7241379310344828 - val_loss: 0.6783352933751163\n",
      "EarlyStopping counter: 4 out of 50\n",
      "epoch: 49 - train_acc: 0.941747572815534 - train_loss: 0.13575905964067553 - val_acc: 0.8275862068965517 - val_loss: 0.593536439641241\n",
      "EarlyStopping counter: 5 out of 50\n",
      "epoch: 50 - train_acc: 0.9587378640776699 - train_loss: 0.11429408642231212 - val_acc: 0.7241379310344828 - val_loss: 0.9493439289312738\n",
      "EarlyStopping counter: 6 out of 50\n",
      "epoch: 51 - train_acc: 0.9660194174757282 - train_loss: 0.09348610723806762 - val_acc: 0.7241379310344828 - val_loss: 1.0147042513548183\n",
      "EarlyStopping counter: 7 out of 50\n",
      "epoch: 52 - train_acc: 0.9805825242718447 - train_loss: 0.06947233617306223 - val_acc: 0.8620689655172413 - val_loss: 0.7567219598410064\n",
      "EarlyStopping counter: 8 out of 50\n",
      "epoch: 53 - train_acc: 0.9733009708737864 - train_loss: 0.07236088659758974 - val_acc: 0.8275862068965517 - val_loss: 1.2168704980817535\n",
      "EarlyStopping counter: 9 out of 50\n",
      "epoch: 54 - train_acc: 0.9587378640776699 - train_loss: 0.10245149074303003 - val_acc: 0.7241379310344828 - val_loss: 0.9624421509870532\n",
      "EarlyStopping counter: 10 out of 50\n",
      "epoch: 55 - train_acc: 0.9563106796116505 - train_loss: 0.14353190203262406 - val_acc: 0.7586206896551724 - val_loss: 1.0196439029387283\n",
      "EarlyStopping counter: 11 out of 50\n",
      "epoch: 56 - train_acc: 0.9441747572815534 - train_loss: 0.14404616839770812 - val_acc: 0.7586206896551724 - val_loss: 0.591806250966263\n",
      "EarlyStopping counter: 12 out of 50\n",
      "epoch: 57 - train_acc: 0.9393203883495146 - train_loss: 0.14426166548217753 - val_acc: 0.7586206896551724 - val_loss: 1.4347735129185792\n",
      "EarlyStopping counter: 13 out of 50\n",
      "epoch: 58 - train_acc: 0.9514563106796117 - train_loss: 0.16119189012381074 - val_acc: 0.6896551724137931 - val_loss: 0.818859441230865\n",
      "EarlyStopping counter: 14 out of 50\n",
      "epoch: 59 - train_acc: 0.9514563106796117 - train_loss: 0.1424652498644162 - val_acc: 0.7931034482758621 - val_loss: 0.6675123376008154\n",
      "EarlyStopping counter: 15 out of 50\n",
      "epoch: 60 - train_acc: 0.9466019417475728 - train_loss: 0.12361674855134684 - val_acc: 0.6896551724137931 - val_loss: 1.1405595149554206\n",
      "EarlyStopping counter: 16 out of 50\n",
      "epoch: 61 - train_acc: 0.9635922330097088 - train_loss: 0.09765701721852618 - val_acc: 0.7586206896551724 - val_loss: 2.192381521197936\n",
      "EarlyStopping counter: 17 out of 50\n",
      "epoch: 62 - train_acc: 0.970873786407767 - train_loss: 0.0761381130995034 - val_acc: 0.7241379310344828 - val_loss: 0.7151662104297176\n",
      "EarlyStopping counter: 18 out of 50\n",
      "epoch: 63 - train_acc: 0.9781553398058253 - train_loss: 0.06549458079663201 - val_acc: 0.7241379310344828 - val_loss: 2.5608149584107314\n",
      "EarlyStopping counter: 19 out of 50\n",
      "epoch: 64 - train_acc: 0.9684466019417476 - train_loss: 0.09322023115348768 - val_acc: 0.7931034482758621 - val_loss: 0.9734107333435247\n",
      "EarlyStopping counter: 20 out of 50\n",
      "epoch: 65 - train_acc: 0.970873786407767 - train_loss: 0.08592717666322891 - val_acc: 0.7586206896551724 - val_loss: 1.409545741219841\n",
      "EarlyStopping counter: 21 out of 50\n",
      "epoch: 66 - train_acc: 0.9660194174757282 - train_loss: 0.09429706008839286 - val_acc: 0.7586206896551724 - val_loss: 0.6780134157652904\n",
      "EarlyStopping counter: 22 out of 50\n",
      "epoch: 67 - train_acc: 0.9805825242718447 - train_loss: 0.06013724271083382 - val_acc: 0.7586206896551724 - val_loss: 0.6288061524460052\n",
      "EarlyStopping counter: 23 out of 50\n",
      "epoch: 68 - train_acc: 0.9878640776699029 - train_loss: 0.050661494546634905 - val_acc: 0.8275862068965517 - val_loss: 0.6639444179041574\n",
      "EarlyStopping counter: 24 out of 50\n",
      "epoch: 69 - train_acc: 0.9854368932038835 - train_loss: 0.05044318623772881 - val_acc: 0.7586206896551724 - val_loss: 0.7185930737737474\n",
      "EarlyStopping counter: 25 out of 50\n",
      "epoch: 70 - train_acc: 0.9805825242718447 - train_loss: 0.07110559048150704 - val_acc: 0.7241379310344828 - val_loss: 2.1355845451060547\n",
      "EarlyStopping counter: 26 out of 50\n",
      "epoch: 71 - train_acc: 0.9490291262135923 - train_loss: 0.11565008549803171 - val_acc: 0.7586206896551724 - val_loss: 0.8933739581727952\n",
      "EarlyStopping counter: 27 out of 50\n",
      "epoch: 72 - train_acc: 0.9757281553398058 - train_loss: 0.06104054073173329 - val_acc: 0.8275862068965517 - val_loss: 0.788407508022064\n",
      "EarlyStopping counter: 28 out of 50\n",
      "epoch: 73 - train_acc: 0.9854368932038835 - train_loss: 0.05098837519900306 - val_acc: 0.8275862068965517 - val_loss: 0.4825444645602246\n",
      "EarlyStopping counter: 29 out of 50\n",
      "epoch: 74 - train_acc: 0.9830097087378641 - train_loss: 0.05544983897369626 - val_acc: 0.7931034482758621 - val_loss: 0.4114167682910569\n",
      "EarlyStopping counter: 30 out of 50\n",
      "epoch: 75 - train_acc: 0.9635922330097088 - train_loss: 0.12306011841534086 - val_acc: 0.7586206896551724 - val_loss: 0.8359569881294516\n",
      "EarlyStopping counter: 31 out of 50\n",
      "epoch: 76 - train_acc: 0.9538834951456311 - train_loss: 0.11208053762424496 - val_acc: 0.7586206896551724 - val_loss: 0.8229666942523343\n",
      "EarlyStopping counter: 32 out of 50\n",
      "epoch: 77 - train_acc: 0.9757281553398058 - train_loss: 0.08169472038225728 - val_acc: 0.7241379310344828 - val_loss: 1.7348347015283134\n",
      "EarlyStopping counter: 33 out of 50\n",
      "epoch: 78 - train_acc: 0.9757281553398058 - train_loss: 0.06696857943410521 - val_acc: 0.7241379310344828 - val_loss: 0.8433929360360405\n",
      "EarlyStopping counter: 34 out of 50\n",
      "epoch: 79 - train_acc: 0.9854368932038835 - train_loss: 0.049793188848252554 - val_acc: 0.7586206896551724 - val_loss: 1.100859740808938\n",
      "EarlyStopping counter: 35 out of 50\n",
      "epoch: 80 - train_acc: 0.9781553398058253 - train_loss: 0.06727010455583074 - val_acc: 0.7931034482758621 - val_loss: 0.6928603584980747\n",
      "EarlyStopping counter: 36 out of 50\n",
      "epoch: 81 - train_acc: 0.9805825242718447 - train_loss: 0.05781493249669608 - val_acc: 0.6896551724137931 - val_loss: 1.3809973558288742\n",
      "EarlyStopping counter: 37 out of 50\n",
      "epoch: 82 - train_acc: 0.9781553398058253 - train_loss: 0.06291466900058937 - val_acc: 0.7586206896551724 - val_loss: 0.6775436906321058\n",
      "EarlyStopping counter: 38 out of 50\n",
      "epoch: 83 - train_acc: 0.9902912621359223 - train_loss: 0.041649591912869266 - val_acc: 0.7931034482758621 - val_loss: 0.7875486040710399\n",
      "EarlyStopping counter: 39 out of 50\n",
      "epoch: 84 - train_acc: 0.9635922330097088 - train_loss: 0.09372008058322166 - val_acc: 0.7241379310344828 - val_loss: 1.8496451400572478\n",
      "EarlyStopping counter: 40 out of 50\n",
      "epoch: 85 - train_acc: 0.9757281553398058 - train_loss: 0.09535654134736922 - val_acc: 0.7931034482758621 - val_loss: 0.878328312229921\n",
      "EarlyStopping counter: 41 out of 50\n",
      "epoch: 86 - train_acc: 0.9902912621359223 - train_loss: 0.052395117681295594 - val_acc: 0.8275862068965517 - val_loss: 0.7965026090801088\n",
      "EarlyStopping counter: 42 out of 50\n",
      "epoch: 87 - train_acc: 0.9830097087378641 - train_loss: 0.05867233475785875 - val_acc: 0.7931034482758621 - val_loss: 0.7539554346033412\n",
      "EarlyStopping counter: 43 out of 50\n",
      "epoch: 88 - train_acc: 0.9635922330097088 - train_loss: 0.09452579307011526 - val_acc: 0.8275862068965517 - val_loss: 0.47903430839988026\n",
      "EarlyStopping counter: 44 out of 50\n",
      "epoch: 89 - train_acc: 0.970873786407767 - train_loss: 0.08199563016837039 - val_acc: 0.7241379310344828 - val_loss: 0.4432161269323604\n",
      "EarlyStopping counter: 45 out of 50\n",
      "epoch: 90 - train_acc: 0.9805825242718447 - train_loss: 0.0538338850051166 - val_acc: 0.7241379310344828 - val_loss: 1.006509935848869\n",
      "EarlyStopping counter: 46 out of 50\n",
      "epoch: 91 - train_acc: 0.9805825242718447 - train_loss: 0.04849009550250263 - val_acc: 0.7586206896551724 - val_loss: 0.3619399816677552\n",
      "EarlyStopping counter: 47 out of 50\n",
      "epoch: 92 - train_acc: 0.9927184466019418 - train_loss: 0.043597324900404896 - val_acc: 0.8620689655172413 - val_loss: 0.39717318405236535\n",
      "EarlyStopping counter: 48 out of 50\n",
      "epoch: 93 - train_acc: 0.9927184466019418 - train_loss: 0.042361322810627725 - val_acc: 0.7586206896551724 - val_loss: 0.8151948103454433\n",
      "EarlyStopping counter: 49 out of 50\n",
      "epoch: 94 - train_acc: 0.9684466019417476 - train_loss: 0.08305986276018945 - val_acc: 0.8620689655172413 - val_loss: 0.3740378447919266\n",
      "EarlyStopping counter: 50 out of 50\n",
      "epoch: 95 - train_acc: 0.9757281553398058 - train_loss: 0.07140111689534331 - val_acc: 0.6551724137931034 - val_loss: 1.0635418817303877\n",
      "=> Early stopped !!\n",
      "Accuracy: 0.7633928571428571\n",
      "Precision: 0.8230088495575221\n",
      "Recall: 0.7380952380952381\n",
      "F1: 0.7782426778242678\n",
      "\n",
      "[Fold 13]: \n",
      "len(train_sampler)=412\n",
      "len(val_sampler)=29\n",
      "[ 13  21  48  50  54  58  88 134 166 169 174 187 189 235 241 243 252 264\n",
      " 273 306 315 319 328 344 363 387 423 433 437]\n",
      "Validation loss decreased (inf --> 0.631750).  Saving model ...\n",
      "epoch: 0 - train_acc: 0.5995145631067961 - train_loss: 0.674368488352693 - val_acc: 0.5517241379310345 - val_loss: 0.6317503684475625\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 1 - train_acc: 0.6432038834951457 - train_loss: 0.6227328502750812 - val_acc: 0.5517241379310345 - val_loss: 0.7818445226563628\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 2 - train_acc: 0.6699029126213593 - train_loss: 0.5875772341233039 - val_acc: 0.5862068965517241 - val_loss: 0.7246744898214237\n",
      "Validation loss decreased (0.631750 --> 0.621432).  Saving model ...\n",
      "epoch: 3 - train_acc: 0.7330097087378641 - train_loss: 0.5380280169556989 - val_acc: 0.5862068965517241 - val_loss: 0.6214316927226685\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 4 - train_acc: 0.7621359223300971 - train_loss: 0.4972312624457382 - val_acc: 0.6551724137931034 - val_loss: 0.8088077750056228\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 5 - train_acc: 0.7427184466019418 - train_loss: 0.5025363705001803 - val_acc: 0.6206896551724138 - val_loss: 0.6382538198009112\n",
      "EarlyStopping counter: 3 out of 50\n",
      "epoch: 6 - train_acc: 0.7936893203883495 - train_loss: 0.45455983098145714 - val_acc: 0.6896551724137931 - val_loss: 0.6704667671316777\n",
      "Validation loss decreased (0.621432 --> 0.583602).  Saving model ...\n",
      "epoch: 7 - train_acc: 0.7742718446601942 - train_loss: 0.43641989165026046 - val_acc: 0.6206896551724138 - val_loss: 0.5836023211661887\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 8 - train_acc: 0.779126213592233 - train_loss: 0.4286451216063273 - val_acc: 0.7241379310344828 - val_loss: 0.7750173093401252\n",
      "Validation loss decreased (0.583602 --> 0.505490).  Saving model ...\n",
      "epoch: 9 - train_acc: 0.8179611650485437 - train_loss: 0.40717387287709855 - val_acc: 0.7586206896551724 - val_loss: 0.5054903131453969\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 10 - train_acc: 0.8131067961165048 - train_loss: 0.38710592027622526 - val_acc: 0.7241379310344828 - val_loss: 0.5600249333614693\n",
      "Validation loss decreased (0.505490 --> 0.441857).  Saving model ...\n",
      "epoch: 11 - train_acc: 0.8155339805825242 - train_loss: 0.3707051014862768 - val_acc: 0.7586206896551724 - val_loss: 0.4418572438807473\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 12 - train_acc: 0.8349514563106796 - train_loss: 0.3370358883769358 - val_acc: 0.7241379310344828 - val_loss: 0.5926910352005885\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 13 - train_acc: 0.8810679611650486 - train_loss: 0.2944438328190842 - val_acc: 0.6896551724137931 - val_loss: 0.5570481488776384\n",
      "EarlyStopping counter: 3 out of 50\n",
      "epoch: 14 - train_acc: 0.8762135922330098 - train_loss: 0.3068682076524696 - val_acc: 0.6551724137931034 - val_loss: 0.7863768897902675\n",
      "Validation loss decreased (0.441857 --> 0.416533).  Saving model ...\n",
      "epoch: 15 - train_acc: 0.8786407766990292 - train_loss: 0.2730227531533657 - val_acc: 0.7241379310344828 - val_loss: 0.41653306374513777\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 16 - train_acc: 0.8640776699029126 - train_loss: 0.27865960537253515 - val_acc: 0.6551724137931034 - val_loss: 0.514068004515507\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 17 - train_acc: 0.9029126213592233 - train_loss: 0.23327335592400336 - val_acc: 0.5862068965517241 - val_loss: 0.7081151185024271\n",
      "Validation loss decreased (0.416533 --> 0.370097).  Saving model ...\n",
      "epoch: 18 - train_acc: 0.9004854368932039 - train_loss: 0.24428656540243698 - val_acc: 0.8275862068965517 - val_loss: 0.3700973251466372\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 19 - train_acc: 0.883495145631068 - train_loss: 0.26415662859310546 - val_acc: 0.6896551724137931 - val_loss: 0.4345416663537578\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 20 - train_acc: 0.9174757281553398 - train_loss: 0.19747001867117298 - val_acc: 0.7586206896551724 - val_loss: 0.4766832498147854\n",
      "EarlyStopping counter: 3 out of 50\n",
      "epoch: 21 - train_acc: 0.8689320388349514 - train_loss: 0.2691898283474692 - val_acc: 0.6896551724137931 - val_loss: 0.5140698218142616\n",
      "EarlyStopping counter: 4 out of 50\n",
      "epoch: 22 - train_acc: 0.8859223300970874 - train_loss: 0.23788236800094753 - val_acc: 0.7586206896551724 - val_loss: 0.5195072117863686\n",
      "EarlyStopping counter: 5 out of 50\n",
      "epoch: 23 - train_acc: 0.9004854368932039 - train_loss: 0.22653509439544217 - val_acc: 0.6206896551724138 - val_loss: 0.7419056379413207\n",
      "EarlyStopping counter: 6 out of 50\n",
      "epoch: 24 - train_acc: 0.9150485436893204 - train_loss: 0.2258423264920822 - val_acc: 0.6896551724137931 - val_loss: 0.6240200127932494\n",
      "EarlyStopping counter: 7 out of 50\n",
      "epoch: 25 - train_acc: 0.8883495145631068 - train_loss: 0.24289175472519445 - val_acc: 0.7241379310344828 - val_loss: 0.8836137857193443\n",
      "EarlyStopping counter: 8 out of 50\n",
      "epoch: 26 - train_acc: 0.9004854368932039 - train_loss: 0.2411162452913588 - val_acc: 0.6896551724137931 - val_loss: 0.5386405588282315\n",
      "EarlyStopping counter: 9 out of 50\n",
      "epoch: 27 - train_acc: 0.9466019417475728 - train_loss: 0.15784813649602386 - val_acc: 0.6896551724137931 - val_loss: 0.6519711813937559\n",
      "EarlyStopping counter: 10 out of 50\n",
      "epoch: 28 - train_acc: 0.9393203883495146 - train_loss: 0.15032743030591117 - val_acc: 0.7931034482758621 - val_loss: 0.5677545884243498\n",
      "EarlyStopping counter: 11 out of 50\n",
      "epoch: 29 - train_acc: 0.8956310679611651 - train_loss: 0.22311146676324206 - val_acc: 0.6206896551724138 - val_loss: 0.6495188883336347\n",
      "EarlyStopping counter: 12 out of 50\n",
      "epoch: 30 - train_acc: 0.9344660194174758 - train_loss: 0.15601750365159325 - val_acc: 0.7931034482758621 - val_loss: 1.0094236577357076\n",
      "EarlyStopping counter: 13 out of 50\n",
      "epoch: 31 - train_acc: 0.9514563106796117 - train_loss: 0.1532794773335397 - val_acc: 0.7586206896551724 - val_loss: 0.7842681540277823\n",
      "EarlyStopping counter: 14 out of 50\n",
      "epoch: 32 - train_acc: 0.941747572815534 - train_loss: 0.15959354434301334 - val_acc: 0.7586206896551724 - val_loss: 0.5461388311076789\n",
      "EarlyStopping counter: 15 out of 50\n",
      "epoch: 33 - train_acc: 0.9101941747572816 - train_loss: 0.2034324507615439 - val_acc: 0.7586206896551724 - val_loss: 0.780398764199836\n",
      "Validation loss decreased (0.370097 --> 0.355255).  Saving model ...\n",
      "epoch: 34 - train_acc: 0.9563106796116505 - train_loss: 0.12189204140368637 - val_acc: 0.7241379310344828 - val_loss: 0.35525490517230324\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 35 - train_acc: 0.9320388349514563 - train_loss: 0.1581534367563834 - val_acc: 0.7241379310344828 - val_loss: 0.5511223900195765\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 36 - train_acc: 0.9320388349514563 - train_loss: 0.16283106754531043 - val_acc: 0.7586206896551724 - val_loss: 0.43506138061172517\n",
      "Validation loss decreased (0.355255 --> 0.345291).  Saving model ...\n",
      "epoch: 37 - train_acc: 0.9053398058252428 - train_loss: 0.23010398939059729 - val_acc: 0.7586206896551724 - val_loss: 0.3452907499804122\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 38 - train_acc: 0.941747572815534 - train_loss: 0.17342077435140388 - val_acc: 0.7586206896551724 - val_loss: 0.5237404288484545\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 39 - train_acc: 0.9490291262135923 - train_loss: 0.1256314721551705 - val_acc: 0.7931034482758621 - val_loss: 0.621682009508596\n",
      "EarlyStopping counter: 3 out of 50\n",
      "epoch: 40 - train_acc: 0.9611650485436893 - train_loss: 0.10718881577222478 - val_acc: 0.7586206896551724 - val_loss: 0.5685353870433156\n",
      "EarlyStopping counter: 4 out of 50\n",
      "epoch: 41 - train_acc: 0.9635922330097088 - train_loss: 0.12139883763070064 - val_acc: 0.7931034482758621 - val_loss: 0.4683927808044698\n",
      "EarlyStopping counter: 5 out of 50\n",
      "epoch: 42 - train_acc: 0.9393203883495146 - train_loss: 0.17956666639329658 - val_acc: 0.7586206896551724 - val_loss: 0.6478684297602313\n",
      "EarlyStopping counter: 6 out of 50\n",
      "epoch: 43 - train_acc: 0.9368932038834952 - train_loss: 0.15499724932181386 - val_acc: 0.6206896551724138 - val_loss: 1.2906309897631223\n",
      "EarlyStopping counter: 7 out of 50\n",
      "epoch: 44 - train_acc: 0.9514563106796117 - train_loss: 0.13169488600927254 - val_acc: 0.7241379310344828 - val_loss: 0.547227022835196\n",
      "EarlyStopping counter: 8 out of 50\n",
      "epoch: 45 - train_acc: 0.9563106796116505 - train_loss: 0.13688670455097493 - val_acc: 0.8275862068965517 - val_loss: 0.5382550984483951\n",
      "EarlyStopping counter: 9 out of 50\n",
      "epoch: 46 - train_acc: 0.9538834951456311 - train_loss: 0.13943563765980613 - val_acc: 0.6206896551724138 - val_loss: 0.9539319455180555\n",
      "EarlyStopping counter: 10 out of 50\n",
      "epoch: 47 - train_acc: 0.9660194174757282 - train_loss: 0.1275806935766892 - val_acc: 0.8275862068965517 - val_loss: 0.5833666489150746\n",
      "EarlyStopping counter: 11 out of 50\n",
      "epoch: 48 - train_acc: 0.9757281553398058 - train_loss: 0.08369507895888254 - val_acc: 0.7586206896551724 - val_loss: 0.36349780860630654\n",
      "EarlyStopping counter: 12 out of 50\n",
      "epoch: 49 - train_acc: 0.9733009708737864 - train_loss: 0.07431786771974531 - val_acc: 0.7586206896551724 - val_loss: 0.44449248071410596\n",
      "EarlyStopping counter: 13 out of 50\n",
      "epoch: 50 - train_acc: 0.9830097087378641 - train_loss: 0.07174105342278883 - val_acc: 0.7586206896551724 - val_loss: 0.5112889295504356\n",
      "EarlyStopping counter: 14 out of 50\n",
      "epoch: 51 - train_acc: 0.9757281553398058 - train_loss: 0.06573840594110339 - val_acc: 0.8275862068965517 - val_loss: 0.9320004679054887\n",
      "EarlyStopping counter: 15 out of 50\n",
      "epoch: 52 - train_acc: 0.9830097087378641 - train_loss: 0.06253733356930315 - val_acc: 0.7586206896551724 - val_loss: 0.386322314924453\n",
      "EarlyStopping counter: 16 out of 50\n",
      "epoch: 53 - train_acc: 0.9490291262135923 - train_loss: 0.11707957421214618 - val_acc: 0.7241379310344828 - val_loss: 0.5086673522820526\n",
      "EarlyStopping counter: 17 out of 50\n",
      "epoch: 54 - train_acc: 0.9587378640776699 - train_loss: 0.10931020768050341 - val_acc: 0.7241379310344828 - val_loss: 0.4182701059121636\n",
      "Validation loss decreased (0.345291 --> 0.296548).  Saving model ...\n",
      "epoch: 55 - train_acc: 0.9441747572815534 - train_loss: 0.16778301555024766 - val_acc: 0.7931034482758621 - val_loss: 0.29654784440837006\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 56 - train_acc: 0.9733009708737864 - train_loss: 0.09490003267753536 - val_acc: 0.7931034482758621 - val_loss: 0.45492772401288445\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 57 - train_acc: 0.970873786407767 - train_loss: 0.0870520633999967 - val_acc: 0.896551724137931 - val_loss: 0.3869133790231739\n",
      "EarlyStopping counter: 3 out of 50\n",
      "epoch: 58 - train_acc: 0.9611650485436893 - train_loss: 0.10610600331306277 - val_acc: 0.6896551724137931 - val_loss: 0.7184913681978365\n",
      "EarlyStopping counter: 4 out of 50\n",
      "epoch: 59 - train_acc: 0.9830097087378641 - train_loss: 0.09260148172111445 - val_acc: 0.8275862068965517 - val_loss: 0.3760508316762268\n",
      "EarlyStopping counter: 5 out of 50\n",
      "epoch: 60 - train_acc: 0.9490291262135923 - train_loss: 0.11260663898317394 - val_acc: 0.7241379310344828 - val_loss: 0.40561193215117\n",
      "EarlyStopping counter: 6 out of 50\n",
      "epoch: 61 - train_acc: 0.9514563106796117 - train_loss: 0.10834289943073842 - val_acc: 0.7586206896551724 - val_loss: 0.35773325548002344\n",
      "EarlyStopping counter: 7 out of 50\n",
      "epoch: 62 - train_acc: 0.9781553398058253 - train_loss: 0.06451284287925398 - val_acc: 0.7586206896551724 - val_loss: 0.3140416176503052\n",
      "EarlyStopping counter: 8 out of 50\n",
      "epoch: 63 - train_acc: 0.970873786407767 - train_loss: 0.07675730514374834 - val_acc: 0.7586206896551724 - val_loss: 0.5643872977923299\n",
      "EarlyStopping counter: 9 out of 50\n",
      "epoch: 64 - train_acc: 0.970873786407767 - train_loss: 0.08472404470999764 - val_acc: 0.6896551724137931 - val_loss: 0.5862856617618113\n",
      "EarlyStopping counter: 10 out of 50\n",
      "epoch: 65 - train_acc: 0.9830097087378641 - train_loss: 0.06469511865411062 - val_acc: 0.7586206896551724 - val_loss: 1.2270964256645593\n",
      "EarlyStopping counter: 11 out of 50\n",
      "epoch: 66 - train_acc: 0.9587378640776699 - train_loss: 0.09272677240900015 - val_acc: 0.7586206896551724 - val_loss: 0.6173521609616832\n",
      "EarlyStopping counter: 12 out of 50\n",
      "epoch: 67 - train_acc: 0.9563106796116505 - train_loss: 0.1167531378867298 - val_acc: 0.8275862068965517 - val_loss: 0.6842789383380551\n",
      "EarlyStopping counter: 13 out of 50\n",
      "epoch: 68 - train_acc: 0.970873786407767 - train_loss: 0.07602855349524329 - val_acc: 0.8275862068965517 - val_loss: 0.3867207082069758\n",
      "EarlyStopping counter: 14 out of 50\n",
      "epoch: 69 - train_acc: 0.9781553398058253 - train_loss: 0.06500616240786418 - val_acc: 0.8275862068965517 - val_loss: 0.7850688602813418\n",
      "EarlyStopping counter: 15 out of 50\n",
      "epoch: 70 - train_acc: 0.9830097087378641 - train_loss: 0.05634271480249859 - val_acc: 0.7241379310344828 - val_loss: 0.4124033924306114\n",
      "EarlyStopping counter: 16 out of 50\n",
      "epoch: 71 - train_acc: 0.9902912621359223 - train_loss: 0.03957767205969447 - val_acc: 0.7586206896551724 - val_loss: 0.3559592616103358\n",
      "EarlyStopping counter: 17 out of 50\n",
      "epoch: 72 - train_acc: 0.9830097087378641 - train_loss: 0.039511361515121565 - val_acc: 0.7586206896551724 - val_loss: 0.6232898086709777\n",
      "EarlyStopping counter: 18 out of 50\n",
      "epoch: 73 - train_acc: 0.9902912621359223 - train_loss: 0.03518380670933087 - val_acc: 0.7931034482758621 - val_loss: 0.5669022613840519\n",
      "EarlyStopping counter: 19 out of 50\n",
      "epoch: 74 - train_acc: 0.9781553398058253 - train_loss: 0.06538051797734615 - val_acc: 0.7931034482758621 - val_loss: 0.5380582386369681\n",
      "EarlyStopping counter: 20 out of 50\n",
      "epoch: 75 - train_acc: 0.970873786407767 - train_loss: 0.07835263435407538 - val_acc: 0.8275862068965517 - val_loss: 0.33535241894238854\n",
      "EarlyStopping counter: 21 out of 50\n",
      "epoch: 76 - train_acc: 0.9781553398058253 - train_loss: 0.05311100438152607 - val_acc: 0.896551724137931 - val_loss: 0.3607049067644523\n",
      "EarlyStopping counter: 22 out of 50\n",
      "epoch: 77 - train_acc: 0.9830097087378641 - train_loss: 0.07237985460679232 - val_acc: 0.7241379310344828 - val_loss: 1.0929931631879946\n",
      "EarlyStopping counter: 23 out of 50\n",
      "epoch: 78 - train_acc: 0.9733009708737864 - train_loss: 0.08517075117770473 - val_acc: 0.7931034482758621 - val_loss: 0.5467390721075506\n",
      "EarlyStopping counter: 24 out of 50\n",
      "epoch: 79 - train_acc: 0.9733009708737864 - train_loss: 0.07974677599498811 - val_acc: 0.7241379310344828 - val_loss: 0.4778800921721251\n",
      "EarlyStopping counter: 25 out of 50\n",
      "epoch: 80 - train_acc: 0.9587378640776699 - train_loss: 0.13029551825257119 - val_acc: 0.8275862068965517 - val_loss: 0.3927726636443209\n",
      "EarlyStopping counter: 26 out of 50\n",
      "epoch: 81 - train_acc: 0.9393203883495146 - train_loss: 0.14770506873224332 - val_acc: 0.6551724137931034 - val_loss: 1.1053789667054177\n",
      "EarlyStopping counter: 27 out of 50\n",
      "epoch: 82 - train_acc: 0.9635922330097088 - train_loss: 0.0870957184177571 - val_acc: 0.7241379310344828 - val_loss: 0.7073621370239102\n",
      "EarlyStopping counter: 28 out of 50\n",
      "epoch: 83 - train_acc: 0.9733009708737864 - train_loss: 0.07359330821659732 - val_acc: 0.8620689655172413 - val_loss: 0.34245222402417613\n",
      "EarlyStopping counter: 29 out of 50\n",
      "epoch: 84 - train_acc: 0.9781553398058253 - train_loss: 0.07682792318190583 - val_acc: 0.8275862068965517 - val_loss: 0.9047400056445338\n",
      "EarlyStopping counter: 30 out of 50\n",
      "epoch: 85 - train_acc: 0.9878640776699029 - train_loss: 0.039941467001398355 - val_acc: 0.8275862068965517 - val_loss: 0.4554427241132796\n",
      "EarlyStopping counter: 31 out of 50\n",
      "epoch: 86 - train_acc: 0.9902912621359223 - train_loss: 0.04884163575025046 - val_acc: 0.8275862068965517 - val_loss: 0.41727232534843955\n",
      "EarlyStopping counter: 32 out of 50\n",
      "epoch: 87 - train_acc: 0.9854368932038835 - train_loss: 0.0426598451298766 - val_acc: 0.8620689655172413 - val_loss: 0.46895578955573614\n",
      "EarlyStopping counter: 33 out of 50\n",
      "epoch: 88 - train_acc: 0.9660194174757282 - train_loss: 0.07792141458317164 - val_acc: 0.7586206896551724 - val_loss: 0.49868998172848067\n",
      "EarlyStopping counter: 34 out of 50\n",
      "epoch: 89 - train_acc: 0.9733009708737864 - train_loss: 0.0793169639035818 - val_acc: 0.7586206896551724 - val_loss: 0.35003173085595163\n",
      "EarlyStopping counter: 35 out of 50\n",
      "epoch: 90 - train_acc: 0.9684466019417476 - train_loss: 0.0735103645525646 - val_acc: 0.8620689655172413 - val_loss: 0.4132704404801449\n",
      "EarlyStopping counter: 36 out of 50\n",
      "epoch: 91 - train_acc: 0.9854368932038835 - train_loss: 0.055205124632010556 - val_acc: 0.6896551724137931 - val_loss: 0.6222558935625503\n",
      "EarlyStopping counter: 37 out of 50\n",
      "epoch: 92 - train_acc: 0.9587378640776699 - train_loss: 0.08520245803933142 - val_acc: 0.7931034482758621 - val_loss: 0.48925215561048685\n",
      "EarlyStopping counter: 38 out of 50\n",
      "epoch: 93 - train_acc: 0.9854368932038835 - train_loss: 0.05184612813368836 - val_acc: 0.896551724137931 - val_loss: 0.33532440696988647\n",
      "EarlyStopping counter: 39 out of 50\n",
      "epoch: 94 - train_acc: 0.9927184466019418 - train_loss: 0.033975994554659154 - val_acc: 0.8620689655172413 - val_loss: 0.37598027959923536\n",
      "EarlyStopping counter: 40 out of 50\n",
      "epoch: 95 - train_acc: 0.9975728155339806 - train_loss: 0.022774962898078777 - val_acc: 0.7931034482758621 - val_loss: 0.5891128545272883\n",
      "Validation loss decreased (0.296548 --> 0.273580).  Saving model ...\n",
      "epoch: 96 - train_acc: 0.9927184466019418 - train_loss: 0.020712820377041103 - val_acc: 0.8275862068965517 - val_loss: 0.273580450293097\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 97 - train_acc: 0.9878640776699029 - train_loss: 0.035282909626132127 - val_acc: 0.7931034482758621 - val_loss: 0.5345950135220585\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 98 - train_acc: 0.9951456310679612 - train_loss: 0.03302212361326021 - val_acc: 0.7931034482758621 - val_loss: 0.5022487261262312\n",
      "EarlyStopping counter: 3 out of 50\n",
      "epoch: 99 - train_acc: 0.9878640776699029 - train_loss: 0.04397953334926658 - val_acc: 0.8620689655172413 - val_loss: 0.6830142619962674\n",
      "EarlyStopping counter: 4 out of 50\n",
      "epoch: 100 - train_acc: 1.0 - train_loss: 0.019712127214378698 - val_acc: 0.8620689655172413 - val_loss: 0.6909155172828163\n",
      "EarlyStopping counter: 5 out of 50\n",
      "epoch: 101 - train_acc: 0.9902912621359223 - train_loss: 0.031193485568127736 - val_acc: 0.7586206896551724 - val_loss: 0.8995913121153456\n",
      "EarlyStopping counter: 6 out of 50\n",
      "epoch: 102 - train_acc: 0.9951456310679612 - train_loss: 0.020660794161117848 - val_acc: 0.7241379310344828 - val_loss: 0.4093946712919608\n",
      "EarlyStopping counter: 7 out of 50\n",
      "epoch: 103 - train_acc: 0.9951456310679612 - train_loss: 0.02561520987725077 - val_acc: 0.8275862068965517 - val_loss: 0.41068213702307804\n",
      "EarlyStopping counter: 8 out of 50\n",
      "epoch: 104 - train_acc: 0.9951456310679612 - train_loss: 0.024975491046958585 - val_acc: 0.8275862068965517 - val_loss: 0.4136550929695197\n",
      "EarlyStopping counter: 9 out of 50\n",
      "epoch: 105 - train_acc: 0.9902912621359223 - train_loss: 0.0338462658399742 - val_acc: 0.8620689655172413 - val_loss: 0.39423762391105877\n",
      "EarlyStopping counter: 10 out of 50\n",
      "epoch: 106 - train_acc: 0.9854368932038835 - train_loss: 0.028472763812169385 - val_acc: 0.7931034482758621 - val_loss: 0.35948126456915797\n",
      "EarlyStopping counter: 11 out of 50\n",
      "epoch: 107 - train_acc: 0.9902912621359223 - train_loss: 0.034464695632740454 - val_acc: 0.7931034482758621 - val_loss: 0.4239576985071197\n",
      "EarlyStopping counter: 12 out of 50\n",
      "epoch: 108 - train_acc: 0.9733009708737864 - train_loss: 0.08398395554877462 - val_acc: 0.7931034482758621 - val_loss: 0.9082210404606483\n",
      "EarlyStopping counter: 13 out of 50\n",
      "epoch: 109 - train_acc: 0.9611650485436893 - train_loss: 0.09207646943897528 - val_acc: 0.7931034482758621 - val_loss: 0.45435148364543876\n",
      "EarlyStopping counter: 14 out of 50\n",
      "epoch: 110 - train_acc: 0.9951456310679612 - train_loss: 0.053998170270133074 - val_acc: 0.7586206896551724 - val_loss: 0.8216988503464051\n",
      "EarlyStopping counter: 15 out of 50\n",
      "epoch: 111 - train_acc: 0.9781553398058253 - train_loss: 0.06697611647649192 - val_acc: 0.7586206896551724 - val_loss: 0.9667791252649596\n",
      "EarlyStopping counter: 16 out of 50\n",
      "epoch: 112 - train_acc: 0.9902912621359223 - train_loss: 0.03271905991139744 - val_acc: 0.7931034482758621 - val_loss: 0.36556098723085706\n",
      "Validation loss decreased (0.273580 --> 0.208482).  Saving model ...\n",
      "epoch: 113 - train_acc: 0.9927184466019418 - train_loss: 0.020886079663393523 - val_acc: 0.9310344827586207 - val_loss: 0.20848177375164056\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 114 - train_acc: 0.9951456310679612 - train_loss: 0.020598324370796953 - val_acc: 0.8275862068965517 - val_loss: 0.6122694022335133\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 115 - train_acc: 0.9927184466019418 - train_loss: 0.03008011137067966 - val_acc: 0.7931034482758621 - val_loss: 0.5030183288214549\n",
      "EarlyStopping counter: 3 out of 50\n",
      "epoch: 116 - train_acc: 0.9805825242718447 - train_loss: 0.052955805426109316 - val_acc: 0.7931034482758621 - val_loss: 0.7301267847977213\n",
      "EarlyStopping counter: 4 out of 50\n",
      "epoch: 117 - train_acc: 0.9830097087378641 - train_loss: 0.07817368690025354 - val_acc: 0.7241379310344828 - val_loss: 0.7481563575335614\n",
      "EarlyStopping counter: 5 out of 50\n",
      "epoch: 118 - train_acc: 0.9733009708737864 - train_loss: 0.07177689601551679 - val_acc: 0.7931034482758621 - val_loss: 0.32362981737350804\n",
      "EarlyStopping counter: 6 out of 50\n",
      "epoch: 119 - train_acc: 0.9805825242718447 - train_loss: 0.06283593042751089 - val_acc: 0.7586206896551724 - val_loss: 0.8999648416543033\n",
      "EarlyStopping counter: 7 out of 50\n",
      "epoch: 120 - train_acc: 0.9830097087378641 - train_loss: 0.03849385134331073 - val_acc: 0.7586206896551724 - val_loss: 0.5788546941606364\n",
      "EarlyStopping counter: 8 out of 50\n",
      "epoch: 121 - train_acc: 0.9878640776699029 - train_loss: 0.03663386042259051 - val_acc: 0.7586206896551724 - val_loss: 0.8819415200091654\n",
      "EarlyStopping counter: 9 out of 50\n",
      "epoch: 122 - train_acc: 1.0 - train_loss: 0.015945101264401296 - val_acc: 0.7241379310344828 - val_loss: 0.5187868214151606\n",
      "EarlyStopping counter: 10 out of 50\n",
      "epoch: 123 - train_acc: 0.9975728155339806 - train_loss: 0.017374448042208696 - val_acc: 0.6896551724137931 - val_loss: 0.6543224776871922\n",
      "EarlyStopping counter: 11 out of 50\n",
      "epoch: 124 - train_acc: 0.9854368932038835 - train_loss: 0.038405391017413415 - val_acc: 0.7931034482758621 - val_loss: 0.4644431590393219\n",
      "EarlyStopping counter: 12 out of 50\n",
      "epoch: 125 - train_acc: 0.9951456310679612 - train_loss: 0.018063824925255444 - val_acc: 0.7931034482758621 - val_loss: 1.0377384697623449\n",
      "EarlyStopping counter: 13 out of 50\n",
      "epoch: 126 - train_acc: 0.9927184466019418 - train_loss: 0.01737214259618415 - val_acc: 0.7931034482758621 - val_loss: 0.7902878015436351\n",
      "EarlyStopping counter: 14 out of 50\n",
      "epoch: 127 - train_acc: 0.9902912621359223 - train_loss: 0.023478827375736182 - val_acc: 0.7586206896551724 - val_loss: 0.5180098240675093\n",
      "EarlyStopping counter: 15 out of 50\n",
      "epoch: 128 - train_acc: 1.0 - train_loss: 0.007158625664969201 - val_acc: 0.7931034482758621 - val_loss: 0.5742846578369709\n",
      "EarlyStopping counter: 16 out of 50\n",
      "epoch: 129 - train_acc: 0.9975728155339806 - train_loss: 0.010996139971315821 - val_acc: 0.7241379310344828 - val_loss: 0.48411688041323575\n",
      "EarlyStopping counter: 17 out of 50\n",
      "epoch: 130 - train_acc: 0.9927184466019418 - train_loss: 0.02420066736110602 - val_acc: 0.6206896551724138 - val_loss: 0.9689547173867499\n",
      "EarlyStopping counter: 18 out of 50\n",
      "epoch: 131 - train_acc: 1.0 - train_loss: 0.020222521423888546 - val_acc: 0.7241379310344828 - val_loss: 0.8248438189755702\n",
      "EarlyStopping counter: 19 out of 50\n",
      "epoch: 132 - train_acc: 0.9878640776699029 - train_loss: 0.02746987295307403 - val_acc: 0.7586206896551724 - val_loss: 0.4667080398606101\n",
      "EarlyStopping counter: 20 out of 50\n",
      "epoch: 133 - train_acc: 0.9878640776699029 - train_loss: 0.03285830230554742 - val_acc: 0.7586206896551724 - val_loss: 0.8213016855434377\n",
      "EarlyStopping counter: 21 out of 50\n",
      "epoch: 134 - train_acc: 0.9781553398058253 - train_loss: 0.06787457471472313 - val_acc: 0.7586206896551724 - val_loss: 0.7649800268124652\n",
      "EarlyStopping counter: 22 out of 50\n",
      "epoch: 135 - train_acc: 0.9902912621359223 - train_loss: 0.03417888243013314 - val_acc: 0.7586206896551724 - val_loss: 0.7455766723355997\n",
      "EarlyStopping counter: 23 out of 50\n",
      "epoch: 136 - train_acc: 0.9805825242718447 - train_loss: 0.05376989572746581 - val_acc: 0.8275862068965517 - val_loss: 0.4062735363932899\n",
      "EarlyStopping counter: 24 out of 50\n",
      "epoch: 137 - train_acc: 0.9927184466019418 - train_loss: 0.03450359515451237 - val_acc: 0.6551724137931034 - val_loss: 0.5232343778794706\n",
      "EarlyStopping counter: 25 out of 50\n",
      "epoch: 138 - train_acc: 0.9927184466019418 - train_loss: 0.03349845639901716 - val_acc: 0.7241379310344828 - val_loss: 1.1678502975231768\n",
      "EarlyStopping counter: 26 out of 50\n",
      "epoch: 139 - train_acc: 0.9902912621359223 - train_loss: 0.022902720713051248 - val_acc: 0.7586206896551724 - val_loss: 0.4528001213531456\n",
      "EarlyStopping counter: 27 out of 50\n",
      "epoch: 140 - train_acc: 1.0 - train_loss: 0.008271261947041033 - val_acc: 0.7586206896551724 - val_loss: 0.5190112455623005\n",
      "EarlyStopping counter: 28 out of 50\n",
      "epoch: 141 - train_acc: 0.9975728155339806 - train_loss: 0.012988583251617496 - val_acc: 0.8275862068965517 - val_loss: 0.629409772658899\n",
      "EarlyStopping counter: 29 out of 50\n",
      "epoch: 142 - train_acc: 0.9781553398058253 - train_loss: 0.04793747453341902 - val_acc: 0.7931034482758621 - val_loss: 0.8019496712966017\n",
      "EarlyStopping counter: 30 out of 50\n",
      "epoch: 143 - train_acc: 0.970873786407767 - train_loss: 0.06648518376787123 - val_acc: 0.7586206896551724 - val_loss: 0.626074264345784\n",
      "EarlyStopping counter: 31 out of 50\n",
      "epoch: 144 - train_acc: 0.9854368932038835 - train_loss: 0.06234779917319555 - val_acc: 0.8275862068965517 - val_loss: 0.28595498003413655\n",
      "EarlyStopping counter: 32 out of 50\n",
      "epoch: 145 - train_acc: 0.9660194174757282 - train_loss: 0.07436654468874153 - val_acc: 0.7931034482758621 - val_loss: 0.5604401829821293\n",
      "EarlyStopping counter: 33 out of 50\n",
      "epoch: 146 - train_acc: 0.9975728155339806 - train_loss: 0.02103140341683047 - val_acc: 0.7586206896551724 - val_loss: 1.0654114052593273\n",
      "EarlyStopping counter: 34 out of 50\n",
      "epoch: 147 - train_acc: 0.9951456310679612 - train_loss: 0.012844979298318163 - val_acc: 0.7931034482758621 - val_loss: 0.7834766808582152\n",
      "EarlyStopping counter: 35 out of 50\n",
      "epoch: 148 - train_acc: 0.9902912621359223 - train_loss: 0.033245029794224626 - val_acc: 0.7931034482758621 - val_loss: 0.4151810325831925\n",
      "EarlyStopping counter: 36 out of 50\n",
      "epoch: 149 - train_acc: 0.9878640776699029 - train_loss: 0.03788608441503837 - val_acc: 0.7241379310344828 - val_loss: 0.6781203726814884\n",
      "EarlyStopping counter: 37 out of 50\n",
      "epoch: 150 - train_acc: 0.9902912621359223 - train_loss: 0.028508154125254148 - val_acc: 0.7586206896551724 - val_loss: 0.6410869179222154\n",
      "EarlyStopping counter: 38 out of 50\n",
      "epoch: 151 - train_acc: 0.9951456310679612 - train_loss: 0.017797824043907857 - val_acc: 0.7586206896551724 - val_loss: 0.7815211078307281\n",
      "EarlyStopping counter: 39 out of 50\n",
      "epoch: 152 - train_acc: 0.9975728155339806 - train_loss: 0.007819836239588308 - val_acc: 0.8275862068965517 - val_loss: 0.5596879671564855\n",
      "EarlyStopping counter: 40 out of 50\n",
      "epoch: 153 - train_acc: 0.9902912621359223 - train_loss: 0.027490385625739598 - val_acc: 0.7931034482758621 - val_loss: 0.3746813739904637\n",
      "EarlyStopping counter: 41 out of 50\n",
      "epoch: 154 - train_acc: 0.9902912621359223 - train_loss: 0.03270864007114275 - val_acc: 0.7241379310344828 - val_loss: 0.9191806269775984\n",
      "EarlyStopping counter: 42 out of 50\n",
      "epoch: 155 - train_acc: 0.9902912621359223 - train_loss: 0.023523207840827322 - val_acc: 0.7241379310344828 - val_loss: 1.0655082346346116\n",
      "EarlyStopping counter: 43 out of 50\n",
      "epoch: 156 - train_acc: 0.9951456310679612 - train_loss: 0.01577657768856624 - val_acc: 0.7931034482758621 - val_loss: 0.3383018106220692\n",
      "EarlyStopping counter: 44 out of 50\n",
      "epoch: 157 - train_acc: 1.0 - train_loss: 0.005749434885473737 - val_acc: 0.8275862068965517 - val_loss: 0.5850712685749253\n",
      "EarlyStopping counter: 45 out of 50\n",
      "epoch: 158 - train_acc: 0.9975728155339806 - train_loss: 0.008560845151696113 - val_acc: 0.8620689655172413 - val_loss: 0.4243041469178662\n",
      "EarlyStopping counter: 46 out of 50\n",
      "epoch: 159 - train_acc: 0.9951456310679612 - train_loss: 0.014125393663691383 - val_acc: 0.7931034482758621 - val_loss: 0.9433479367944695\n",
      "EarlyStopping counter: 47 out of 50\n",
      "epoch: 160 - train_acc: 0.9927184466019418 - train_loss: 0.03280584021121672 - val_acc: 0.6896551724137931 - val_loss: 0.559063672435765\n",
      "EarlyStopping counter: 48 out of 50\n",
      "epoch: 161 - train_acc: 0.9975728155339806 - train_loss: 0.01224478142984798 - val_acc: 0.8275862068965517 - val_loss: 0.5444258575178493\n",
      "EarlyStopping counter: 49 out of 50\n",
      "epoch: 162 - train_acc: 0.9951456310679612 - train_loss: 0.016126493394935288 - val_acc: 0.7586206896551724 - val_loss: 0.8071407569955003\n",
      "EarlyStopping counter: 50 out of 50\n",
      "epoch: 163 - train_acc: 0.9951456310679612 - train_loss: 0.019850081405505687 - val_acc: 0.7241379310344828 - val_loss: 0.4733196459676464\n",
      "=> Early stopped !!\n",
      "Accuracy: 0.7098214285714286\n",
      "Precision: 0.7074829931972789\n",
      "Recall: 0.8253968253968254\n",
      "F1: 0.7619047619047619\n",
      "\n",
      "[Fold 14]: \n",
      "len(train_sampler)=412\n",
      "len(val_sampler)=29\n",
      "[ 20  71  87  99 102 106 121 130 149 151 160 188 191 214 257 270 276 293\n",
      " 308 313 330 343 348 359 372 385 413 435 440]\n",
      "Validation loss decreased (inf --> 0.869283).  Saving model ...\n",
      "epoch: 0 - train_acc: 0.5849514563106796 - train_loss: 0.6836584612792516 - val_acc: 0.4827586206896552 - val_loss: 0.8692833200231312\n",
      "Validation loss decreased (0.869283 --> 0.665910).  Saving model ...\n",
      "epoch: 1 - train_acc: 0.633495145631068 - train_loss: 0.6328632114978211 - val_acc: 0.5172413793103449 - val_loss: 0.6659095469747695\n",
      "Validation loss decreased (0.665910 --> 0.639463).  Saving model ...\n",
      "epoch: 2 - train_acc: 0.691747572815534 - train_loss: 0.5850600845213386 - val_acc: 0.5862068965517241 - val_loss: 0.6394627576940072\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 3 - train_acc: 0.7087378640776699 - train_loss: 0.5620107038323722 - val_acc: 0.6206896551724138 - val_loss: 0.7184329632713833\n",
      "Validation loss decreased (0.639463 --> 0.339686).  Saving model ...\n",
      "epoch: 4 - train_acc: 0.7597087378640777 - train_loss: 0.5059020292131551 - val_acc: 0.8275862068965517 - val_loss: 0.3396856312842169\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 5 - train_acc: 0.7524271844660194 - train_loss: 0.49608825148562324 - val_acc: 0.6551724137931034 - val_loss: 0.6445032915396987\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 6 - train_acc: 0.7694174757281553 - train_loss: 0.4767076089828102 - val_acc: 0.7931034482758621 - val_loss: 0.4015865812893214\n",
      "EarlyStopping counter: 3 out of 50\n",
      "epoch: 7 - train_acc: 0.7742718446601942 - train_loss: 0.45564913852447864 - val_acc: 0.7586206896551724 - val_loss: 0.37332380838434903\n",
      "EarlyStopping counter: 4 out of 50\n",
      "epoch: 8 - train_acc: 0.8106796116504854 - train_loss: 0.4124775169835965 - val_acc: 0.7586206896551724 - val_loss: 0.3546521772930617\n",
      "EarlyStopping counter: 5 out of 50\n",
      "epoch: 9 - train_acc: 0.8349514563106796 - train_loss: 0.3642897427250985 - val_acc: 0.7931034482758621 - val_loss: 0.4969985998014768\n",
      "EarlyStopping counter: 6 out of 50\n",
      "epoch: 10 - train_acc: 0.8398058252427184 - train_loss: 0.3670008974019757 - val_acc: 0.8620689655172413 - val_loss: 0.35918311737054603\n",
      "Validation loss decreased (0.339686 --> 0.326634).  Saving model ...\n",
      "epoch: 11 - train_acc: 0.8470873786407767 - train_loss: 0.35602483788269973 - val_acc: 0.7931034482758621 - val_loss: 0.32663417468424716\n",
      "Validation loss decreased (0.326634 --> 0.326068).  Saving model ...\n",
      "epoch: 12 - train_acc: 0.8640776699029126 - train_loss: 0.3233563571577933 - val_acc: 0.7931034482758621 - val_loss: 0.32606772528767036\n",
      "Validation loss decreased (0.326068 --> 0.277979).  Saving model ...\n",
      "epoch: 13 - train_acc: 0.8737864077669902 - train_loss: 0.32338212832945323 - val_acc: 0.8620689655172413 - val_loss: 0.27797907059804655\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 14 - train_acc: 0.8470873786407767 - train_loss: 0.31712554303437457 - val_acc: 0.7586206896551724 - val_loss: 0.4533365226620004\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 15 - train_acc: 0.8495145631067961 - train_loss: 0.311741607163 - val_acc: 0.7586206896551724 - val_loss: 0.3941639341204791\n",
      "EarlyStopping counter: 3 out of 50\n",
      "epoch: 16 - train_acc: 0.883495145631068 - train_loss: 0.2895422693693911 - val_acc: 0.7931034482758621 - val_loss: 0.397262926889947\n",
      "EarlyStopping counter: 4 out of 50\n",
      "epoch: 17 - train_acc: 0.8859223300970874 - train_loss: 0.28009552240855734 - val_acc: 0.7931034482758621 - val_loss: 0.3033953674131037\n",
      "EarlyStopping counter: 5 out of 50\n",
      "epoch: 18 - train_acc: 0.8786407766990292 - train_loss: 0.2967597204307809 - val_acc: 0.7586206896551724 - val_loss: 0.43379106570162573\n",
      "EarlyStopping counter: 6 out of 50\n",
      "epoch: 19 - train_acc: 0.8859223300970874 - train_loss: 0.26982302943623493 - val_acc: 0.7241379310344828 - val_loss: 0.6235312050518221\n",
      "Validation loss decreased (0.277979 --> 0.239854).  Saving model ...\n",
      "epoch: 20 - train_acc: 0.8786407766990292 - train_loss: 0.2610732139763397 - val_acc: 0.9310344827586207 - val_loss: 0.23985400357728492\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 21 - train_acc: 0.9174757281553398 - train_loss: 0.20649515911097102 - val_acc: 0.8275862068965517 - val_loss: 0.38178627937451004\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 22 - train_acc: 0.9053398058252428 - train_loss: 0.19990506526270196 - val_acc: 0.7586206896551724 - val_loss: 0.4197006476900924\n",
      "EarlyStopping counter: 3 out of 50\n",
      "epoch: 23 - train_acc: 0.9441747572815534 - train_loss: 0.1817860211782544 - val_acc: 0.8275862068965517 - val_loss: 0.3619921840325954\n",
      "EarlyStopping counter: 4 out of 50\n",
      "epoch: 24 - train_acc: 0.8762135922330098 - train_loss: 0.3048741527106054 - val_acc: 0.8620689655172413 - val_loss: 0.3770575778962387\n",
      "EarlyStopping counter: 5 out of 50\n",
      "epoch: 25 - train_acc: 0.8543689320388349 - train_loss: 0.33674399764808793 - val_acc: 0.8275862068965517 - val_loss: 1.0249499492214058\n",
      "EarlyStopping counter: 6 out of 50\n",
      "epoch: 26 - train_acc: 0.8859223300970874 - train_loss: 0.24936958565621176 - val_acc: 0.8620689655172413 - val_loss: 0.4960337708762716\n",
      "EarlyStopping counter: 7 out of 50\n",
      "epoch: 27 - train_acc: 0.9077669902912622 - train_loss: 0.20649726155635575 - val_acc: 0.8275862068965517 - val_loss: 0.4067249194767002\n",
      "EarlyStopping counter: 8 out of 50\n",
      "epoch: 28 - train_acc: 0.9247572815533981 - train_loss: 0.1885735718925646 - val_acc: 0.7931034482758621 - val_loss: 0.33082079294549993\n",
      "EarlyStopping counter: 9 out of 50\n",
      "epoch: 29 - train_acc: 0.9199029126213593 - train_loss: 0.1979108436492977 - val_acc: 0.8275862068965517 - val_loss: 0.47913698734494137\n",
      "EarlyStopping counter: 10 out of 50\n",
      "epoch: 30 - train_acc: 0.9223300970873787 - train_loss: 0.19103177993158504 - val_acc: 0.8275862068965517 - val_loss: 0.4007940569100263\n",
      "EarlyStopping counter: 11 out of 50\n",
      "epoch: 31 - train_acc: 0.9344660194174758 - train_loss: 0.15274153334688148 - val_acc: 0.8620689655172413 - val_loss: 0.655790297259925\n",
      "EarlyStopping counter: 12 out of 50\n",
      "epoch: 32 - train_acc: 0.9490291262135923 - train_loss: 0.17552471752266954 - val_acc: 0.8275862068965517 - val_loss: 0.475530353060007\n",
      "EarlyStopping counter: 13 out of 50\n",
      "epoch: 33 - train_acc: 0.9199029126213593 - train_loss: 0.18503120920786675 - val_acc: 0.7931034482758621 - val_loss: 0.35795414320219915\n",
      "EarlyStopping counter: 14 out of 50\n",
      "epoch: 34 - train_acc: 0.9490291262135923 - train_loss: 0.12905967622380665 - val_acc: 0.8620689655172413 - val_loss: 0.7705492432426777\n",
      "EarlyStopping counter: 15 out of 50\n",
      "epoch: 35 - train_acc: 0.9441747572815534 - train_loss: 0.15020905066245316 - val_acc: 0.8275862068965517 - val_loss: 0.9546919593143999\n",
      "EarlyStopping counter: 16 out of 50\n",
      "epoch: 36 - train_acc: 0.912621359223301 - train_loss: 0.2310056762773436 - val_acc: 0.896551724137931 - val_loss: 1.5726362390805653\n",
      "EarlyStopping counter: 17 out of 50\n",
      "epoch: 37 - train_acc: 0.9199029126213593 - train_loss: 0.1957954477209758 - val_acc: 0.7586206896551724 - val_loss: 0.6417691710986022\n",
      "EarlyStopping counter: 18 out of 50\n",
      "epoch: 38 - train_acc: 0.9101941747572816 - train_loss: 0.2425215910042863 - val_acc: 0.8620689655172413 - val_loss: 0.46646929462276643\n",
      "EarlyStopping counter: 19 out of 50\n",
      "epoch: 39 - train_acc: 0.9247572815533981 - train_loss: 0.16626951740620127 - val_acc: 0.8620689655172413 - val_loss: 0.645301569817475\n",
      "EarlyStopping counter: 20 out of 50\n",
      "epoch: 40 - train_acc: 0.9660194174757282 - train_loss: 0.11385603654469419 - val_acc: 0.8620689655172413 - val_loss: 0.4211929656204598\n",
      "EarlyStopping counter: 21 out of 50\n",
      "epoch: 41 - train_acc: 0.9393203883495146 - train_loss: 0.14019722275006397 - val_acc: 0.896551724137931 - val_loss: 0.3616887759255738\n",
      "EarlyStopping counter: 22 out of 50\n",
      "epoch: 42 - train_acc: 0.9611650485436893 - train_loss: 0.1496115666308064 - val_acc: 0.8620689655172413 - val_loss: 0.49996371927515715\n",
      "EarlyStopping counter: 23 out of 50\n",
      "epoch: 43 - train_acc: 0.9781553398058253 - train_loss: 0.08709032330469846 - val_acc: 0.8275862068965517 - val_loss: 0.5172307651955586\n",
      "EarlyStopping counter: 24 out of 50\n",
      "epoch: 44 - train_acc: 0.9635922330097088 - train_loss: 0.1034481817878463 - val_acc: 0.7931034482758621 - val_loss: 1.4973693094965517\n",
      "EarlyStopping counter: 25 out of 50\n",
      "epoch: 45 - train_acc: 0.9611650485436893 - train_loss: 0.09852403487755296 - val_acc: 0.8275862068965517 - val_loss: 0.4993542187487979\n",
      "EarlyStopping counter: 26 out of 50\n",
      "epoch: 46 - train_acc: 0.970873786407767 - train_loss: 0.1253676152812802 - val_acc: 0.7931034482758621 - val_loss: 0.4910340832829596\n",
      "EarlyStopping counter: 27 out of 50\n",
      "epoch: 47 - train_acc: 0.9538834951456311 - train_loss: 0.12174667195167767 - val_acc: 0.7931034482758621 - val_loss: 0.5841683136923838\n",
      "EarlyStopping counter: 28 out of 50\n",
      "epoch: 48 - train_acc: 0.9563106796116505 - train_loss: 0.13262988310794807 - val_acc: 0.8620689655172413 - val_loss: 0.5847100322087191\n",
      "EarlyStopping counter: 29 out of 50\n",
      "epoch: 49 - train_acc: 0.9538834951456311 - train_loss: 0.11944137028540941 - val_acc: 0.7931034482758621 - val_loss: 1.0708099341199433\n",
      "EarlyStopping counter: 30 out of 50\n",
      "epoch: 50 - train_acc: 0.9563106796116505 - train_loss: 0.12344425750212287 - val_acc: 0.8620689655172413 - val_loss: 1.4426420393805885\n",
      "EarlyStopping counter: 31 out of 50\n",
      "epoch: 51 - train_acc: 0.9538834951456311 - train_loss: 0.1261191198877465 - val_acc: 0.8275862068965517 - val_loss: 0.529082958335824\n",
      "EarlyStopping counter: 32 out of 50\n",
      "epoch: 52 - train_acc: 0.970873786407767 - train_loss: 0.09240563398443488 - val_acc: 0.8275862068965517 - val_loss: 0.6020305959147451\n",
      "EarlyStopping counter: 33 out of 50\n",
      "epoch: 53 - train_acc: 0.9878640776699029 - train_loss: 0.06318387683314364 - val_acc: 0.9310344827586207 - val_loss: 0.3354218100588668\n",
      "EarlyStopping counter: 34 out of 50\n",
      "epoch: 54 - train_acc: 0.9611650485436893 - train_loss: 0.10043382824799034 - val_acc: 0.7931034482758621 - val_loss: 0.8140549704937065\n",
      "EarlyStopping counter: 35 out of 50\n",
      "epoch: 55 - train_acc: 0.9611650485436893 - train_loss: 0.11564338760878816 - val_acc: 0.896551724137931 - val_loss: 0.4877164455161975\n",
      "EarlyStopping counter: 36 out of 50\n",
      "epoch: 56 - train_acc: 0.9781553398058253 - train_loss: 0.06739350483754782 - val_acc: 0.7931034482758621 - val_loss: 0.4875045567428479\n",
      "EarlyStopping counter: 37 out of 50\n",
      "epoch: 57 - train_acc: 0.9684466019417476 - train_loss: 0.07447769229881485 - val_acc: 0.8620689655172413 - val_loss: 0.3504865078134665\n",
      "EarlyStopping counter: 38 out of 50\n",
      "epoch: 58 - train_acc: 0.9781553398058253 - train_loss: 0.0656615652468336 - val_acc: 0.9310344827586207 - val_loss: 0.24628861909690866\n",
      "EarlyStopping counter: 39 out of 50\n",
      "epoch: 59 - train_acc: 0.970873786407767 - train_loss: 0.09616083413263453 - val_acc: 0.8620689655172413 - val_loss: 0.3406594848324961\n",
      "EarlyStopping counter: 40 out of 50\n",
      "epoch: 60 - train_acc: 0.9587378640776699 - train_loss: 0.09208918472302823 - val_acc: 0.8275862068965517 - val_loss: 0.4777794153547271\n",
      "EarlyStopping counter: 41 out of 50\n",
      "epoch: 61 - train_acc: 0.9635922330097088 - train_loss: 0.10239332173173013 - val_acc: 0.8275862068965517 - val_loss: 0.5821295795750792\n",
      "EarlyStopping counter: 42 out of 50\n",
      "epoch: 62 - train_acc: 0.9441747572815534 - train_loss: 0.15191288266522748 - val_acc: 0.8275862068965517 - val_loss: 0.5781966795821323\n",
      "EarlyStopping counter: 43 out of 50\n",
      "epoch: 63 - train_acc: 0.9684466019417476 - train_loss: 0.09245277619950333 - val_acc: 0.8275862068965517 - val_loss: 0.6053331690018428\n",
      "EarlyStopping counter: 44 out of 50\n",
      "epoch: 64 - train_acc: 0.9587378640776699 - train_loss: 0.10550482278882248 - val_acc: 0.7931034482758621 - val_loss: 0.4352001615549709\n",
      "EarlyStopping counter: 45 out of 50\n",
      "epoch: 65 - train_acc: 0.9902912621359223 - train_loss: 0.05329850348362976 - val_acc: 0.9310344827586207 - val_loss: 0.2792333173060448\n",
      "EarlyStopping counter: 46 out of 50\n",
      "epoch: 66 - train_acc: 0.9830097087378641 - train_loss: 0.05359122438679394 - val_acc: 0.8620689655172413 - val_loss: 0.5184159088918584\n",
      "EarlyStopping counter: 47 out of 50\n",
      "epoch: 67 - train_acc: 0.9635922330097088 - train_loss: 0.09525305309706136 - val_acc: 0.7931034482758621 - val_loss: 1.726479879735198\n",
      "EarlyStopping counter: 48 out of 50\n",
      "epoch: 68 - train_acc: 0.970873786407767 - train_loss: 0.09613282269556932 - val_acc: 0.8275862068965517 - val_loss: 0.950019572135654\n",
      "EarlyStopping counter: 49 out of 50\n",
      "epoch: 69 - train_acc: 0.9660194174757282 - train_loss: 0.1111918777484749 - val_acc: 0.8275862068965517 - val_loss: 0.29678595852131245\n",
      "EarlyStopping counter: 50 out of 50\n",
      "epoch: 70 - train_acc: 0.9660194174757282 - train_loss: 0.07932903841974256 - val_acc: 0.9655172413793104 - val_loss: 0.3579396024726752\n",
      "=> Early stopped !!\n",
      "Accuracy: 0.7410714285714286\n",
      "Precision: 0.753731343283582\n",
      "Recall: 0.8015873015873016\n",
      "F1: 0.7769230769230769\n"
     ]
    }
   ],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "                            Default: False\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.fname = 'checkpoints/P2B2_SeparableConv.pt'\n",
    "\n",
    "    def __call__(self, val_loss, model, fname):\n",
    "        \n",
    "        self.fname = fname\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.fname)\n",
    "        self.val_loss_min = val_loss\n",
    "\n",
    "\n",
    "\n",
    "def get_num_correct(preds_logits, labels):\n",
    "    return (preds_logits.argmax(dim=1) == labels).sum().item()\n",
    "\n",
    "def count_TP(preds_logits, labels):\n",
    "    preds = preds_logits.argmax(dim=1)\n",
    "    tmp = torch.add(preds, labels)\n",
    "    return (tmp == 2).sum().item()\n",
    "\n",
    "def count_TN(preds_logits, labels):\n",
    "    preds = preds_logits.argmax(dim=1)\n",
    "    tmp = torch.add(preds, labels)\n",
    "    return (tmp == 0).sum().item()\n",
    "\n",
    "def count_FP(preds_logits, labels):\n",
    "    preds = preds_logits.argmax(dim=1)\n",
    "    tmp = torch.subtract(preds, labels)\n",
    "    return (tmp == 1).sum().item()\n",
    "\n",
    "def count_FN(preds_logits, labels):\n",
    "    preds = preds_logits.argmax(dim=1)\n",
    "    tmp = torch.subtract(preds, labels)\n",
    "    return (tmp == -1).sum().item()\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def fixed_seed(seed_value):\n",
    "    # 1. Set `PYTHONHASHSEED` environment variable at a fixed value\n",
    "    os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "    \n",
    "    # 2. Set `python` built-in pseudo-random generator at a fixed value\n",
    "    random.seed(seed_value)\n",
    "\n",
    "    # 3. Set `numpy` pseudo-random generator at a fixed value\n",
    "    np.random.seed(seed_value)\n",
    "\n",
    "    # 4. Set `pytorch` pseudo-random generator at a fixed value\n",
    "    torch.manual_seed(seed_value)\n",
    "\n",
    "fixed_seed(42)\n",
    "\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "data = EyeBlinkDataset(train=True)\n",
    "data_test = EyeBlinkDataset(train=False)\n",
    "\n",
    "test_results = []\n",
    "\n",
    "EYE = 'right'\n",
    "\n",
    "kfold = KFold(n_splits=15, shuffle=True)\n",
    "for fold, (train_ids, val_ids) in enumerate(kfold.split(data)):\n",
    "    print(f'\\n[Fold {fold}]: ')\n",
    "    train_sampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
    "    val_sampler = torch.utils.data.SubsetRandomSampler(val_ids)\n",
    "    train_loader = torch.utils.data.DataLoader(data, batch_size=13, sampler=train_sampler, pin_memory=True)\n",
    "    val_loader = torch.utils.data.DataLoader(data, batch_size=13, sampler=val_sampler, pin_memory=True)\n",
    "    print(f'len(train_sampler)={len(train_sampler)}')\n",
    "    print(f'len(val_sampler)={len(val_sampler)}')\n",
    "    print(val_ids)\n",
    "\n",
    "    # model = P2B2().double()\n",
    "    model = P2B2_SeparableConv().double()\n",
    "    # model = P3B3().double()\n",
    "    # model = P3B3_SeparableConv().double()\n",
    "    model.to(device)\n",
    "\n",
    "    # loss_func = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "    min_loss = float('inf')\n",
    "    num_epochs = 500\n",
    "\n",
    "    early_stopping = EarlyStopping(patience=50, verbose=True)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        ## train set\n",
    "        avg_train_loss = 0\n",
    "        total_train_correct = 0\n",
    "        model.train()\n",
    "\n",
    "        for seqs, labels in train_loader:\n",
    "            seqs = seqs.permute((0,4,3,1,2)).to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            preds_logits = model(seqs)\n",
    "            \n",
    "            # print('---',seqs.shape)\n",
    "            # print(preds_logits.shape)\n",
    "            # print(labels.shape,'---')\n",
    "        \n",
    "            loss = F.cross_entropy(preds_logits, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            avg_train_loss += loss.detach().item() / len(train_loader)\n",
    "            total_train_correct += get_num_correct(preds_logits, labels)\n",
    "\n",
    "        ## val set\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            avg_val_loss = 0\n",
    "            total_val_correct = 0\n",
    "\n",
    "            for seqs, labels in val_loader:\n",
    "                seqs = seqs.permute((0,4,3,1,2)).to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                preds = model(seqs)\n",
    "                loss = F.cross_entropy(preds, labels)\n",
    "                avg_val_loss += loss.detach().item() / len(val_loader)\n",
    "                total_val_correct += get_num_correct(preds, labels) \n",
    "\n",
    "        early_stopping(avg_val_loss, model, f'checkpoints/{EYE}/P2B2_SeparableConv-fold_{fold}.pt')\n",
    "        # early_stopping(avg_val_loss, model, f'checkpoints/{EYE}/P2B2-fold_{fold}.pt')\n",
    "        # early_stopping(avg_val_loss, model, f'checkpoints/{EYE}/P3B3-fold_{fold}.pt')\n",
    "        # early_stopping(avg_val_loss, model, f'checkpoints/{EYE}/P3B3_SeparableConv-fold_{fold}.pt')\n",
    "\n",
    "        print(f'epoch: {epoch} - train_acc: {total_train_correct/len(train_sampler)} - train_loss: {avg_train_loss} - val_acc: {total_val_correct/len(val_sampler)} - val_loss: {avg_val_loss}')\n",
    "\n",
    "        if early_stopping.early_stop:\n",
    "            print('=> Early stopped !!')\n",
    "            break\n",
    "    \n",
    "\n",
    "    ## test set\n",
    "    with torch.no_grad():\n",
    "        data_test_loader = torch.utils.data.DataLoader(data_test, batch_size=13, shuffle=True, pin_memory=True)\n",
    "        # model.load_state_dict(torch.load(f'checkpoints/{EYE}/P2B2-fold_{fold}.pt'))\n",
    "        model.load_state_dict(torch.load(f'checkpoints/{EYE}/P2B2_SeparableConv-fold_{fold}.pt'))\n",
    "        # model.load_state_dict(torch.load(f'checkpoints/{EYE}/P3B3-fold_{fold}.pt'))\n",
    "        # model.load_state_dict(torch.load(f'checkpoints/{EYE}/P3B3_SeparableConv-fold_{fold}.pt'))\n",
    "        model.eval()\n",
    "        \n",
    "        total_TP = 0\n",
    "        total_TN = 0\n",
    "        total_FP = 0\n",
    "        total_FN = 0\n",
    "\n",
    "        for seqs, labels in data_test_loader:\n",
    "            seqs = seqs.permute((0,4,3,1,2)).to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            preds = model(seqs)\n",
    "            total_TP += count_TP(preds, labels)\n",
    "            total_TN += count_TN(preds, labels)\n",
    "            total_FP += count_FP(preds, labels)\n",
    "            total_FN += count_FN(preds, labels)\n",
    "    \n",
    "    accuracy  = (total_TP + total_TN)  / len(data_test)\n",
    "    precision = total_TP / (total_TP + total_FP)\n",
    "    recall    = total_TP / (total_TP + total_FN)\n",
    "    F1        = 2 / ((1 / precision) + (1 / recall))\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f'Precision: {precision}')\n",
    "    print(f'Recall: {recall}')\n",
    "    print(f'F1: {F1}')\n",
    "    \n",
    "    result = {'acc':accuracy, 'pre':precision, 'rec': recall, 'f1': F1}\n",
    "    test_results.append(result)\n",
    "\n",
    "import json\n",
    "\n",
    "with open(f'checkpoints/{EYE}/P2B2_SeparableConv-results.txt','w') as fp:\n",
    "# with open(f'checkpoints/{EYE}/P2B2-results.txt','w') as fp:\n",
    "# with open(f'checkpoints/{EYE}/P3B3-results.txt','w') as fp:\n",
    "# with open(f'checkpoints/{EYE}/P3B3_SeparableConv-results.txt','w') as fp:\n",
    "    fp.write(json.dumps(test_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'checkpoints/{EYE}/P3B3_SeparableConv-results.txt','w') as fp:\n",
    "    fp.write(json.dumps(test_results))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training - gradient accumulation - EarlyStopping + k-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['datasets/train/training/blink/8', 'datasets/train/training/blink/141', 'datasets/train/training/blink/151', 'datasets/train/training/blink/153', 'datasets/train/training/blink/155', 'datasets/train/training/blink/158', 'datasets/train/training/blink/160', 'datasets/train/training/blink/179', 'datasets/train/training/blink/181', 'datasets/train/training/blink/198', 'datasets/train/training/unblink/55', 'datasets/train/training/unblink/61', 'datasets/train/training/unblink/76', 'datasets/train/training/unblink/78', 'datasets/train/training/unblink/80', 'datasets/train/training/unblink/82', 'datasets/train/training/unblink/93', 'datasets/train/training/unblink/95', 'datasets/train/training/unblink/144', 'datasets/train/training/unblink/146']\n",
      "['datasets/test/test/blink/4', 'datasets/test/test/blink/69', 'datasets/test/test/blink/74', 'datasets/test/test/blink/92', 'datasets/test/test/blink/94']\n",
      "\n",
      "[Fold 0]: \n",
      "len(train_sampler)=399\n",
      "len(val_sampler)=29\n",
      "[ 11  48  60  62 114 119 124 148 149 158 202 220 234 263 274 285 301 306\n",
      " 311 314 323 324 333 338 346 348 389 413 421]\n",
      "Validation loss decreased (inf --> 0.684088).  Saving model ...\n",
      "epoch: 0 - train_acc: 0.42105263157894735 - train_loss: 0.21249051876200023 - val_acc: 0.5517241379310345 - val_loss: 0.6840879240515332\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [3], line 134\u001b[0m\n\u001b[1;32m    131\u001b[0m         optimizer\u001b[39m.\u001b[39mzero_grad()  \n\u001b[1;32m    132\u001b[0m         optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m--> 134\u001b[0m     avg_train_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39;49mdetach()\u001b[39m.\u001b[39;49mitem() \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(train_loader)\n\u001b[1;32m    135\u001b[0m     total_train_correct \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m get_num_correct(preds_logits, labels)\n\u001b[1;32m    137\u001b[0m \u001b[39m## val set\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "                            Default: False\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.fname = 'checkpoints/P2B2_SeparableConv.pt'\n",
    "\n",
    "    def __call__(self, val_loss, model, fname):\n",
    "        \n",
    "        self.fname = fname\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.fname)\n",
    "        self.val_loss_min = val_loss\n",
    "\n",
    "\n",
    "\n",
    "def get_num_correct(preds_logits, labels):\n",
    "    return (preds_logits.argmax(dim=1) == labels).sum().item()\n",
    "\n",
    "def count_TP(preds_logits, labels):\n",
    "    preds = preds_logits.argmax(dim=1)\n",
    "    tmp = torch.add(preds, labels)\n",
    "    return (tmp == 2).sum().item()\n",
    "\n",
    "def count_TN(preds_logits, labels):\n",
    "    preds = preds_logits.argmax(dim=1)\n",
    "    tmp = torch.add(preds, labels)\n",
    "    return (tmp == 0).sum().item()\n",
    "\n",
    "def count_FP(preds_logits, labels):\n",
    "    preds = preds_logits.argmax(dim=1)\n",
    "    tmp = torch.subtract(preds, labels)\n",
    "    return (tmp == 1).sum().item()\n",
    "\n",
    "def count_FN(preds_logits, labels):\n",
    "    preds = preds_logits.argmax(dim=1)\n",
    "    tmp = torch.subtract(preds, labels)\n",
    "    return (tmp == -1).sum().item()\n",
    "\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "data = EyeBlinkDataset(train=True)\n",
    "data_test = EyeBlinkDataset(train=False)\n",
    "\n",
    "test_results = []\n",
    "\n",
    "kfold = KFold(n_splits=15, shuffle=True)\n",
    "for fold, (train_ids, val_ids) in enumerate(kfold.split(data)):\n",
    "    print(f'\\n[Fold {fold}]: ')\n",
    "    train_sampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
    "    val_sampler = torch.utils.data.SubsetRandomSampler(val_ids)\n",
    "    train_loader = torch.utils.data.DataLoader(data, batch_size=13, sampler=train_sampler, pin_memory=True)\n",
    "    val_loader = torch.utils.data.DataLoader(data, batch_size=13, sampler=val_sampler, pin_memory=True)\n",
    "    print(f'len(train_sampler)={len(train_sampler)}')\n",
    "    print(f'len(val_sampler)={len(val_sampler)}')\n",
    "    print(val_ids)\n",
    "\n",
    "    # model = P2B2().double()\n",
    "    # model = P2B2_SeparableConv().double()\n",
    "    # model = P3B3().double()\n",
    "    model = P3B3_SeparableConv().double()\n",
    "    model.to(device)\n",
    "\n",
    "    # loss_func = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "    min_loss = float('inf')\n",
    "    num_epochs = 500\n",
    "\n",
    "    early_stopping = EarlyStopping(patience=50, verbose=True)\n",
    "\n",
    "    # batch accumulation parameter\n",
    "    accum_iter = 4  \n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        ## train set\n",
    "        avg_train_loss = 0\n",
    "        total_train_correct = 0\n",
    "        model.train()\n",
    "\n",
    "        for batch_idx, (seqs, labels) in enumerate(train_loader):\n",
    "            # print(batch_idx)\n",
    "            seqs = seqs.permute((0,4,3,1,2)).to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            preds_logits = model(seqs)\n",
    "            \n",
    "            # print('---',seqs.shape)\n",
    "            # print(preds_logits.shape)\n",
    "            # print(labels.shape,'---')\n",
    "        \n",
    "            loss = F.cross_entropy(preds_logits, labels) / accum_iter\n",
    "            loss.backward()\n",
    "\n",
    "            if ((batch_idx + 1) % accum_iter == 0) or (batch_idx + 1 == len(train_loader)):\n",
    "                optimizer.zero_grad()  \n",
    "                optimizer.step()\n",
    "\n",
    "            avg_train_loss += loss.detach().item() / len(train_loader)\n",
    "            total_train_correct += get_num_correct(preds_logits, labels)\n",
    "\n",
    "        ## val set\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            avg_val_loss = 0\n",
    "            total_val_correct = 0\n",
    "\n",
    "            for seqs, labels in val_loader:\n",
    "                seqs = seqs.permute((0,4,3,1,2)).to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                preds = model(seqs)\n",
    "                loss = F.cross_entropy(preds, labels)\n",
    "                avg_val_loss += loss.detach().item() / len(val_loader)\n",
    "                total_val_correct += get_num_correct(preds, labels) \n",
    "\n",
    "        # early_stopping(avg_val_loss, model, f'checkpoints/P2B2_SeparableConv-fold_{fold}.pt')\n",
    "        # early_stopping(avg_val_loss, model, f'checkpoints/P2B2-fold_{fold}.pt')\n",
    "        # early_stopping(avg_val_loss, model, f'checkpoints/P3B3-fold_{fold}.pt')\n",
    "        early_stopping(avg_val_loss, model, f'checkpoints/P3B3_SeparableConv-fold_{fold}.pt')\n",
    "\n",
    "        print(f'epoch: {epoch} - train_acc: {total_train_correct/len(train_sampler)} - train_loss: {avg_train_loss} - val_acc: {total_val_correct/len(val_sampler)} - val_loss: {avg_val_loss}')\n",
    "\n",
    "        if early_stopping.early_stop:\n",
    "            print('=> Early stopped !!')\n",
    "            break\n",
    "    \n",
    "\n",
    "    ## test set\n",
    "    with torch.no_grad():\n",
    "        data_test_loader = torch.utils.data.DataLoader(data_test, batch_size=13, shuffle=True, pin_memory=True)\n",
    "        # model.load_state_dict(torch.load(f'checkpoints/P2B2-fold_{fold}.pt'))\n",
    "        # model.load_state_dict(torch.load(f'checkpoints/P2B2_SeparableConv-fold_{fold}.pt'))\n",
    "        # model.load_state_dict(torch.load(f'checkpoints/P3B3-fold_{fold}.pt'))\n",
    "        model.load_state_dict(torch.load(f'checkpoints/P3B3_SeparableConv-fold_{fold}.pt'))\n",
    "        model.eval()\n",
    "        \n",
    "        total_TP = 0\n",
    "        total_TN = 0\n",
    "        total_FP = 0\n",
    "        total_FN = 0\n",
    "\n",
    "        for seqs, labels in data_test_loader:\n",
    "            seqs = seqs.permute((0,4,3,1,2)).to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            preds = model(seqs)\n",
    "            total_TP += count_TP(preds, labels)\n",
    "            total_TN += count_TN(preds, labels)\n",
    "            total_FP += count_FP(preds, labels)\n",
    "            total_FN += count_FN(preds, labels)\n",
    "    \n",
    "    accuracy  = (total_TP + total_TN)  / len(data_test)\n",
    "    precision = total_TP / (total_TP + total_FP)\n",
    "    recall    = total_TP / (total_TP + total_FN)\n",
    "    F1        = 2 / ((1 / precision) + (1 / recall))\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f'Precision: {precision}')\n",
    "    print(f'Recall: {recall}')\n",
    "    print(f'F1: {F1}')\n",
    "    \n",
    "    result = {'acc':accuracy, 'pre':precision, 'rec': recall, 'f1': F1}\n",
    "    test_results.append(result)\n",
    "\n",
    "import json\n",
    "\n",
    "# with open('checkpoints/P2B2_SeparableConv-results.txt','w') as fp:\n",
    "# with open('checkpoints/P2B2-results.txt','w') as fp:\n",
    "# with open('checkpoints/P3B3-results.txt','w') as fp:\n",
    "with open('checkpoints/P3B3_SeparableConv-results.txt','w') as fp:\n",
    "    fp.write(json.dumps(test_results))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training - interpolations - earlystopping + k-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['datasets/train/training/blink/8', 'datasets/train/training/blink/141', 'datasets/train/training/blink/142', 'datasets/train/training/blink/151', 'datasets/train/training/blink/152', 'datasets/train/training/blink/153', 'datasets/train/training/blink/154', 'datasets/train/training/blink/155', 'datasets/train/training/blink/158', 'datasets/train/training/blink/159', 'datasets/train/training/blink/160', 'datasets/train/training/blink/179', 'datasets/train/training/blink/181', 'datasets/train/training/blink/182', 'datasets/train/training/blink/198', 'datasets/train/training/unblink/55', 'datasets/train/training/unblink/61', 'datasets/train/training/unblink/76', 'datasets/train/training/unblink/78', 'datasets/train/training/unblink/79', 'datasets/train/training/unblink/80', 'datasets/train/training/unblink/82', 'datasets/train/training/unblink/83', 'datasets/train/training/unblink/93', 'datasets/train/training/unblink/94', 'datasets/train/training/unblink/95', 'datasets/train/training/unblink/144', 'datasets/train/training/unblink/146']\n",
      "['datasets/test/test/blink/4', 'datasets/test/test/blink/69', 'datasets/test/test/blink/74', 'datasets/test/test/blink/92', 'datasets/test/test/blink/94']\n",
      "['datasets/test/test/blink/4', 'datasets/test/test/blink/69', 'datasets/test/test/blink/74', 'datasets/test/test/blink/92', 'datasets/test/test/blink/94']\n",
      "['datasets/test/test/blink/4', 'datasets/test/test/blink/69', 'datasets/test/test/blink/74', 'datasets/test/test/blink/92', 'datasets/test/test/blink/94']\n",
      "\n",
      "[Fold 0]: \n",
      "len(train_sampler)=392\n",
      "len(val_sampler)=28\n",
      "[  9  30  55  56  70  72  73  90  94 132 137 145 175 192 196 228 231 239\n",
      " 247 297 326 334 347 353 368 369 412 416]\n",
      "Validation loss decreased (inf --> 0.689385).  Saving model ...\n",
      "epoch: 0 - train_acc: 0.5918367346938775 - train_loss: 0.675565800223543 - val_acc: 0.6071428571428571 - val_loss: 0.6893846251956169\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 1 - train_acc: 0.6377551020408163 - train_loss: 0.6339654872365617 - val_acc: 0.5357142857142857 - val_loss: 0.7308568295317146\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 2 - train_acc: 0.6862244897959183 - train_loss: 0.5902036858028195 - val_acc: 0.75 - val_loss: 0.7952852062696038\n",
      "EarlyStopping counter: 3 out of 50\n",
      "epoch: 3 - train_acc: 0.7040816326530612 - train_loss: 0.5646414664572229 - val_acc: 0.5 - val_loss: 0.8026080000139502\n",
      "EarlyStopping counter: 4 out of 50\n",
      "epoch: 4 - train_acc: 0.7576530612244898 - train_loss: 0.5155574395609736 - val_acc: 0.6785714285714286 - val_loss: 1.0702398030762292\n",
      "EarlyStopping counter: 5 out of 50\n",
      "epoch: 5 - train_acc: 0.7448979591836735 - train_loss: 0.48674400041067356 - val_acc: 0.7857142857142857 - val_loss: 0.7391915113840053\n",
      "EarlyStopping counter: 6 out of 50\n",
      "epoch: 6 - train_acc: 0.7678571428571429 - train_loss: 0.49979168818157227 - val_acc: 0.8214285714285714 - val_loss: 0.8100317778641544\n",
      "EarlyStopping counter: 7 out of 50\n",
      "epoch: 7 - train_acc: 0.8061224489795918 - train_loss: 0.4511331666251845 - val_acc: 0.6785714285714286 - val_loss: 0.7063614648603871\n",
      "EarlyStopping counter: 8 out of 50\n",
      "epoch: 8 - train_acc: 0.7959183673469388 - train_loss: 0.4244982847095245 - val_acc: 0.7142857142857143 - val_loss: 0.8567565865439889\n",
      "Validation loss decreased (0.689385 --> 0.635087).  Saving model ...\n",
      "epoch: 9 - train_acc: 0.8341836734693877 - train_loss: 0.39540471349684303 - val_acc: 0.8571428571428571 - val_loss: 0.6350868452970319\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 10 - train_acc: 0.8239795918367347 - train_loss: 0.3940425712015145 - val_acc: 0.6071428571428571 - val_loss: 0.8675015574672109\n",
      "Validation loss decreased (0.635087 --> 0.417570).  Saving model ...\n",
      "epoch: 11 - train_acc: 0.8214285714285714 - train_loss: 0.39680101007256086 - val_acc: 0.9642857142857143 - val_loss: 0.4175695455461865\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 12 - train_acc: 0.8545918367346939 - train_loss: 0.35058733969116407 - val_acc: 0.75 - val_loss: 0.5518192592640849\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 13 - train_acc: 0.8316326530612245 - train_loss: 0.3726835767226926 - val_acc: 0.7857142857142857 - val_loss: 0.7870604171478859\n",
      "EarlyStopping counter: 3 out of 50\n",
      "epoch: 14 - train_acc: 0.8290816326530612 - train_loss: 0.3549757326285584 - val_acc: 0.6071428571428571 - val_loss: 0.821692878280512\n",
      "EarlyStopping counter: 4 out of 50\n",
      "epoch: 15 - train_acc: 0.8418367346938775 - train_loss: 0.355383108592936 - val_acc: 0.8214285714285714 - val_loss: 0.46902087032169665\n",
      "EarlyStopping counter: 5 out of 50\n",
      "epoch: 16 - train_acc: 0.8469387755102041 - train_loss: 0.3468253624486911 - val_acc: 0.6428571428571429 - val_loss: 0.8292115429016388\n",
      "EarlyStopping counter: 6 out of 50\n",
      "epoch: 17 - train_acc: 0.8877551020408163 - train_loss: 0.28456990073514954 - val_acc: 0.7857142857142857 - val_loss: 0.5527256285990995\n",
      "EarlyStopping counter: 7 out of 50\n",
      "epoch: 18 - train_acc: 0.9005102040816326 - train_loss: 0.24554742468951837 - val_acc: 0.8571428571428571 - val_loss: 0.5306338431647659\n",
      "EarlyStopping counter: 8 out of 50\n",
      "epoch: 19 - train_acc: 0.9107142857142857 - train_loss: 0.2632426120356181 - val_acc: 0.8571428571428571 - val_loss: 0.4943785986583324\n",
      "EarlyStopping counter: 9 out of 50\n",
      "epoch: 20 - train_acc: 0.8852040816326531 - train_loss: 0.28244802299426497 - val_acc: 0.7857142857142857 - val_loss: 0.7877170917655829\n",
      "EarlyStopping counter: 10 out of 50\n",
      "epoch: 21 - train_acc: 0.9056122448979592 - train_loss: 0.2537496399579259 - val_acc: 0.7142857142857143 - val_loss: 0.6361230420248184\n",
      "EarlyStopping counter: 11 out of 50\n",
      "epoch: 22 - train_acc: 0.8954081632653061 - train_loss: 0.23021217368925442 - val_acc: 0.75 - val_loss: 0.6647738432798702\n",
      "EarlyStopping counter: 12 out of 50\n",
      "epoch: 23 - train_acc: 0.9005102040816326 - train_loss: 0.23546516739031909 - val_acc: 0.8928571428571429 - val_loss: 0.5790298036396504\n",
      "Validation loss decreased (0.417570 --> 0.369545).  Saving model ...\n",
      "epoch: 24 - train_acc: 0.9362244897959183 - train_loss: 0.19958749358930816 - val_acc: 0.8571428571428571 - val_loss: 0.36954503560227214\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 25 - train_acc: 0.9336734693877551 - train_loss: 0.18475734938168742 - val_acc: 0.8214285714285714 - val_loss: 1.4957811135151913\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 26 - train_acc: 0.9005102040816326 - train_loss: 0.22809336166809938 - val_acc: 0.8571428571428571 - val_loss: 0.7899499361841793\n",
      "EarlyStopping counter: 3 out of 50\n",
      "epoch: 27 - train_acc: 0.9158163265306123 - train_loss: 0.25276488347184234 - val_acc: 0.75 - val_loss: 0.4821290838159008\n",
      "EarlyStopping counter: 4 out of 50\n",
      "epoch: 28 - train_acc: 0.9336734693877551 - train_loss: 0.1953053551950491 - val_acc: 0.9285714285714286 - val_loss: 0.5941838380859182\n",
      "EarlyStopping counter: 5 out of 50\n",
      "epoch: 29 - train_acc: 0.9387755102040817 - train_loss: 0.18674718042474414 - val_acc: 0.8214285714285714 - val_loss: 0.5413141973255193\n",
      "EarlyStopping counter: 6 out of 50\n",
      "epoch: 30 - train_acc: 0.9336734693877551 - train_loss: 0.17114850974620194 - val_acc: 0.8928571428571429 - val_loss: 0.7799066311018686\n",
      "EarlyStopping counter: 7 out of 50\n",
      "epoch: 31 - train_acc: 0.9617346938775511 - train_loss: 0.12381296621406934 - val_acc: 0.8214285714285714 - val_loss: 0.40805018128921644\n",
      "EarlyStopping counter: 8 out of 50\n",
      "epoch: 32 - train_acc: 0.9438775510204082 - train_loss: 0.17895416548629212 - val_acc: 0.8571428571428571 - val_loss: 0.7348836966790616\n",
      "EarlyStopping counter: 9 out of 50\n",
      "epoch: 33 - train_acc: 0.8852040816326531 - train_loss: 0.24280512167127224 - val_acc: 0.8571428571428571 - val_loss: 0.5015652160814775\n",
      "EarlyStopping counter: 10 out of 50\n",
      "epoch: 34 - train_acc: 0.9209183673469388 - train_loss: 0.1945782200554544 - val_acc: 0.8928571428571429 - val_loss: 0.6317935444784402\n",
      "EarlyStopping counter: 11 out of 50\n",
      "epoch: 35 - train_acc: 0.9336734693877551 - train_loss: 0.1706906298517078 - val_acc: 0.8928571428571429 - val_loss: 0.48833658619062215\n",
      "EarlyStopping counter: 12 out of 50\n",
      "epoch: 36 - train_acc: 0.9362244897959183 - train_loss: 0.14103535308446893 - val_acc: 0.8571428571428571 - val_loss: 0.6394084295503233\n",
      "EarlyStopping counter: 13 out of 50\n",
      "epoch: 37 - train_acc: 0.9693877551020408 - train_loss: 0.12883641021429737 - val_acc: 0.8214285714285714 - val_loss: 0.84010283751543\n",
      "EarlyStopping counter: 14 out of 50\n",
      "epoch: 38 - train_acc: 0.9489795918367347 - train_loss: 0.14825142528344656 - val_acc: 0.9285714285714286 - val_loss: 0.6031094004818389\n",
      "EarlyStopping counter: 15 out of 50\n",
      "epoch: 39 - train_acc: 0.9362244897959183 - train_loss: 0.1541326992199598 - val_acc: 0.8571428571428571 - val_loss: 1.0315319249420036\n",
      "EarlyStopping counter: 16 out of 50\n",
      "epoch: 40 - train_acc: 0.9438775510204082 - train_loss: 0.18464863844538987 - val_acc: 0.7857142857142857 - val_loss: 0.9196775659516689\n",
      "EarlyStopping counter: 17 out of 50\n",
      "epoch: 41 - train_acc: 0.951530612244898 - train_loss: 0.14536097137728107 - val_acc: 0.8571428571428571 - val_loss: 0.42368483714904265\n",
      "EarlyStopping counter: 18 out of 50\n",
      "epoch: 42 - train_acc: 0.9438775510204082 - train_loss: 0.12864550415844317 - val_acc: 0.8214285714285714 - val_loss: 0.5515123504945975\n",
      "EarlyStopping counter: 19 out of 50\n",
      "epoch: 43 - train_acc: 0.9617346938775511 - train_loss: 0.09158641858012716 - val_acc: 0.8928571428571429 - val_loss: 0.5417098846907639\n",
      "EarlyStopping counter: 20 out of 50\n",
      "epoch: 44 - train_acc: 0.9617346938775511 - train_loss: 0.0947196552571911 - val_acc: 0.9285714285714286 - val_loss: 0.6948738571833208\n",
      "EarlyStopping counter: 21 out of 50\n",
      "epoch: 45 - train_acc: 0.951530612244898 - train_loss: 0.10743792364474106 - val_acc: 0.8214285714285714 - val_loss: 1.234756257150076\n",
      "EarlyStopping counter: 22 out of 50\n",
      "epoch: 46 - train_acc: 0.9642857142857143 - train_loss: 0.09050080386897881 - val_acc: 0.7857142857142857 - val_loss: 0.4741697462566826\n",
      "EarlyStopping counter: 23 out of 50\n",
      "epoch: 47 - train_acc: 0.9617346938775511 - train_loss: 0.11570988997613248 - val_acc: 0.7857142857142857 - val_loss: 0.43462874800954454\n",
      "EarlyStopping counter: 24 out of 50\n",
      "epoch: 48 - train_acc: 0.9642857142857143 - train_loss: 0.09904254575546614 - val_acc: 0.8928571428571429 - val_loss: 1.3206718587739417\n",
      "EarlyStopping counter: 25 out of 50\n",
      "epoch: 49 - train_acc: 0.9719387755102041 - train_loss: 0.07342552919700931 - val_acc: 0.8571428571428571 - val_loss: 0.5794777362039993\n",
      "EarlyStopping counter: 26 out of 50\n",
      "epoch: 50 - train_acc: 0.9489795918367347 - train_loss: 0.11953924282399675 - val_acc: 0.75 - val_loss: 0.48863657435866326\n",
      "EarlyStopping counter: 27 out of 50\n",
      "epoch: 51 - train_acc: 0.951530612244898 - train_loss: 0.11264747888758635 - val_acc: 0.7857142857142857 - val_loss: 0.6058922362954993\n",
      "EarlyStopping counter: 28 out of 50\n",
      "epoch: 52 - train_acc: 0.9795918367346939 - train_loss: 0.07284433280975683 - val_acc: 0.8571428571428571 - val_loss: 0.5856076883961363\n",
      "EarlyStopping counter: 29 out of 50\n",
      "epoch: 53 - train_acc: 0.9642857142857143 - train_loss: 0.07969694961893674 - val_acc: 0.8214285714285714 - val_loss: 0.5921350080965692\n",
      "EarlyStopping counter: 30 out of 50\n",
      "epoch: 54 - train_acc: 0.9770408163265306 - train_loss: 0.09884485087552707 - val_acc: 0.75 - val_loss: 0.7330887969919685\n",
      "EarlyStopping counter: 31 out of 50\n",
      "epoch: 55 - train_acc: 0.9540816326530612 - train_loss: 0.12079443300056762 - val_acc: 0.8928571428571429 - val_loss: 0.9142538254164044\n",
      "Validation loss decreased (0.369545 --> 0.284079).  Saving model ...\n",
      "epoch: 56 - train_acc: 0.9617346938775511 - train_loss: 0.10396026933996473 - val_acc: 0.9285714285714286 - val_loss: 0.284079130800565\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 57 - train_acc: 0.9821428571428571 - train_loss: 0.06562476595131306 - val_acc: 0.8571428571428571 - val_loss: 0.5274631672768956\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 58 - train_acc: 0.9770408163265306 - train_loss: 0.053733611825750725 - val_acc: 0.9285714285714286 - val_loss: 0.32244858379161745\n",
      "EarlyStopping counter: 3 out of 50\n",
      "epoch: 59 - train_acc: 0.9923469387755102 - train_loss: 0.04268486640650785 - val_acc: 0.9285714285714286 - val_loss: 0.4425215539119869\n",
      "EarlyStopping counter: 4 out of 50\n",
      "epoch: 60 - train_acc: 0.9872448979591837 - train_loss: 0.05331480089451517 - val_acc: 0.8571428571428571 - val_loss: 0.7886668558620518\n",
      "Validation loss decreased (0.284079 --> 0.126507).  Saving model ...\n",
      "epoch: 61 - train_acc: 0.9744897959183674 - train_loss: 0.06845376575795574 - val_acc: 0.9642857142857143 - val_loss: 0.12650706737547848\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 62 - train_acc: 0.9770408163265306 - train_loss: 0.07212173402469776 - val_acc: 0.9285714285714286 - val_loss: 0.4395840506084583\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 63 - train_acc: 0.9642857142857143 - train_loss: 0.09110033419807828 - val_acc: 0.8928571428571429 - val_loss: 0.6174182475399773\n",
      "EarlyStopping counter: 3 out of 50\n",
      "epoch: 64 - train_acc: 0.951530612244898 - train_loss: 0.12660223587284306 - val_acc: 0.8571428571428571 - val_loss: 0.7094239279984341\n",
      "EarlyStopping counter: 4 out of 50\n",
      "epoch: 65 - train_acc: 0.9362244897959183 - train_loss: 0.1584396357586932 - val_acc: 0.7857142857142857 - val_loss: 1.212711678393545\n",
      "EarlyStopping counter: 5 out of 50\n",
      "epoch: 66 - train_acc: 0.9668367346938775 - train_loss: 0.08848073804126207 - val_acc: 0.8928571428571429 - val_loss: 0.4697168301557715\n",
      "EarlyStopping counter: 6 out of 50\n",
      "epoch: 67 - train_acc: 0.9540816326530612 - train_loss: 0.10838200654160823 - val_acc: 0.8571428571428571 - val_loss: 0.6325737824092339\n",
      "EarlyStopping counter: 7 out of 50\n",
      "epoch: 68 - train_acc: 0.9540816326530612 - train_loss: 0.1222516948351491 - val_acc: 0.8571428571428571 - val_loss: 3.1302572418634718\n",
      "EarlyStopping counter: 8 out of 50\n",
      "epoch: 69 - train_acc: 0.9770408163265306 - train_loss: 0.08323273093662995 - val_acc: 0.8571428571428571 - val_loss: 0.5523750388041313\n",
      "EarlyStopping counter: 9 out of 50\n",
      "epoch: 70 - train_acc: 0.9693877551020408 - train_loss: 0.07226671753193983 - val_acc: 0.8928571428571429 - val_loss: 0.6279489224507997\n",
      "EarlyStopping counter: 10 out of 50\n",
      "epoch: 71 - train_acc: 0.9821428571428571 - train_loss: 0.0451069457103073 - val_acc: 0.8928571428571429 - val_loss: 0.4676133745825338\n",
      "EarlyStopping counter: 11 out of 50\n",
      "epoch: 72 - train_acc: 0.9897959183673469 - train_loss: 0.04658856795531191 - val_acc: 0.8928571428571429 - val_loss: 0.9826218500529493\n",
      "EarlyStopping counter: 12 out of 50\n",
      "epoch: 73 - train_acc: 0.9591836734693877 - train_loss: 0.10959549865732365 - val_acc: 0.8214285714285714 - val_loss: 0.7998259338392176\n",
      "EarlyStopping counter: 13 out of 50\n",
      "epoch: 74 - train_acc: 0.9617346938775511 - train_loss: 0.09316525674571836 - val_acc: 0.8214285714285714 - val_loss: 1.000848845035811\n",
      "EarlyStopping counter: 14 out of 50\n",
      "epoch: 75 - train_acc: 0.9719387755102041 - train_loss: 0.07233904073883819 - val_acc: 0.8571428571428571 - val_loss: 1.177665479443669\n",
      "EarlyStopping counter: 15 out of 50\n",
      "epoch: 76 - train_acc: 0.9744897959183674 - train_loss: 0.07310033315751456 - val_acc: 0.9285714285714286 - val_loss: 0.5647959591621045\n",
      "EarlyStopping counter: 16 out of 50\n",
      "epoch: 77 - train_acc: 0.9693877551020408 - train_loss: 0.07011316685298895 - val_acc: 0.9285714285714286 - val_loss: 0.4577380752914064\n",
      "EarlyStopping counter: 17 out of 50\n",
      "epoch: 78 - train_acc: 0.9872448979591837 - train_loss: 0.04557068802687941 - val_acc: 0.8571428571428571 - val_loss: 0.6440621936451202\n",
      "EarlyStopping counter: 18 out of 50\n",
      "epoch: 79 - train_acc: 0.9948979591836735 - train_loss: 0.03426120480859981 - val_acc: 0.8928571428571429 - val_loss: 0.9715058748089217\n",
      "EarlyStopping counter: 19 out of 50\n",
      "epoch: 80 - train_acc: 0.9948979591836735 - train_loss: 0.02900739199523554 - val_acc: 0.8928571428571429 - val_loss: 0.7254852510774763\n",
      "EarlyStopping counter: 20 out of 50\n",
      "epoch: 81 - train_acc: 0.9923469387755102 - train_loss: 0.042290893681115764 - val_acc: 0.8928571428571429 - val_loss: 0.5231917946613186\n",
      "EarlyStopping counter: 21 out of 50\n",
      "epoch: 82 - train_acc: 0.9948979591836735 - train_loss: 0.02949149430061506 - val_acc: 0.8928571428571429 - val_loss: 0.4334458296316023\n",
      "EarlyStopping counter: 22 out of 50\n",
      "epoch: 83 - train_acc: 0.9948979591836735 - train_loss: 0.024978814508091883 - val_acc: 0.9285714285714286 - val_loss: 0.5598372388685691\n",
      "EarlyStopping counter: 23 out of 50\n",
      "epoch: 84 - train_acc: 0.9744897959183674 - train_loss: 0.07624108953198322 - val_acc: 0.8571428571428571 - val_loss: 0.937549267551103\n",
      "EarlyStopping counter: 24 out of 50\n",
      "epoch: 85 - train_acc: 0.9821428571428571 - train_loss: 0.07042001881789942 - val_acc: 0.8928571428571429 - val_loss: 0.606871633964694\n",
      "EarlyStopping counter: 25 out of 50\n",
      "epoch: 86 - train_acc: 0.9897959183673469 - train_loss: 0.0472726847073597 - val_acc: 0.7857142857142857 - val_loss: 1.1069878343468846\n",
      "EarlyStopping counter: 26 out of 50\n",
      "epoch: 87 - train_acc: 0.9897959183673469 - train_loss: 0.04688695962434298 - val_acc: 0.8571428571428571 - val_loss: 0.5563453071844977\n",
      "EarlyStopping counter: 27 out of 50\n",
      "epoch: 88 - train_acc: 0.9872448979591837 - train_loss: 0.07566546530257737 - val_acc: 0.8571428571428571 - val_loss: 0.3991677965035587\n",
      "EarlyStopping counter: 28 out of 50\n",
      "epoch: 89 - train_acc: 0.9642857142857143 - train_loss: 0.09733860613973154 - val_acc: 0.8214285714285714 - val_loss: 0.8412984484212069\n",
      "EarlyStopping counter: 29 out of 50\n",
      "epoch: 90 - train_acc: 0.9668367346938775 - train_loss: 0.08854632392853574 - val_acc: 0.8928571428571429 - val_loss: 0.8895705541634413\n",
      "EarlyStopping counter: 30 out of 50\n",
      "epoch: 91 - train_acc: 0.9668367346938775 - train_loss: 0.08886712611350625 - val_acc: 0.8928571428571429 - val_loss: 0.5357060951410982\n",
      "EarlyStopping counter: 31 out of 50\n",
      "epoch: 92 - train_acc: 0.9821428571428571 - train_loss: 0.07841419909024308 - val_acc: 0.9285714285714286 - val_loss: 0.6966332032990719\n",
      "EarlyStopping counter: 32 out of 50\n",
      "epoch: 93 - train_acc: 0.9846938775510204 - train_loss: 0.03713576432500127 - val_acc: 0.8214285714285714 - val_loss: 0.8255872822252475\n",
      "EarlyStopping counter: 33 out of 50\n",
      "epoch: 94 - train_acc: 0.9846938775510204 - train_loss: 0.04740523967400512 - val_acc: 0.9285714285714286 - val_loss: 0.3929404967303188\n",
      "EarlyStopping counter: 34 out of 50\n",
      "epoch: 95 - train_acc: 1.0 - train_loss: 0.017997304542783726 - val_acc: 0.9285714285714286 - val_loss: 0.46004333224185806\n",
      "EarlyStopping counter: 35 out of 50\n",
      "epoch: 96 - train_acc: 0.9948979591836735 - train_loss: 0.02227834599239586 - val_acc: 0.8571428571428571 - val_loss: 0.688385229771325\n",
      "EarlyStopping counter: 36 out of 50\n",
      "epoch: 97 - train_acc: 1.0 - train_loss: 0.016936144857370713 - val_acc: 0.9285714285714286 - val_loss: 0.6327594169280661\n",
      "EarlyStopping counter: 37 out of 50\n",
      "epoch: 98 - train_acc: 0.9923469387755102 - train_loss: 0.03242017198755705 - val_acc: 0.8571428571428571 - val_loss: 0.49858513039797525\n",
      "EarlyStopping counter: 38 out of 50\n",
      "epoch: 99 - train_acc: 0.9846938775510204 - train_loss: 0.04676995562799737 - val_acc: 0.8214285714285714 - val_loss: 0.5841163754003917\n",
      "EarlyStopping counter: 39 out of 50\n",
      "epoch: 100 - train_acc: 0.9795918367346939 - train_loss: 0.04886148213671611 - val_acc: 0.8928571428571429 - val_loss: 0.738628332046234\n",
      "EarlyStopping counter: 40 out of 50\n",
      "epoch: 101 - train_acc: 0.9974489795918368 - train_loss: 0.019189141802964473 - val_acc: 0.8928571428571429 - val_loss: 0.31962566689110283\n",
      "EarlyStopping counter: 41 out of 50\n",
      "epoch: 102 - train_acc: 0.9948979591836735 - train_loss: 0.023364025734193453 - val_acc: 0.8928571428571429 - val_loss: 0.6836150984012438\n",
      "EarlyStopping counter: 42 out of 50\n",
      "epoch: 103 - train_acc: 0.951530612244898 - train_loss: 0.11394607751023682 - val_acc: 0.8571428571428571 - val_loss: 1.9491832889909093\n",
      "EarlyStopping counter: 43 out of 50\n",
      "epoch: 104 - train_acc: 0.9668367346938775 - train_loss: 0.11915158248552826 - val_acc: 0.75 - val_loss: 1.2616007103260332\n",
      "EarlyStopping counter: 44 out of 50\n",
      "epoch: 105 - train_acc: 0.9311224489795918 - train_loss: 0.1636789896720532 - val_acc: 0.7857142857142857 - val_loss: 0.7819392711473214\n",
      "EarlyStopping counter: 45 out of 50\n",
      "epoch: 106 - train_acc: 0.9668367346938775 - train_loss: 0.10942322420336031 - val_acc: 0.7857142857142857 - val_loss: 0.46003938906269\n",
      "EarlyStopping counter: 46 out of 50\n",
      "epoch: 107 - train_acc: 0.9591836734693877 - train_loss: 0.09040175195526551 - val_acc: 0.8571428571428571 - val_loss: 0.5187722128788702\n",
      "EarlyStopping counter: 47 out of 50\n",
      "epoch: 108 - train_acc: 0.9795918367346939 - train_loss: 0.07116417308307664 - val_acc: 0.8928571428571429 - val_loss: 1.1788053264096603\n",
      "EarlyStopping counter: 48 out of 50\n",
      "epoch: 109 - train_acc: 0.9744897959183674 - train_loss: 0.06626688960552211 - val_acc: 0.9285714285714286 - val_loss: 0.4641288363292496\n",
      "EarlyStopping counter: 49 out of 50\n",
      "epoch: 110 - train_acc: 0.9795918367346939 - train_loss: 0.05561368498285736 - val_acc: 0.9285714285714286 - val_loss: 0.36786149188137224\n",
      "EarlyStopping counter: 50 out of 50\n",
      "epoch: 111 - train_acc: 0.9872448979591837 - train_loss: 0.04006669121024549 - val_acc: 0.8928571428571429 - val_loss: 0.596849500986436\n",
      "=> Early stopped !!\n",
      "[total]\n",
      "Accuracy: 0.6863636363636364\n",
      "Precision: 0.7572815533980582\n",
      "Recall: 0.639344262295082\n",
      "F1: 0.6933333333333334\n",
      "[low]\n",
      "Accuracy: 0.671875\n",
      "Precision: 0.7560975609756098\n",
      "Recall: 0.5904761904761905\n",
      "F1: 0.6631016042780749\n",
      "[high]\n",
      "Accuracy: 0.75\n",
      "Precision: 0.6428571428571429\n",
      "Recall: 1.0\n",
      "F1: 0.782608695652174\n",
      "\n",
      "[Fold 1]: \n",
      "len(train_sampler)=392\n",
      "len(val_sampler)=28\n",
      "[  0  15  19  33  39  75  76  78  79 113 116 148 168 184 194 208 218 262\n",
      " 271 278 294 317 329 337 367 375 399 415]\n",
      "Validation loss decreased (inf --> 0.668858).  Saving model ...\n",
      "epoch: 0 - train_acc: 0.5841836734693877 - train_loss: 0.66723414287989 - val_acc: 0.6071428571428571 - val_loss: 0.6688577399642212\n",
      "Validation loss decreased (0.668858 --> 0.644170).  Saving model ...\n",
      "epoch: 1 - train_acc: 0.6198979591836735 - train_loss: 0.6241589130183207 - val_acc: 0.6071428571428571 - val_loss: 0.6441696914910144\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 2 - train_acc: 0.7117346938775511 - train_loss: 0.5880579305197856 - val_acc: 0.5714285714285714 - val_loss: 0.693340921241262\n",
      "Validation loss decreased (0.644170 --> 0.612759).  Saving model ...\n",
      "epoch: 3 - train_acc: 0.7193877551020408 - train_loss: 0.5586668115509132 - val_acc: 0.6428571428571429 - val_loss: 0.6127587463944761\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 4 - train_acc: 0.7117346938775511 - train_loss: 0.5463697912100558 - val_acc: 0.6428571428571429 - val_loss: 0.7137732880599822\n",
      "Validation loss decreased (0.612759 --> 0.598128).  Saving model ...\n",
      "epoch: 5 - train_acc: 0.7346938775510204 - train_loss: 0.5370745072889748 - val_acc: 0.6428571428571429 - val_loss: 0.5981275678400093\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 6 - train_acc: 0.7704081632653061 - train_loss: 0.46541095302148494 - val_acc: 0.6428571428571429 - val_loss: 0.636726150802649\n",
      "Validation loss decreased (0.598128 --> 0.508507).  Saving model ...\n",
      "epoch: 7 - train_acc: 0.7780612244897959 - train_loss: 0.45929509231509563 - val_acc: 0.6785714285714286 - val_loss: 0.5085065660849225\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 8 - train_acc: 0.798469387755102 - train_loss: 0.446827380643749 - val_acc: 0.6428571428571429 - val_loss: 0.7523252952955922\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 9 - train_acc: 0.8010204081632653 - train_loss: 0.43083677880193455 - val_acc: 0.6071428571428571 - val_loss: 0.5437083458364735\n",
      "EarlyStopping counter: 3 out of 50\n",
      "epoch: 10 - train_acc: 0.8392857142857143 - train_loss: 0.3704920008794831 - val_acc: 0.75 - val_loss: 0.6323712494001791\n",
      "EarlyStopping counter: 4 out of 50\n",
      "epoch: 11 - train_acc: 0.8239795918367347 - train_loss: 0.4117942493859934 - val_acc: 0.7142857142857143 - val_loss: 0.5200854172955011\n",
      "Validation loss decreased (0.508507 --> 0.457463).  Saving model ...\n",
      "epoch: 12 - train_acc: 0.8494897959183674 - train_loss: 0.3608463135346991 - val_acc: 0.6428571428571429 - val_loss: 0.4574625222577313\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 13 - train_acc: 0.8698979591836735 - train_loss: 0.3296484035927151 - val_acc: 0.75 - val_loss: 0.4912964077354357\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 14 - train_acc: 0.8520408163265306 - train_loss: 0.34018039349089946 - val_acc: 0.6428571428571429 - val_loss: 0.6323520673768934\n",
      "EarlyStopping counter: 3 out of 50\n",
      "epoch: 15 - train_acc: 0.8418367346938775 - train_loss: 0.3908393147133428 - val_acc: 0.6785714285714286 - val_loss: 0.5341094689252452\n",
      "EarlyStopping counter: 4 out of 50\n",
      "epoch: 16 - train_acc: 0.875 - train_loss: 0.308746149367219 - val_acc: 0.7142857142857143 - val_loss: 0.567823274069567\n",
      "EarlyStopping counter: 5 out of 50\n",
      "epoch: 17 - train_acc: 0.8852040816326531 - train_loss: 0.2821321388564131 - val_acc: 0.7142857142857143 - val_loss: 0.685332835038593\n",
      "EarlyStopping counter: 6 out of 50\n",
      "epoch: 18 - train_acc: 0.8979591836734694 - train_loss: 0.2603401924217163 - val_acc: 0.7142857142857143 - val_loss: 0.5224202389023709\n",
      "EarlyStopping counter: 7 out of 50\n",
      "epoch: 19 - train_acc: 0.8852040816326531 - train_loss: 0.2852094128724031 - val_acc: 0.6785714285714286 - val_loss: 0.5235716875686964\n",
      "EarlyStopping counter: 8 out of 50\n",
      "epoch: 20 - train_acc: 0.8954081632653061 - train_loss: 0.2626595102870206 - val_acc: 0.6785714285714286 - val_loss: 0.7805925892938785\n",
      "EarlyStopping counter: 9 out of 50\n",
      "epoch: 21 - train_acc: 0.875 - train_loss: 0.31832715096641717 - val_acc: 0.6071428571428571 - val_loss: 0.6281081954725718\n",
      "EarlyStopping counter: 10 out of 50\n",
      "epoch: 22 - train_acc: 0.9413265306122449 - train_loss: 0.20400002163233066 - val_acc: 0.6428571428571429 - val_loss: 0.7471534212843188\n",
      "EarlyStopping counter: 11 out of 50\n",
      "epoch: 23 - train_acc: 0.9107142857142857 - train_loss: 0.23034826592896201 - val_acc: 0.8214285714285714 - val_loss: 0.4861197463440483\n",
      "EarlyStopping counter: 12 out of 50\n",
      "epoch: 24 - train_acc: 0.9132653061224489 - train_loss: 0.23599345631952007 - val_acc: 0.6785714285714286 - val_loss: 0.9289496766446479\n",
      "EarlyStopping counter: 13 out of 50\n",
      "epoch: 25 - train_acc: 0.9056122448979592 - train_loss: 0.2391547150843487 - val_acc: 0.6071428571428571 - val_loss: 0.5510545425867875\n",
      "EarlyStopping counter: 14 out of 50\n",
      "epoch: 26 - train_acc: 0.9464285714285714 - train_loss: 0.16486000499141248 - val_acc: 0.6428571428571429 - val_loss: 0.6456062407473085\n",
      "EarlyStopping counter: 15 out of 50\n",
      "epoch: 27 - train_acc: 0.9285714285714286 - train_loss: 0.1875712503666849 - val_acc: 0.75 - val_loss: 0.5900711866670479\n",
      "Validation loss decreased (0.457463 --> 0.425760).  Saving model ...\n",
      "epoch: 28 - train_acc: 0.9260204081632653 - train_loss: 0.17670988788973535 - val_acc: 0.8571428571428571 - val_loss: 0.4257600166608093\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 29 - train_acc: 0.9362244897959183 - train_loss: 0.18386718400367338 - val_acc: 0.6785714285714286 - val_loss: 0.6020194898043709\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 30 - train_acc: 0.9438775510204082 - train_loss: 0.15067742110143395 - val_acc: 0.75 - val_loss: 0.5923147225131156\n",
      "EarlyStopping counter: 3 out of 50\n",
      "epoch: 31 - train_acc: 0.9260204081632653 - train_loss: 0.17812734802984823 - val_acc: 0.6785714285714286 - val_loss: 0.8671721296237989\n",
      "EarlyStopping counter: 4 out of 50\n",
      "epoch: 32 - train_acc: 0.9387755102040817 - train_loss: 0.19041986695498403 - val_acc: 0.6785714285714286 - val_loss: 0.624733395948863\n",
      "EarlyStopping counter: 5 out of 50\n",
      "epoch: 33 - train_acc: 0.9183673469387755 - train_loss: 0.1826876964572347 - val_acc: 0.7142857142857143 - val_loss: 0.738504716707318\n",
      "EarlyStopping counter: 6 out of 50\n",
      "epoch: 34 - train_acc: 0.9413265306122449 - train_loss: 0.13408455355023813 - val_acc: 0.7142857142857143 - val_loss: 0.5372606168836715\n",
      "EarlyStopping counter: 7 out of 50\n",
      "epoch: 35 - train_acc: 0.9464285714285714 - train_loss: 0.13790973945069462 - val_acc: 0.7142857142857143 - val_loss: 0.898333331019094\n",
      "EarlyStopping counter: 8 out of 50\n",
      "epoch: 36 - train_acc: 0.9668367346938775 - train_loss: 0.12099793061354346 - val_acc: 0.6785714285714286 - val_loss: 0.7745461119460239\n",
      "EarlyStopping counter: 9 out of 50\n",
      "epoch: 37 - train_acc: 0.9591836734693877 - train_loss: 0.12357980659465827 - val_acc: 0.75 - val_loss: 0.6957121950590313\n",
      "EarlyStopping counter: 10 out of 50\n",
      "epoch: 38 - train_acc: 0.9489795918367347 - train_loss: 0.13279938751317202 - val_acc: 0.7142857142857143 - val_loss: 0.9906625141176424\n",
      "EarlyStopping counter: 11 out of 50\n",
      "epoch: 39 - train_acc: 0.923469387755102 - train_loss: 0.18322326599597213 - val_acc: 0.6428571428571429 - val_loss: 0.8925573944954355\n",
      "EarlyStopping counter: 12 out of 50\n",
      "epoch: 40 - train_acc: 0.9413265306122449 - train_loss: 0.13977492476746628 - val_acc: 0.6428571428571429 - val_loss: 0.6695875544541708\n",
      "EarlyStopping counter: 13 out of 50\n",
      "epoch: 41 - train_acc: 0.9413265306122449 - train_loss: 0.16206871137860862 - val_acc: 0.75 - val_loss: 0.8068121228163421\n",
      "EarlyStopping counter: 14 out of 50\n",
      "epoch: 42 - train_acc: 0.9438775510204082 - train_loss: 0.1711628732543603 - val_acc: 0.6785714285714286 - val_loss: 1.5359976220418634\n",
      "EarlyStopping counter: 15 out of 50\n",
      "epoch: 43 - train_acc: 0.9336734693877551 - train_loss: 0.1316445177398605 - val_acc: 0.6428571428571429 - val_loss: 0.9120079073969236\n",
      "EarlyStopping counter: 16 out of 50\n",
      "epoch: 44 - train_acc: 0.9617346938775511 - train_loss: 0.09033578197785115 - val_acc: 0.6785714285714286 - val_loss: 0.8999403010696732\n",
      "EarlyStopping counter: 17 out of 50\n",
      "epoch: 45 - train_acc: 0.9719387755102041 - train_loss: 0.10924470525280143 - val_acc: 0.7142857142857143 - val_loss: 0.8658578601580834\n",
      "EarlyStopping counter: 18 out of 50\n",
      "epoch: 46 - train_acc: 0.9566326530612245 - train_loss: 0.1368503798126726 - val_acc: 0.75 - val_loss: 0.7064409660170614\n",
      "EarlyStopping counter: 19 out of 50\n",
      "epoch: 47 - train_acc: 0.9209183673469388 - train_loss: 0.17030724971663908 - val_acc: 0.7142857142857143 - val_loss: 0.5416470464133637\n",
      "EarlyStopping counter: 20 out of 50\n",
      "epoch: 48 - train_acc: 0.9693877551020408 - train_loss: 0.12107186148099854 - val_acc: 0.75 - val_loss: 0.9246976783796139\n",
      "EarlyStopping counter: 21 out of 50\n",
      "epoch: 49 - train_acc: 0.9566326530612245 - train_loss: 0.10898339251863423 - val_acc: 0.6428571428571429 - val_loss: 0.858393750582773\n",
      "EarlyStopping counter: 22 out of 50\n",
      "epoch: 50 - train_acc: 0.9719387755102041 - train_loss: 0.08406309465684841 - val_acc: 0.6071428571428571 - val_loss: 0.9034431397998\n",
      "EarlyStopping counter: 23 out of 50\n",
      "epoch: 51 - train_acc: 0.9872448979591837 - train_loss: 0.058347587468637586 - val_acc: 0.75 - val_loss: 0.7135181614572386\n",
      "EarlyStopping counter: 24 out of 50\n",
      "epoch: 52 - train_acc: 0.9770408163265306 - train_loss: 0.07420882506687947 - val_acc: 0.7142857142857143 - val_loss: 0.6896235729639778\n",
      "EarlyStopping counter: 25 out of 50\n",
      "epoch: 53 - train_acc: 0.9566326530612245 - train_loss: 0.13366231521682934 - val_acc: 0.6428571428571429 - val_loss: 0.7403692676842909\n",
      "EarlyStopping counter: 26 out of 50\n",
      "epoch: 54 - train_acc: 0.9311224489795918 - train_loss: 0.18430499107027276 - val_acc: 0.6428571428571429 - val_loss: 0.7832696543149856\n",
      "EarlyStopping counter: 27 out of 50\n",
      "epoch: 55 - train_acc: 0.9668367346938775 - train_loss: 0.08325982777280894 - val_acc: 0.7142857142857143 - val_loss: 0.7466494595925113\n",
      "EarlyStopping counter: 28 out of 50\n",
      "epoch: 56 - train_acc: 0.9591836734693877 - train_loss: 0.08886974981812201 - val_acc: 0.7857142857142857 - val_loss: 0.5766406953550727\n",
      "EarlyStopping counter: 29 out of 50\n",
      "epoch: 57 - train_acc: 0.9770408163265306 - train_loss: 0.08193441320950669 - val_acc: 0.6785714285714286 - val_loss: 0.8664492218075628\n",
      "EarlyStopping counter: 30 out of 50\n",
      "epoch: 58 - train_acc: 0.9821428571428571 - train_loss: 0.05890458582129692 - val_acc: 0.75 - val_loss: 0.7146521859418757\n",
      "EarlyStopping counter: 31 out of 50\n",
      "epoch: 59 - train_acc: 0.9846938775510204 - train_loss: 0.05555427674704192 - val_acc: 0.7142857142857143 - val_loss: 0.9738125397938183\n",
      "EarlyStopping counter: 32 out of 50\n",
      "epoch: 60 - train_acc: 0.9795918367346939 - train_loss: 0.07917214445854726 - val_acc: 0.7142857142857143 - val_loss: 0.9326619880039951\n",
      "EarlyStopping counter: 33 out of 50\n",
      "epoch: 61 - train_acc: 0.9795918367346939 - train_loss: 0.07618949253034199 - val_acc: 0.7857142857142857 - val_loss: 0.7224749249948128\n",
      "EarlyStopping counter: 34 out of 50\n",
      "epoch: 62 - train_acc: 0.9668367346938775 - train_loss: 0.08833563054028996 - val_acc: 0.6785714285714286 - val_loss: 0.5890616874490824\n",
      "EarlyStopping counter: 35 out of 50\n",
      "epoch: 63 - train_acc: 0.9668367346938775 - train_loss: 0.08232988021358119 - val_acc: 0.75 - val_loss: 0.7188368908835656\n",
      "EarlyStopping counter: 36 out of 50\n",
      "epoch: 64 - train_acc: 0.9642857142857143 - train_loss: 0.08048130969504413 - val_acc: 0.5357142857142857 - val_loss: 0.9835969703811351\n",
      "EarlyStopping counter: 37 out of 50\n",
      "epoch: 65 - train_acc: 0.9897959183673469 - train_loss: 0.06394539243050977 - val_acc: 0.7142857142857143 - val_loss: 1.0088858798174438\n",
      "EarlyStopping counter: 38 out of 50\n",
      "epoch: 66 - train_acc: 0.9693877551020408 - train_loss: 0.0824852552385172 - val_acc: 0.6428571428571429 - val_loss: 1.0569676984696976\n",
      "EarlyStopping counter: 39 out of 50\n",
      "epoch: 67 - train_acc: 0.9642857142857143 - train_loss: 0.08258427789350271 - val_acc: 0.75 - val_loss: 0.8469968038169259\n",
      "EarlyStopping counter: 40 out of 50\n",
      "epoch: 68 - train_acc: 0.9872448979591837 - train_loss: 0.05794879698962184 - val_acc: 0.7857142857142857 - val_loss: 0.9616581090310009\n",
      "EarlyStopping counter: 41 out of 50\n",
      "epoch: 69 - train_acc: 0.9642857142857143 - train_loss: 0.1080339878608165 - val_acc: 0.6428571428571429 - val_loss: 0.9512627210807088\n",
      "EarlyStopping counter: 42 out of 50\n",
      "epoch: 70 - train_acc: 0.9846938775510204 - train_loss: 0.058285334507947456 - val_acc: 0.7142857142857143 - val_loss: 0.9279374109064902\n",
      "EarlyStopping counter: 43 out of 50\n",
      "epoch: 71 - train_acc: 0.9821428571428571 - train_loss: 0.06190884016723499 - val_acc: 0.6785714285714286 - val_loss: 1.0937347037120841\n",
      "EarlyStopping counter: 44 out of 50\n",
      "epoch: 72 - train_acc: 0.9872448979591837 - train_loss: 0.05562638260939918 - val_acc: 0.75 - val_loss: 0.9313779930264576\n",
      "EarlyStopping counter: 45 out of 50\n",
      "epoch: 73 - train_acc: 0.9821428571428571 - train_loss: 0.06470937243191292 - val_acc: 0.75 - val_loss: 1.1100286376232813\n",
      "EarlyStopping counter: 46 out of 50\n",
      "epoch: 74 - train_acc: 0.9974489795918368 - train_loss: 0.03390743581149484 - val_acc: 0.6785714285714286 - val_loss: 1.0079024900029652\n",
      "EarlyStopping counter: 47 out of 50\n",
      "epoch: 75 - train_acc: 0.9872448979591837 - train_loss: 0.04065906304156173 - val_acc: 0.6428571428571429 - val_loss: 1.2076070815398128\n",
      "EarlyStopping counter: 48 out of 50\n",
      "epoch: 76 - train_acc: 1.0 - train_loss: 0.020285106642034326 - val_acc: 0.7142857142857143 - val_loss: 1.014543272157921\n",
      "EarlyStopping counter: 49 out of 50\n",
      "epoch: 77 - train_acc: 0.9974489795918368 - train_loss: 0.02536314216466616 - val_acc: 0.75 - val_loss: 1.1070296880452761\n",
      "EarlyStopping counter: 50 out of 50\n",
      "epoch: 78 - train_acc: 0.9897959183673469 - train_loss: 0.028323100924672034 - val_acc: 0.75 - val_loss: 1.1232689133539426\n",
      "=> Early stopped !!\n",
      "[total]\n",
      "Accuracy: 0.7363636363636363\n",
      "Precision: 0.8018867924528302\n",
      "Recall: 0.6967213114754098\n",
      "F1: 0.7456140350877193\n",
      "[low]\n",
      "Accuracy: 0.7291666666666666\n",
      "Precision: 0.8045977011494253\n",
      "Recall: 0.6666666666666666\n",
      "F1: 0.7291666666666666\n",
      "[high]\n",
      "Accuracy: 0.75\n",
      "Precision: 0.6666666666666666\n",
      "Recall: 0.8888888888888888\n",
      "F1: 0.7619047619047619\n",
      "\n",
      "[Fold 2]: \n",
      "len(train_sampler)=392\n",
      "len(val_sampler)=28\n",
      "[ 17  22  24  25  42  46  57  77  82  93 104 124 172 173 266 272 277 280\n",
      " 284 296 356 362 366 370 379 406 408 411]\n",
      "Validation loss decreased (inf --> 0.743615).  Saving model ...\n",
      "epoch: 0 - train_acc: 0.6224489795918368 - train_loss: 0.6585748182698181 - val_acc: 0.5 - val_loss: 0.7436148966118263\n",
      "Validation loss decreased (0.743615 --> 0.694155).  Saving model ...\n",
      "epoch: 1 - train_acc: 0.6530612244897959 - train_loss: 0.6216421856278147 - val_acc: 0.6071428571428571 - val_loss: 0.6941552289808567\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 2 - train_acc: 0.6836734693877551 - train_loss: 0.59865445368337 - val_acc: 0.75 - val_loss: 0.698444703867285\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 3 - train_acc: 0.7066326530612245 - train_loss: 0.5593543203429687 - val_acc: 0.5714285714285714 - val_loss: 0.8360256788334424\n",
      "Validation loss decreased (0.694155 --> 0.577595).  Saving model ...\n",
      "epoch: 4 - train_acc: 0.7423469387755102 - train_loss: 0.524647125797174 - val_acc: 0.7857142857142857 - val_loss: 0.5775952801574931\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 5 - train_acc: 0.7346938775510204 - train_loss: 0.5208596281208973 - val_acc: 0.7142857142857143 - val_loss: 0.662379361663024\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 6 - train_acc: 0.7729591836734694 - train_loss: 0.47930866448392767 - val_acc: 0.6428571428571429 - val_loss: 0.7847154568639456\n",
      "EarlyStopping counter: 3 out of 50\n",
      "epoch: 7 - train_acc: 0.8035714285714286 - train_loss: 0.4192653575316391 - val_acc: 0.6428571428571429 - val_loss: 0.6926962334765656\n",
      "EarlyStopping counter: 4 out of 50\n",
      "epoch: 8 - train_acc: 0.8188775510204082 - train_loss: 0.4154156496815139 - val_acc: 0.6071428571428571 - val_loss: 0.7552605603130276\n",
      "EarlyStopping counter: 5 out of 50\n",
      "epoch: 9 - train_acc: 0.8061224489795918 - train_loss: 0.424585949263359 - val_acc: 0.75 - val_loss: 0.5787303233446924\n",
      "Validation loss decreased (0.577595 --> 0.479870).  Saving model ...\n",
      "epoch: 10 - train_acc: 0.8086734693877551 - train_loss: 0.40828919961049587 - val_acc: 0.6785714285714286 - val_loss: 0.47987026937666116\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 11 - train_acc: 0.8290816326530612 - train_loss: 0.36664918449820966 - val_acc: 0.6071428571428571 - val_loss: 0.8908705846935867\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 12 - train_acc: 0.8316326530612245 - train_loss: 0.378724353335049 - val_acc: 0.6785714285714286 - val_loss: 0.7338252672745023\n",
      "EarlyStopping counter: 3 out of 50\n",
      "epoch: 13 - train_acc: 0.8239795918367347 - train_loss: 0.37543605371841904 - val_acc: 0.7142857142857143 - val_loss: 0.6210851577225527\n",
      "EarlyStopping counter: 4 out of 50\n",
      "epoch: 14 - train_acc: 0.8698979591836735 - train_loss: 0.33050319873410816 - val_acc: 0.75 - val_loss: 0.5628666746260786\n",
      "EarlyStopping counter: 5 out of 50\n",
      "epoch: 15 - train_acc: 0.8520408163265306 - train_loss: 0.3333131090509017 - val_acc: 0.6428571428571429 - val_loss: 0.9312100251173188\n",
      "EarlyStopping counter: 6 out of 50\n",
      "epoch: 16 - train_acc: 0.8954081632653061 - train_loss: 0.28360476860362704 - val_acc: 0.75 - val_loss: 0.8011409578947941\n",
      "EarlyStopping counter: 7 out of 50\n",
      "epoch: 17 - train_acc: 0.8571428571428571 - train_loss: 0.28818647291646976 - val_acc: 0.7857142857142857 - val_loss: 0.6295894286546208\n",
      "EarlyStopping counter: 8 out of 50\n",
      "epoch: 18 - train_acc: 0.8954081632653061 - train_loss: 0.26153686880767046 - val_acc: 0.6785714285714286 - val_loss: 1.030372467193143\n",
      "EarlyStopping counter: 9 out of 50\n",
      "epoch: 19 - train_acc: 0.8801020408163265 - train_loss: 0.2826311826171328 - val_acc: 0.6071428571428571 - val_loss: 0.8693327326499422\n",
      "EarlyStopping counter: 10 out of 50\n",
      "epoch: 20 - train_acc: 0.8954081632653061 - train_loss: 0.2441883276101441 - val_acc: 0.5714285714285714 - val_loss: 0.7361257477624568\n",
      "EarlyStopping counter: 11 out of 50\n",
      "epoch: 21 - train_acc: 0.923469387755102 - train_loss: 0.22796072215678095 - val_acc: 0.6428571428571429 - val_loss: 0.9410821233977675\n",
      "EarlyStopping counter: 12 out of 50\n",
      "epoch: 22 - train_acc: 0.9158163265306123 - train_loss: 0.22782890548227963 - val_acc: 0.6071428571428571 - val_loss: 0.8838663669433988\n",
      "EarlyStopping counter: 13 out of 50\n",
      "epoch: 23 - train_acc: 0.8928571428571429 - train_loss: 0.2500365884477681 - val_acc: 0.6785714285714286 - val_loss: 0.9651194197072461\n",
      "EarlyStopping counter: 14 out of 50\n",
      "epoch: 24 - train_acc: 0.923469387755102 - train_loss: 0.2296385794371879 - val_acc: 0.75 - val_loss: 0.7220648777343556\n",
      "EarlyStopping counter: 15 out of 50\n",
      "epoch: 25 - train_acc: 0.9362244897959183 - train_loss: 0.16716744244942686 - val_acc: 0.6785714285714286 - val_loss: 0.7586349100686547\n",
      "EarlyStopping counter: 16 out of 50\n",
      "epoch: 26 - train_acc: 0.9336734693877551 - train_loss: 0.18260748450934317 - val_acc: 0.7142857142857143 - val_loss: 0.6139479313494768\n",
      "EarlyStopping counter: 17 out of 50\n",
      "epoch: 27 - train_acc: 0.9362244897959183 - train_loss: 0.16081498347399983 - val_acc: 0.5714285714285714 - val_loss: 1.0414290323268123\n",
      "EarlyStopping counter: 18 out of 50\n",
      "epoch: 28 - train_acc: 0.9030612244897959 - train_loss: 0.2635430548978504 - val_acc: 0.5714285714285714 - val_loss: 0.8436500004151939\n",
      "EarlyStopping counter: 19 out of 50\n",
      "epoch: 29 - train_acc: 0.9336734693877551 - train_loss: 0.17601665624919238 - val_acc: 0.6428571428571429 - val_loss: 0.9830453523291484\n",
      "EarlyStopping counter: 20 out of 50\n",
      "epoch: 30 - train_acc: 0.9362244897959183 - train_loss: 0.17772561427892714 - val_acc: 0.7142857142857143 - val_loss: 0.7206842077387571\n",
      "EarlyStopping counter: 21 out of 50\n",
      "epoch: 31 - train_acc: 0.951530612244898 - train_loss: 0.14137206482138936 - val_acc: 0.6428571428571429 - val_loss: 0.8231074800321476\n",
      "EarlyStopping counter: 22 out of 50\n",
      "epoch: 32 - train_acc: 0.951530612244898 - train_loss: 0.13494690945259497 - val_acc: 0.6428571428571429 - val_loss: 0.9139272229515313\n",
      "EarlyStopping counter: 23 out of 50\n",
      "epoch: 33 - train_acc: 0.9489795918367347 - train_loss: 0.14649021155596956 - val_acc: 0.75 - val_loss: 0.7397962677841348\n",
      "EarlyStopping counter: 24 out of 50\n",
      "epoch: 34 - train_acc: 0.9693877551020408 - train_loss: 0.09046298721076076 - val_acc: 0.6428571428571429 - val_loss: 0.988283577072325\n",
      "EarlyStopping counter: 25 out of 50\n",
      "epoch: 35 - train_acc: 0.9744897959183674 - train_loss: 0.09963128635439071 - val_acc: 0.6071428571428571 - val_loss: 1.000440082534705\n",
      "EarlyStopping counter: 26 out of 50\n",
      "epoch: 36 - train_acc: 0.9566326530612245 - train_loss: 0.12988567214478758 - val_acc: 0.7142857142857143 - val_loss: 0.9599836966009466\n",
      "EarlyStopping counter: 27 out of 50\n",
      "epoch: 37 - train_acc: 0.9413265306122449 - train_loss: 0.12397910735948185 - val_acc: 0.75 - val_loss: 0.8445088979473295\n",
      "EarlyStopping counter: 28 out of 50\n",
      "epoch: 38 - train_acc: 0.9362244897959183 - train_loss: 0.1517973521021648 - val_acc: 0.6785714285714286 - val_loss: 1.0314752936286986\n",
      "EarlyStopping counter: 29 out of 50\n",
      "epoch: 39 - train_acc: 0.9311224489795918 - train_loss: 0.1804390354264179 - val_acc: 0.6785714285714286 - val_loss: 0.8921536756183638\n",
      "EarlyStopping counter: 30 out of 50\n",
      "epoch: 40 - train_acc: 0.9362244897959183 - train_loss: 0.14294795849446956 - val_acc: 0.75 - val_loss: 0.7924065795146984\n",
      "EarlyStopping counter: 31 out of 50\n",
      "epoch: 41 - train_acc: 0.9413265306122449 - train_loss: 0.16394097070085234 - val_acc: 0.6071428571428571 - val_loss: 1.3337103072218222\n",
      "EarlyStopping counter: 32 out of 50\n",
      "epoch: 42 - train_acc: 0.9642857142857143 - train_loss: 0.10462446598203153 - val_acc: 0.7142857142857143 - val_loss: 0.6279547579883766\n",
      "EarlyStopping counter: 33 out of 50\n",
      "epoch: 43 - train_acc: 0.9438775510204082 - train_loss: 0.14439417954451608 - val_acc: 0.6428571428571429 - val_loss: 1.1589826713690679\n",
      "EarlyStopping counter: 34 out of 50\n",
      "epoch: 44 - train_acc: 0.9336734693877551 - train_loss: 0.16216079655117005 - val_acc: 0.8214285714285714 - val_loss: 0.5376104275655652\n",
      "EarlyStopping counter: 35 out of 50\n",
      "epoch: 45 - train_acc: 0.9770408163265306 - train_loss: 0.08998552997518383 - val_acc: 0.75 - val_loss: 0.691977690231838\n",
      "EarlyStopping counter: 36 out of 50\n",
      "epoch: 46 - train_acc: 0.9744897959183674 - train_loss: 0.09666855459869139 - val_acc: 0.75 - val_loss: 0.7428851430582327\n",
      "EarlyStopping counter: 37 out of 50\n",
      "epoch: 47 - train_acc: 0.9617346938775511 - train_loss: 0.1112724847375114 - val_acc: 0.6785714285714286 - val_loss: 0.8370019276328937\n",
      "EarlyStopping counter: 38 out of 50\n",
      "epoch: 48 - train_acc: 0.9795918367346939 - train_loss: 0.06689878122762637 - val_acc: 0.7857142857142857 - val_loss: 0.8602814430944384\n",
      "EarlyStopping counter: 39 out of 50\n",
      "epoch: 49 - train_acc: 0.9693877551020408 - train_loss: 0.06794617124549465 - val_acc: 0.6785714285714286 - val_loss: 0.7151428725607548\n",
      "EarlyStopping counter: 40 out of 50\n",
      "epoch: 50 - train_acc: 0.9693877551020408 - train_loss: 0.09102908302299087 - val_acc: 0.6785714285714286 - val_loss: 0.7957837927416589\n",
      "EarlyStopping counter: 41 out of 50\n",
      "epoch: 51 - train_acc: 0.9744897959183674 - train_loss: 0.06795261972825502 - val_acc: 0.6785714285714286 - val_loss: 0.849660153787059\n",
      "EarlyStopping counter: 42 out of 50\n",
      "epoch: 52 - train_acc: 0.9948979591836735 - train_loss: 0.044433761538108844 - val_acc: 0.6785714285714286 - val_loss: 0.9127453288458608\n",
      "EarlyStopping counter: 43 out of 50\n",
      "epoch: 53 - train_acc: 0.9974489795918368 - train_loss: 0.027598053656479257 - val_acc: 0.6428571428571429 - val_loss: 0.8137990911946217\n",
      "EarlyStopping counter: 44 out of 50\n",
      "epoch: 54 - train_acc: 0.9897959183673469 - train_loss: 0.049535610371625266 - val_acc: 0.6071428571428571 - val_loss: 0.912762728713788\n",
      "EarlyStopping counter: 45 out of 50\n",
      "epoch: 55 - train_acc: 0.9464285714285714 - train_loss: 0.15943869735377433 - val_acc: 0.7142857142857143 - val_loss: 1.0519489434406144\n",
      "EarlyStopping counter: 46 out of 50\n",
      "epoch: 56 - train_acc: 0.9413265306122449 - train_loss: 0.1649124576839451 - val_acc: 0.6428571428571429 - val_loss: 0.9130474406118396\n",
      "EarlyStopping counter: 47 out of 50\n",
      "epoch: 57 - train_acc: 0.9438775510204082 - train_loss: 0.15954103583638257 - val_acc: 0.7142857142857143 - val_loss: 0.8413140453807743\n",
      "EarlyStopping counter: 48 out of 50\n",
      "epoch: 58 - train_acc: 0.9642857142857143 - train_loss: 0.08559076242515957 - val_acc: 0.75 - val_loss: 0.7286325172941226\n",
      "EarlyStopping counter: 49 out of 50\n",
      "epoch: 59 - train_acc: 0.9795918367346939 - train_loss: 0.06867734477161716 - val_acc: 0.75 - val_loss: 0.6156618331912929\n",
      "EarlyStopping counter: 50 out of 50\n",
      "epoch: 60 - train_acc: 0.9617346938775511 - train_loss: 0.09585947673272367 - val_acc: 0.6428571428571429 - val_loss: 0.9853630047334654\n",
      "=> Early stopped !!\n",
      "[total]\n",
      "Accuracy: 0.7318181818181818\n",
      "Precision: 0.8\n",
      "Recall: 0.6885245901639344\n",
      "F1: 0.7400881057268722\n",
      "[low]\n",
      "Accuracy: 0.71875\n",
      "Precision: 0.7741935483870968\n",
      "Recall: 0.6857142857142857\n",
      "F1: 0.7272727272727273\n",
      "[high]\n",
      "Accuracy: 0.85\n",
      "Precision: 1.0\n",
      "Recall: 0.6666666666666666\n",
      "F1: 0.8\n",
      "\n",
      "[Fold 3]: \n",
      "len(train_sampler)=392\n",
      "len(val_sampler)=28\n",
      "[  3   5  16  18  31  45  60  63  66  84 131 153 154 203 204 210 246 248\n",
      " 265 281 289 302 333 361 371 388 393 402]\n",
      "Validation loss decreased (inf --> 0.670561).  Saving model ...\n",
      "epoch: 0 - train_acc: 0.5688775510204082 - train_loss: 0.68657126991017 - val_acc: 0.5714285714285714 - val_loss: 0.670561236987816\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 1 - train_acc: 0.6224489795918368 - train_loss: 0.6457126747647496 - val_acc: 0.5714285714285714 - val_loss: 0.6849769990302585\n",
      "Validation loss decreased (0.670561 --> 0.611544).  Saving model ...\n",
      "epoch: 2 - train_acc: 0.6862244897959183 - train_loss: 0.591457911621623 - val_acc: 0.7142857142857143 - val_loss: 0.6115443283391804\n",
      "Validation loss decreased (0.611544 --> 0.541630).  Saving model ...\n",
      "epoch: 3 - train_acc: 0.7066326530612245 - train_loss: 0.5625852668603366 - val_acc: 0.6428571428571429 - val_loss: 0.5416303506594932\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 4 - train_acc: 0.7576530612244898 - train_loss: 0.5185696949931881 - val_acc: 0.6785714285714286 - val_loss: 0.5420473549195273\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 5 - train_acc: 0.7423469387755102 - train_loss: 0.5447587308282656 - val_acc: 0.5714285714285714 - val_loss: 0.6317305454332652\n",
      "EarlyStopping counter: 3 out of 50\n",
      "epoch: 6 - train_acc: 0.7142857142857143 - train_loss: 0.564966527340516 - val_acc: 0.7142857142857143 - val_loss: 0.5897335200493967\n",
      "EarlyStopping counter: 4 out of 50\n",
      "epoch: 7 - train_acc: 0.7704081632653061 - train_loss: 0.4858234048091543 - val_acc: 0.6785714285714286 - val_loss: 0.6333706520468072\n",
      "Validation loss decreased (0.541630 --> 0.429014).  Saving model ...\n",
      "epoch: 8 - train_acc: 0.7959183673469388 - train_loss: 0.46119782805926135 - val_acc: 0.8928571428571429 - val_loss: 0.42901410278567015\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 9 - train_acc: 0.7959183673469388 - train_loss: 0.43033953332306096 - val_acc: 0.7142857142857143 - val_loss: 0.5183474033473585\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 10 - train_acc: 0.8137755102040817 - train_loss: 0.4237027864913436 - val_acc: 0.7857142857142857 - val_loss: 0.46381269090516375\n",
      "EarlyStopping counter: 3 out of 50\n",
      "epoch: 11 - train_acc: 0.8520408163265306 - train_loss: 0.3783025546017236 - val_acc: 0.8214285714285714 - val_loss: 0.44188160214295114\n",
      "EarlyStopping counter: 4 out of 50\n",
      "epoch: 12 - train_acc: 0.8545918367346939 - train_loss: 0.3824158580822798 - val_acc: 0.6785714285714286 - val_loss: 0.6014534222868548\n",
      "EarlyStopping counter: 5 out of 50\n",
      "epoch: 13 - train_acc: 0.8316326530612245 - train_loss: 0.3714823239832044 - val_acc: 0.7857142857142857 - val_loss: 0.4738457857937308\n",
      "EarlyStopping counter: 6 out of 50\n",
      "epoch: 14 - train_acc: 0.8418367346938775 - train_loss: 0.3544393655858384 - val_acc: 0.7857142857142857 - val_loss: 0.4699324846085321\n",
      "EarlyStopping counter: 7 out of 50\n",
      "epoch: 15 - train_acc: 0.8673469387755102 - train_loss: 0.31795558133282975 - val_acc: 0.8571428571428571 - val_loss: 0.4553482516737257\n",
      "EarlyStopping counter: 8 out of 50\n",
      "epoch: 16 - train_acc: 0.8571428571428571 - train_loss: 0.3277382586426817 - val_acc: 0.8214285714285714 - val_loss: 0.5021439474426673\n",
      "EarlyStopping counter: 9 out of 50\n",
      "epoch: 17 - train_acc: 0.8545918367346939 - train_loss: 0.33538691775104007 - val_acc: 0.8928571428571429 - val_loss: 0.4718840781715904\n",
      "EarlyStopping counter: 10 out of 50\n",
      "epoch: 18 - train_acc: 0.8903061224489796 - train_loss: 0.30078798166657517 - val_acc: 0.8214285714285714 - val_loss: 0.448094091884511\n",
      "EarlyStopping counter: 11 out of 50\n",
      "epoch: 19 - train_acc: 0.8979591836734694 - train_loss: 0.27732070716059615 - val_acc: 0.7857142857142857 - val_loss: 0.8781194728571862\n",
      "EarlyStopping counter: 12 out of 50\n",
      "epoch: 20 - train_acc: 0.9158163265306123 - train_loss: 0.2534351829995127 - val_acc: 0.8571428571428571 - val_loss: 0.45563506113962876\n",
      "EarlyStopping counter: 13 out of 50\n",
      "epoch: 21 - train_acc: 0.8724489795918368 - train_loss: 0.28983972267814506 - val_acc: 0.8928571428571429 - val_loss: 0.5254697636625508\n",
      "EarlyStopping counter: 14 out of 50\n",
      "epoch: 22 - train_acc: 0.8673469387755102 - train_loss: 0.31291907309049616 - val_acc: 0.7142857142857143 - val_loss: 1.552498700370598\n",
      "EarlyStopping counter: 15 out of 50\n",
      "epoch: 23 - train_acc: 0.8877551020408163 - train_loss: 0.2635132469237147 - val_acc: 0.7857142857142857 - val_loss: 0.5558309200649579\n",
      "EarlyStopping counter: 16 out of 50\n",
      "epoch: 24 - train_acc: 0.9005102040816326 - train_loss: 0.2816618168827671 - val_acc: 0.75 - val_loss: 0.6013424038825435\n",
      "EarlyStopping counter: 17 out of 50\n",
      "epoch: 25 - train_acc: 0.8979591836734694 - train_loss: 0.2730954152511083 - val_acc: 0.75 - val_loss: 0.672612219980239\n",
      "EarlyStopping counter: 18 out of 50\n",
      "epoch: 26 - train_acc: 0.9005102040816326 - train_loss: 0.2541895922562617 - val_acc: 0.75 - val_loss: 0.5992602653855035\n",
      "EarlyStopping counter: 19 out of 50\n",
      "epoch: 27 - train_acc: 0.8826530612244898 - train_loss: 0.2552149119215773 - val_acc: 0.75 - val_loss: 0.6445800574549867\n",
      "EarlyStopping counter: 20 out of 50\n",
      "epoch: 28 - train_acc: 0.8903061224489796 - train_loss: 0.2824551822647758 - val_acc: 0.8214285714285714 - val_loss: 0.7193106417759427\n",
      "EarlyStopping counter: 21 out of 50\n",
      "epoch: 29 - train_acc: 0.9209183673469388 - train_loss: 0.20801147510040202 - val_acc: 0.8214285714285714 - val_loss: 0.7158908920323199\n",
      "EarlyStopping counter: 22 out of 50\n",
      "epoch: 30 - train_acc: 0.9158163265306123 - train_loss: 0.20723026972792818 - val_acc: 0.7857142857142857 - val_loss: 0.6126020231625158\n",
      "EarlyStopping counter: 23 out of 50\n",
      "epoch: 31 - train_acc: 0.9311224489795918 - train_loss: 0.192125716168232 - val_acc: 0.8571428571428571 - val_loss: 0.5553914200780268\n",
      "EarlyStopping counter: 24 out of 50\n",
      "epoch: 32 - train_acc: 0.9387755102040817 - train_loss: 0.17085738277102178 - val_acc: 0.8214285714285714 - val_loss: 0.5471475300236337\n",
      "EarlyStopping counter: 25 out of 50\n",
      "epoch: 33 - train_acc: 0.951530612244898 - train_loss: 0.1448242841027946 - val_acc: 0.7142857142857143 - val_loss: 0.7335042611647062\n",
      "EarlyStopping counter: 26 out of 50\n",
      "epoch: 34 - train_acc: 0.9668367346938775 - train_loss: 0.14164486124210382 - val_acc: 0.8571428571428571 - val_loss: 0.6656181166428867\n",
      "EarlyStopping counter: 27 out of 50\n",
      "epoch: 35 - train_acc: 0.9285714285714286 - train_loss: 0.17221821024523365 - val_acc: 0.7857142857142857 - val_loss: 0.636799918137515\n",
      "EarlyStopping counter: 28 out of 50\n",
      "epoch: 36 - train_acc: 0.923469387755102 - train_loss: 0.1921012891283401 - val_acc: 0.7857142857142857 - val_loss: 0.7161791047749041\n",
      "EarlyStopping counter: 29 out of 50\n",
      "epoch: 37 - train_acc: 0.8979591836734694 - train_loss: 0.23961078359028815 - val_acc: 0.7857142857142857 - val_loss: 0.8311305167861107\n",
      "EarlyStopping counter: 30 out of 50\n",
      "epoch: 38 - train_acc: 0.9566326530612245 - train_loss: 0.1521198534218648 - val_acc: 0.8214285714285714 - val_loss: 0.8220995021127763\n",
      "EarlyStopping counter: 31 out of 50\n",
      "epoch: 39 - train_acc: 0.9693877551020408 - train_loss: 0.11674177011222717 - val_acc: 0.8214285714285714 - val_loss: 0.7188664513093904\n",
      "EarlyStopping counter: 32 out of 50\n",
      "epoch: 40 - train_acc: 0.9744897959183674 - train_loss: 0.11259566164454912 - val_acc: 0.7857142857142857 - val_loss: 1.010164265167803\n",
      "EarlyStopping counter: 33 out of 50\n",
      "epoch: 41 - train_acc: 0.9591836734693877 - train_loss: 0.12219135606863313 - val_acc: 0.7142857142857143 - val_loss: 1.2263743673950627\n",
      "EarlyStopping counter: 34 out of 50\n",
      "epoch: 42 - train_acc: 0.9438775510204082 - train_loss: 0.14145199798713937 - val_acc: 0.8214285714285714 - val_loss: 0.6730722383734355\n",
      "EarlyStopping counter: 35 out of 50\n",
      "epoch: 43 - train_acc: 0.9693877551020408 - train_loss: 0.12548921403983446 - val_acc: 0.7142857142857143 - val_loss: 0.7847170151837519\n",
      "EarlyStopping counter: 36 out of 50\n",
      "epoch: 44 - train_acc: 0.9311224489795918 - train_loss: 0.22931337215924874 - val_acc: 0.75 - val_loss: 0.7735029400204001\n",
      "EarlyStopping counter: 37 out of 50\n",
      "epoch: 45 - train_acc: 0.9005102040816326 - train_loss: 0.23167649873394197 - val_acc: 0.8214285714285714 - val_loss: 0.9596181733673608\n",
      "EarlyStopping counter: 38 out of 50\n",
      "epoch: 46 - train_acc: 0.923469387755102 - train_loss: 0.22485914644760557 - val_acc: 0.75 - val_loss: 0.7172572546534736\n",
      "EarlyStopping counter: 39 out of 50\n",
      "epoch: 47 - train_acc: 0.9285714285714286 - train_loss: 0.17281231000973277 - val_acc: 0.7142857142857143 - val_loss: 0.8439545288113552\n",
      "EarlyStopping counter: 40 out of 50\n",
      "epoch: 48 - train_acc: 0.9489795918367347 - train_loss: 0.1542600847830244 - val_acc: 0.75 - val_loss: 1.1156542223267298\n",
      "EarlyStopping counter: 41 out of 50\n",
      "epoch: 49 - train_acc: 0.9642857142857143 - train_loss: 0.11189860773984116 - val_acc: 0.75 - val_loss: 0.995638717680621\n",
      "EarlyStopping counter: 42 out of 50\n",
      "epoch: 50 - train_acc: 0.9744897959183674 - train_loss: 0.09358199901302268 - val_acc: 0.7857142857142857 - val_loss: 0.7906523202326444\n",
      "EarlyStopping counter: 43 out of 50\n",
      "epoch: 51 - train_acc: 0.9413265306122449 - train_loss: 0.1675226173461 - val_acc: 0.7857142857142857 - val_loss: 0.9525658514505079\n",
      "EarlyStopping counter: 44 out of 50\n",
      "epoch: 52 - train_acc: 0.9591836734693877 - train_loss: 0.11651806498366453 - val_acc: 0.75 - val_loss: 1.0005563529130759\n",
      "EarlyStopping counter: 45 out of 50\n",
      "epoch: 53 - train_acc: 0.9693877551020408 - train_loss: 0.09711081029632823 - val_acc: 0.75 - val_loss: 0.9697492574780593\n",
      "EarlyStopping counter: 46 out of 50\n",
      "epoch: 54 - train_acc: 0.9719387755102041 - train_loss: 0.10630110611868852 - val_acc: 0.8214285714285714 - val_loss: 0.9762677800586719\n",
      "EarlyStopping counter: 47 out of 50\n",
      "epoch: 55 - train_acc: 0.9617346938775511 - train_loss: 0.09584462104698642 - val_acc: 0.7857142857142857 - val_loss: 0.7224276822317823\n",
      "EarlyStopping counter: 48 out of 50\n",
      "epoch: 56 - train_acc: 0.9566326530612245 - train_loss: 0.1157200574669004 - val_acc: 0.75 - val_loss: 1.0641620355632362\n",
      "EarlyStopping counter: 49 out of 50\n",
      "epoch: 57 - train_acc: 0.9770408163265306 - train_loss: 0.08164652875585737 - val_acc: 0.8214285714285714 - val_loss: 0.9991264248110199\n",
      "EarlyStopping counter: 50 out of 50\n",
      "epoch: 58 - train_acc: 0.9617346938775511 - train_loss: 0.10728204683344676 - val_acc: 0.7857142857142857 - val_loss: 1.4118934792361735\n",
      "=> Early stopped !!\n",
      "[total]\n",
      "Accuracy: 0.5909090909090909\n",
      "Precision: 0.6403508771929824\n",
      "Recall: 0.5983606557377049\n",
      "F1: 0.6186440677966102\n",
      "[low]\n",
      "Accuracy: 0.578125\n",
      "Precision: 0.625\n",
      "Recall: 0.5714285714285714\n",
      "F1: 0.5970149253731343\n",
      "[high]\n",
      "Accuracy: 0.6\n",
      "Precision: 0.5454545454545454\n",
      "Recall: 0.6666666666666666\n",
      "F1: 0.6\n",
      "\n",
      "[Fold 4]: \n",
      "len(train_sampler)=392\n",
      "len(val_sampler)=28\n",
      "[  7  26  29 101 108 110 114 118 126 140 141 152 155 199 211 268 290 298\n",
      " 310 311 312 323 346 358 373 382 391 392]\n",
      "Validation loss decreased (inf --> 0.706019).  Saving model ...\n",
      "epoch: 0 - train_acc: 0.5943877551020408 - train_loss: 0.6720652553362317 - val_acc: 0.5357142857142857 - val_loss: 0.7060194678188231\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 1 - train_acc: 0.6811224489795918 - train_loss: 0.6160660522575484 - val_acc: 0.5357142857142857 - val_loss: 0.7200789316136994\n",
      "Validation loss decreased (0.706019 --> 0.679698).  Saving model ...\n",
      "epoch: 2 - train_acc: 0.6403061224489796 - train_loss: 0.6139854066085596 - val_acc: 0.6428571428571429 - val_loss: 0.6796978969163618\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 3 - train_acc: 0.7346938775510204 - train_loss: 0.5449993536882884 - val_acc: 0.5357142857142857 - val_loss: 0.7333533095433747\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 4 - train_acc: 0.7448979591836735 - train_loss: 0.5267515230364447 - val_acc: 0.6428571428571429 - val_loss: 0.7890804329324161\n",
      "EarlyStopping counter: 3 out of 50\n",
      "epoch: 5 - train_acc: 0.7857142857142857 - train_loss: 0.5033445249609699 - val_acc: 0.5357142857142857 - val_loss: 1.7511885271885894\n",
      "Validation loss decreased (0.679698 --> 0.653569).  Saving model ...\n",
      "epoch: 6 - train_acc: 0.7882653061224489 - train_loss: 0.48805893612558254 - val_acc: 0.6071428571428571 - val_loss: 0.6535688237905761\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 7 - train_acc: 0.7908163265306123 - train_loss: 0.4602697106284391 - val_acc: 0.6428571428571429 - val_loss: 0.8669129224063779\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 8 - train_acc: 0.8035714285714286 - train_loss: 0.45356957643612494 - val_acc: 0.6428571428571429 - val_loss: 0.856251381123841\n",
      "EarlyStopping counter: 3 out of 50\n",
      "epoch: 9 - train_acc: 0.8214285714285714 - train_loss: 0.40765547559610327 - val_acc: 0.6428571428571429 - val_loss: 0.9884655988541733\n",
      "EarlyStopping counter: 4 out of 50\n",
      "epoch: 10 - train_acc: 0.826530612244898 - train_loss: 0.45590898534619717 - val_acc: 0.6071428571428571 - val_loss: 0.7941064229366609\n",
      "EarlyStopping counter: 5 out of 50\n",
      "epoch: 11 - train_acc: 0.8622448979591837 - train_loss: 0.3699276256431905 - val_acc: 0.6428571428571429 - val_loss: 0.8501137282540026\n",
      "EarlyStopping counter: 6 out of 50\n",
      "epoch: 12 - train_acc: 0.8877551020408163 - train_loss: 0.31257872390685154 - val_acc: 0.6428571428571429 - val_loss: 0.6552621208749707\n",
      "EarlyStopping counter: 7 out of 50\n",
      "epoch: 13 - train_acc: 0.8622448979591837 - train_loss: 0.3154746088454912 - val_acc: 0.6071428571428571 - val_loss: 0.7848063174395958\n",
      "EarlyStopping counter: 8 out of 50\n",
      "epoch: 14 - train_acc: 0.8826530612244898 - train_loss: 0.30041761004235745 - val_acc: 0.6071428571428571 - val_loss: 0.9716321803725305\n",
      "EarlyStopping counter: 9 out of 50\n",
      "epoch: 15 - train_acc: 0.8418367346938775 - train_loss: 0.35930763535938776 - val_acc: 0.5357142857142857 - val_loss: 1.1774836568469722\n",
      "EarlyStopping counter: 10 out of 50\n",
      "epoch: 16 - train_acc: 0.8418367346938775 - train_loss: 0.3769673864547658 - val_acc: 0.6071428571428571 - val_loss: 0.8225219093645436\n",
      "EarlyStopping counter: 11 out of 50\n",
      "epoch: 17 - train_acc: 0.8520408163265306 - train_loss: 0.3227741451841028 - val_acc: 0.6428571428571429 - val_loss: 0.7670812616825735\n",
      "EarlyStopping counter: 12 out of 50\n",
      "epoch: 18 - train_acc: 0.8954081632653061 - train_loss: 0.31077815384452745 - val_acc: 0.7142857142857143 - val_loss: 0.6964480744893506\n",
      "EarlyStopping counter: 13 out of 50\n",
      "epoch: 19 - train_acc: 0.8928571428571429 - train_loss: 0.25310750733791965 - val_acc: 0.6428571428571429 - val_loss: 0.7776014453824548\n",
      "EarlyStopping counter: 14 out of 50\n",
      "epoch: 20 - train_acc: 0.8903061224489796 - train_loss: 0.25681799831889024 - val_acc: 0.6785714285714286 - val_loss: 0.8931617441993911\n",
      "EarlyStopping counter: 15 out of 50\n",
      "epoch: 21 - train_acc: 0.8928571428571429 - train_loss: 0.2548344607878727 - val_acc: 0.75 - val_loss: 0.6554913284906168\n",
      "EarlyStopping counter: 16 out of 50\n",
      "epoch: 22 - train_acc: 0.9260204081632653 - train_loss: 0.20796865498438868 - val_acc: 0.7142857142857143 - val_loss: 0.8547812439843834\n",
      "EarlyStopping counter: 17 out of 50\n",
      "epoch: 23 - train_acc: 0.9132653061224489 - train_loss: 0.21755321835907643 - val_acc: 0.6785714285714286 - val_loss: 0.6946260619319304\n",
      "EarlyStopping counter: 18 out of 50\n",
      "epoch: 24 - train_acc: 0.9030612244897959 - train_loss: 0.2163797857418628 - val_acc: 0.6428571428571429 - val_loss: 1.386198134670316\n",
      "EarlyStopping counter: 19 out of 50\n",
      "epoch: 25 - train_acc: 0.9362244897959183 - train_loss: 0.16993870008058232 - val_acc: 0.7142857142857143 - val_loss: 0.9687766104507484\n",
      "EarlyStopping counter: 20 out of 50\n",
      "epoch: 26 - train_acc: 0.9387755102040817 - train_loss: 0.16739018222061222 - val_acc: 0.7142857142857143 - val_loss: 0.8917918082809063\n",
      "EarlyStopping counter: 21 out of 50\n",
      "epoch: 27 - train_acc: 0.9387755102040817 - train_loss: 0.1678673787421546 - val_acc: 0.5357142857142857 - val_loss: 1.4124061619296366\n",
      "EarlyStopping counter: 22 out of 50\n",
      "epoch: 28 - train_acc: 0.8928571428571429 - train_loss: 0.2715000792037457 - val_acc: 0.6428571428571429 - val_loss: 2.130657770679263\n",
      "EarlyStopping counter: 23 out of 50\n",
      "epoch: 29 - train_acc: 0.9107142857142857 - train_loss: 0.26056557725467144 - val_acc: 0.6428571428571429 - val_loss: 0.8939016033462155\n",
      "EarlyStopping counter: 24 out of 50\n",
      "epoch: 30 - train_acc: 0.8979591836734694 - train_loss: 0.21647406721164483 - val_acc: 0.6785714285714286 - val_loss: 1.3058083416078685\n",
      "EarlyStopping counter: 25 out of 50\n",
      "epoch: 31 - train_acc: 0.9336734693877551 - train_loss: 0.188268003351324 - val_acc: 0.5357142857142857 - val_loss: 1.670918612289476\n",
      "EarlyStopping counter: 26 out of 50\n",
      "epoch: 32 - train_acc: 0.9362244897959183 - train_loss: 0.17254333053546578 - val_acc: 0.6785714285714286 - val_loss: 1.1754498246079632\n",
      "EarlyStopping counter: 27 out of 50\n",
      "epoch: 33 - train_acc: 0.923469387755102 - train_loss: 0.1748360236168967 - val_acc: 0.7142857142857143 - val_loss: 0.9902672338179757\n",
      "EarlyStopping counter: 28 out of 50\n",
      "epoch: 34 - train_acc: 0.9362244897959183 - train_loss: 0.14650931348328375 - val_acc: 0.6071428571428571 - val_loss: 1.0549450999812038\n",
      "EarlyStopping counter: 29 out of 50\n",
      "epoch: 35 - train_acc: 0.9336734693877551 - train_loss: 0.14444565761415828 - val_acc: 0.6428571428571429 - val_loss: 1.4779416001996748\n",
      "EarlyStopping counter: 30 out of 50\n",
      "epoch: 36 - train_acc: 0.9387755102040817 - train_loss: 0.15131764587593485 - val_acc: 0.75 - val_loss: 0.693233243454237\n",
      "EarlyStopping counter: 31 out of 50\n",
      "epoch: 37 - train_acc: 0.951530612244898 - train_loss: 0.11649084195870554 - val_acc: 0.7142857142857143 - val_loss: 1.1451453246281535\n",
      "Validation loss decreased (0.653569 --> 0.648105).  Saving model ...\n",
      "epoch: 38 - train_acc: 0.9693877551020408 - train_loss: 0.08902539763846365 - val_acc: 0.75 - val_loss: 0.6481050082075559\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 39 - train_acc: 0.9693877551020408 - train_loss: 0.09337810745126869 - val_acc: 0.6428571428571429 - val_loss: 1.072276031157811\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 40 - train_acc: 0.9566326530612245 - train_loss: 0.1435445093209553 - val_acc: 0.6071428571428571 - val_loss: 1.2259536676464555\n",
      "EarlyStopping counter: 3 out of 50\n",
      "epoch: 41 - train_acc: 0.9362244897959183 - train_loss: 0.18124208834796587 - val_acc: 0.6785714285714286 - val_loss: 2.4291837735258315\n",
      "EarlyStopping counter: 4 out of 50\n",
      "epoch: 42 - train_acc: 0.951530612244898 - train_loss: 0.12313242072273524 - val_acc: 0.6785714285714286 - val_loss: 1.5186345888979096\n",
      "EarlyStopping counter: 5 out of 50\n",
      "epoch: 43 - train_acc: 0.9668367346938775 - train_loss: 0.1487661011058895 - val_acc: 0.7142857142857143 - val_loss: 1.771409922141666\n",
      "EarlyStopping counter: 6 out of 50\n",
      "epoch: 44 - train_acc: 0.9566326530612245 - train_loss: 0.11740015806126078 - val_acc: 0.7142857142857143 - val_loss: 1.2244556345726212\n",
      "EarlyStopping counter: 7 out of 50\n",
      "epoch: 45 - train_acc: 0.9770408163265306 - train_loss: 0.0776949258433864 - val_acc: 0.6785714285714286 - val_loss: 0.9441656937027791\n",
      "EarlyStopping counter: 8 out of 50\n",
      "epoch: 46 - train_acc: 0.9668367346938775 - train_loss: 0.08547788331164795 - val_acc: 0.6785714285714286 - val_loss: 1.1705883780514386\n",
      "EarlyStopping counter: 9 out of 50\n",
      "epoch: 47 - train_acc: 0.9642857142857143 - train_loss: 0.110447079927395 - val_acc: 0.6071428571428571 - val_loss: 1.5769187792172166\n",
      "EarlyStopping counter: 10 out of 50\n",
      "epoch: 48 - train_acc: 0.9617346938775511 - train_loss: 0.11778682723449879 - val_acc: 0.6428571428571429 - val_loss: 1.4917718906126423\n",
      "EarlyStopping counter: 11 out of 50\n",
      "epoch: 49 - train_acc: 0.9591836734693877 - train_loss: 0.12195883162730731 - val_acc: 0.7142857142857143 - val_loss: 1.4718213239404259\n",
      "EarlyStopping counter: 12 out of 50\n",
      "epoch: 50 - train_acc: 0.9617346938775511 - train_loss: 0.1090915027488861 - val_acc: 0.6428571428571429 - val_loss: 1.077324309902622\n",
      "EarlyStopping counter: 13 out of 50\n",
      "epoch: 51 - train_acc: 0.9719387755102041 - train_loss: 0.09205387437674899 - val_acc: 0.7142857142857143 - val_loss: 1.3293394302461288\n",
      "EarlyStopping counter: 14 out of 50\n",
      "epoch: 52 - train_acc: 0.9795918367346939 - train_loss: 0.06488333116207372 - val_acc: 0.6428571428571429 - val_loss: 1.3912681536225933\n",
      "EarlyStopping counter: 15 out of 50\n",
      "epoch: 53 - train_acc: 0.9821428571428571 - train_loss: 0.0668905956227162 - val_acc: 0.6428571428571429 - val_loss: 1.5436890479943566\n",
      "EarlyStopping counter: 16 out of 50\n",
      "epoch: 54 - train_acc: 0.9846938775510204 - train_loss: 0.06107905105495131 - val_acc: 0.6785714285714286 - val_loss: 1.5819898739700293\n",
      "EarlyStopping counter: 17 out of 50\n",
      "epoch: 55 - train_acc: 0.9744897959183674 - train_loss: 0.06091705746053269 - val_acc: 0.6785714285714286 - val_loss: 0.792449165865345\n",
      "EarlyStopping counter: 18 out of 50\n",
      "epoch: 56 - train_acc: 0.9872448979591837 - train_loss: 0.05183258672345627 - val_acc: 0.7142857142857143 - val_loss: 1.103405497506205\n",
      "EarlyStopping counter: 19 out of 50\n",
      "epoch: 57 - train_acc: 0.9897959183673469 - train_loss: 0.06347164063696055 - val_acc: 0.6071428571428571 - val_loss: 1.3041432484290607\n",
      "EarlyStopping counter: 20 out of 50\n",
      "epoch: 58 - train_acc: 0.9923469387755102 - train_loss: 0.047022817041954226 - val_acc: 0.75 - val_loss: 1.2649805376771721\n",
      "EarlyStopping counter: 21 out of 50\n",
      "epoch: 59 - train_acc: 0.9719387755102041 - train_loss: 0.0652146226890951 - val_acc: 0.6785714285714286 - val_loss: 1.1054028925542858\n",
      "EarlyStopping counter: 22 out of 50\n",
      "epoch: 60 - train_acc: 0.9872448979591837 - train_loss: 0.046915352765391105 - val_acc: 0.75 - val_loss: 1.1965312253229392\n",
      "EarlyStopping counter: 23 out of 50\n",
      "epoch: 61 - train_acc: 0.9719387755102041 - train_loss: 0.0821247493420702 - val_acc: 0.6428571428571429 - val_loss: 2.047638462651075\n",
      "EarlyStopping counter: 24 out of 50\n",
      "epoch: 62 - train_acc: 0.9642857142857143 - train_loss: 0.1052785513321956 - val_acc: 0.6428571428571429 - val_loss: 1.7665083616760542\n",
      "EarlyStopping counter: 25 out of 50\n",
      "epoch: 63 - train_acc: 0.9668367346938775 - train_loss: 0.08801178014780436 - val_acc: 0.7142857142857143 - val_loss: 1.7779063030074196\n",
      "EarlyStopping counter: 26 out of 50\n",
      "epoch: 64 - train_acc: 0.9566326530612245 - train_loss: 0.13256086527682134 - val_acc: 0.6428571428571429 - val_loss: 1.2983975881637886\n",
      "EarlyStopping counter: 27 out of 50\n",
      "epoch: 65 - train_acc: 0.9693877551020408 - train_loss: 0.08598386880248729 - val_acc: 0.6785714285714286 - val_loss: 1.8956922562307719\n",
      "EarlyStopping counter: 28 out of 50\n",
      "epoch: 66 - train_acc: 0.9897959183673469 - train_loss: 0.05141341555204312 - val_acc: 0.6785714285714286 - val_loss: 1.8355198858503072\n",
      "EarlyStopping counter: 29 out of 50\n",
      "epoch: 67 - train_acc: 0.9846938775510204 - train_loss: 0.06106189577209177 - val_acc: 0.6428571428571429 - val_loss: 1.368044900072926\n",
      "EarlyStopping counter: 30 out of 50\n",
      "epoch: 68 - train_acc: 0.9770408163265306 - train_loss: 0.08021941172617962 - val_acc: 0.6785714285714286 - val_loss: 1.579309253739815\n",
      "EarlyStopping counter: 31 out of 50\n",
      "epoch: 69 - train_acc: 0.9770408163265306 - train_loss: 0.07607929750574326 - val_acc: 0.6428571428571429 - val_loss: 1.2849381605319823\n",
      "EarlyStopping counter: 32 out of 50\n",
      "epoch: 70 - train_acc: 0.9770408163265306 - train_loss: 0.06785201824983104 - val_acc: 0.6785714285714286 - val_loss: 1.7944339720502818\n",
      "EarlyStopping counter: 33 out of 50\n",
      "epoch: 71 - train_acc: 0.9642857142857143 - train_loss: 0.10115938327373909 - val_acc: 0.6428571428571429 - val_loss: 1.6551123591735857\n",
      "EarlyStopping counter: 34 out of 50\n",
      "epoch: 72 - train_acc: 0.9846938775510204 - train_loss: 0.04447780679349236 - val_acc: 0.6785714285714286 - val_loss: 1.6295055836622057\n",
      "EarlyStopping counter: 35 out of 50\n",
      "epoch: 73 - train_acc: 0.9719387755102041 - train_loss: 0.1357293346031742 - val_acc: 0.6428571428571429 - val_loss: 1.4658353128891721\n",
      "EarlyStopping counter: 36 out of 50\n",
      "epoch: 74 - train_acc: 0.9668367346938775 - train_loss: 0.08185779966159616 - val_acc: 0.6071428571428571 - val_loss: 1.8803889090037993\n",
      "EarlyStopping counter: 37 out of 50\n",
      "epoch: 75 - train_acc: 0.9642857142857143 - train_loss: 0.0960592551532287 - val_acc: 0.6428571428571429 - val_loss: 1.7016792243004661\n",
      "EarlyStopping counter: 38 out of 50\n",
      "epoch: 76 - train_acc: 0.9821428571428571 - train_loss: 0.06192179900988504 - val_acc: 0.6428571428571429 - val_loss: 1.3716648926562245\n",
      "EarlyStopping counter: 39 out of 50\n",
      "epoch: 77 - train_acc: 0.9923469387755102 - train_loss: 0.035933282082082366 - val_acc: 0.6428571428571429 - val_loss: 1.674157916205258\n",
      "EarlyStopping counter: 40 out of 50\n",
      "epoch: 78 - train_acc: 0.9974489795918368 - train_loss: 0.02831213206596601 - val_acc: 0.7142857142857143 - val_loss: 1.5911362478175974\n",
      "EarlyStopping counter: 41 out of 50\n",
      "epoch: 79 - train_acc: 0.9846938775510204 - train_loss: 0.038305065399208575 - val_acc: 0.6785714285714286 - val_loss: 1.672332436485053\n",
      "EarlyStopping counter: 42 out of 50\n",
      "epoch: 80 - train_acc: 1.0 - train_loss: 0.011829469937422387 - val_acc: 0.6428571428571429 - val_loss: 1.682953789947675\n",
      "EarlyStopping counter: 43 out of 50\n",
      "epoch: 81 - train_acc: 0.9923469387755102 - train_loss: 0.018438386627684777 - val_acc: 0.7142857142857143 - val_loss: 1.366670772809114\n",
      "EarlyStopping counter: 44 out of 50\n",
      "epoch: 82 - train_acc: 1.0 - train_loss: 0.015650335706987255 - val_acc: 0.6785714285714286 - val_loss: 1.641134835751313\n",
      "EarlyStopping counter: 45 out of 50\n",
      "epoch: 83 - train_acc: 0.9923469387755102 - train_loss: 0.03220514641455573 - val_acc: 0.6785714285714286 - val_loss: 1.7521755222878361\n",
      "EarlyStopping counter: 46 out of 50\n",
      "epoch: 84 - train_acc: 0.9821428571428571 - train_loss: 0.05626452630139148 - val_acc: 0.4642857142857143 - val_loss: 2.6414849591626646\n",
      "EarlyStopping counter: 47 out of 50\n",
      "epoch: 85 - train_acc: 0.9566326530612245 - train_loss: 0.16007626237966865 - val_acc: 0.6428571428571429 - val_loss: 1.7198453360911437\n",
      "EarlyStopping counter: 48 out of 50\n",
      "epoch: 86 - train_acc: 0.9464285714285714 - train_loss: 0.13987297548070718 - val_acc: 0.7142857142857143 - val_loss: 1.0823696993536447\n",
      "EarlyStopping counter: 49 out of 50\n",
      "epoch: 87 - train_acc: 0.9795918367346939 - train_loss: 0.060973210762881314 - val_acc: 0.6428571428571429 - val_loss: 1.140850206133099\n",
      "EarlyStopping counter: 50 out of 50\n",
      "epoch: 88 - train_acc: 0.9821428571428571 - train_loss: 0.043027104722311145 - val_acc: 0.75 - val_loss: 1.346253723526638\n",
      "=> Early stopped !!\n",
      "[total]\n",
      "Accuracy: 0.7045454545454546\n",
      "Precision: 0.7614678899082569\n",
      "Recall: 0.680327868852459\n",
      "F1: 0.7186147186147186\n",
      "[low]\n",
      "Accuracy: 0.703125\n",
      "Precision: 0.7608695652173914\n",
      "Recall: 0.6666666666666666\n",
      "F1: 0.7106598984771574\n",
      "[high]\n",
      "Accuracy: 0.7\n",
      "Precision: 0.6363636363636364\n",
      "Recall: 0.7777777777777778\n",
      "F1: 0.7000000000000001\n",
      "\n",
      "[Fold 5]: \n",
      "len(train_sampler)=392\n",
      "len(val_sampler)=28\n",
      "[ 36  59  74  83 109 111 119 139 157 165 176 177 180 181 193 209 222 225\n",
      " 238 249 301 325 332 338 350 351 377 378]\n",
      "Validation loss decreased (inf --> 0.617205).  Saving model ...\n",
      "epoch: 0 - train_acc: 0.5612244897959183 - train_loss: 0.6851185634850216 - val_acc: 0.6785714285714286 - val_loss: 0.6172052014568604\n",
      "Validation loss decreased (0.617205 --> 0.570703).  Saving model ...\n",
      "epoch: 1 - train_acc: 0.6632653061224489 - train_loss: 0.6135964731784811 - val_acc: 0.7142857142857143 - val_loss: 0.5707025766512666\n",
      "Validation loss decreased (0.570703 --> 0.569548).  Saving model ...\n",
      "epoch: 2 - train_acc: 0.6785714285714286 - train_loss: 0.589391308382406 - val_acc: 0.6785714285714286 - val_loss: 0.5695484239694026\n",
      "Validation loss decreased (0.569548 --> 0.471191).  Saving model ...\n",
      "epoch: 3 - train_acc: 0.7270408163265306 - train_loss: 0.574905044384486 - val_acc: 0.8571428571428571 - val_loss: 0.4711906263573806\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 4 - train_acc: 0.7117346938775511 - train_loss: 0.5840377332202981 - val_acc: 0.7142857142857143 - val_loss: 0.5982199708421032\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 5 - train_acc: 0.7219387755102041 - train_loss: 0.5797933019735062 - val_acc: 0.75 - val_loss: 0.5405703233329697\n",
      "Validation loss decreased (0.471191 --> 0.386756).  Saving model ...\n",
      "epoch: 6 - train_acc: 0.7168367346938775 - train_loss: 0.5510048822879472 - val_acc: 0.7857142857142857 - val_loss: 0.38675621597869425\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 7 - train_acc: 0.7295918367346939 - train_loss: 0.5181021881095186 - val_acc: 0.7857142857142857 - val_loss: 0.4645501020461794\n",
      "Validation loss decreased (0.386756 --> 0.334967).  Saving model ...\n",
      "epoch: 8 - train_acc: 0.7831632653061225 - train_loss: 0.47908402841955283 - val_acc: 0.8928571428571429 - val_loss: 0.3349674282855843\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 9 - train_acc: 0.7704081632653061 - train_loss: 0.46598931096564916 - val_acc: 0.8928571428571429 - val_loss: 0.35445970720183895\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 10 - train_acc: 0.8112244897959183 - train_loss: 0.4426557202023912 - val_acc: 0.6785714285714286 - val_loss: 0.5606473736684684\n",
      "Validation loss decreased (0.334967 --> 0.309326).  Saving model ...\n",
      "epoch: 11 - train_acc: 0.7908163265306123 - train_loss: 0.46838640956664257 - val_acc: 0.8571428571428571 - val_loss: 0.30932582189793495\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 12 - train_acc: 0.8163265306122449 - train_loss: 0.39465293425954256 - val_acc: 0.9285714285714286 - val_loss: 0.33972549378125955\n",
      "Validation loss decreased (0.309326 --> 0.243052).  Saving model ...\n",
      "epoch: 13 - train_acc: 0.826530612244898 - train_loss: 0.39724375715112675 - val_acc: 0.9642857142857143 - val_loss: 0.24305179834242902\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 14 - train_acc: 0.798469387755102 - train_loss: 0.42688002361403143 - val_acc: 0.7857142857142857 - val_loss: 0.3854866917303815\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 15 - train_acc: 0.826530612244898 - train_loss: 0.37361609850838656 - val_acc: 0.7857142857142857 - val_loss: 0.4358765527985464\n",
      "EarlyStopping counter: 3 out of 50\n",
      "epoch: 16 - train_acc: 0.8520408163265306 - train_loss: 0.34509215059471676 - val_acc: 0.9642857142857143 - val_loss: 0.2646976712687036\n",
      "Validation loss decreased (0.243052 --> 0.198719).  Saving model ...\n",
      "epoch: 17 - train_acc: 0.8775510204081632 - train_loss: 0.3248468589293457 - val_acc: 0.9642857142857143 - val_loss: 0.19871940814176814\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 18 - train_acc: 0.8494897959183674 - train_loss: 0.32134060559990174 - val_acc: 0.9642857142857143 - val_loss: 0.22561224810399133\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 19 - train_acc: 0.8877551020408163 - train_loss: 0.2847615462251975 - val_acc: 0.9285714285714286 - val_loss: 0.2947440072743347\n",
      "EarlyStopping counter: 3 out of 50\n",
      "epoch: 20 - train_acc: 0.9005102040816326 - train_loss: 0.2801911326253683 - val_acc: 0.9285714285714286 - val_loss: 0.31386949595902913\n",
      "EarlyStopping counter: 4 out of 50\n",
      "epoch: 21 - train_acc: 0.8877551020408163 - train_loss: 0.3130798377006663 - val_acc: 0.8571428571428571 - val_loss: 0.5807448334544969\n",
      "EarlyStopping counter: 5 out of 50\n",
      "epoch: 22 - train_acc: 0.8673469387755102 - train_loss: 0.3207606114664657 - val_acc: 0.8928571428571429 - val_loss: 0.41361925209397254\n",
      "Validation loss decreased (0.198719 --> 0.132206).  Saving model ...\n",
      "epoch: 23 - train_acc: 0.8673469387755102 - train_loss: 0.2944655722881467 - val_acc: 0.9642857142857143 - val_loss: 0.13220637473429642\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 24 - train_acc: 0.8954081632653061 - train_loss: 0.23173718835850854 - val_acc: 0.9642857142857143 - val_loss: 0.1549710458118569\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 25 - train_acc: 0.9107142857142857 - train_loss: 0.21957113797521566 - val_acc: 0.9642857142857143 - val_loss: 0.2865422686904498\n",
      "EarlyStopping counter: 3 out of 50\n",
      "epoch: 26 - train_acc: 0.9056122448979592 - train_loss: 0.217383753722472 - val_acc: 0.9642857142857143 - val_loss: 0.2139347035690322\n",
      "EarlyStopping counter: 4 out of 50\n",
      "epoch: 27 - train_acc: 0.9081632653061225 - train_loss: 0.2296004205842248 - val_acc: 0.8571428571428571 - val_loss: 0.4296733353089255\n",
      "EarlyStopping counter: 5 out of 50\n",
      "epoch: 28 - train_acc: 0.8903061224489796 - train_loss: 0.28252002965008244 - val_acc: 0.9285714285714286 - val_loss: 0.1839088376232698\n",
      "EarlyStopping counter: 6 out of 50\n",
      "epoch: 29 - train_acc: 0.8928571428571429 - train_loss: 0.2518480775919865 - val_acc: 0.9642857142857143 - val_loss: 0.19097588031037144\n",
      "EarlyStopping counter: 7 out of 50\n",
      "epoch: 30 - train_acc: 0.9260204081632653 - train_loss: 0.200610454726248 - val_acc: 0.9642857142857143 - val_loss: 0.17438840691799148\n",
      "Validation loss decreased (0.132206 --> 0.102324).  Saving model ...\n",
      "epoch: 31 - train_acc: 0.9438775510204082 - train_loss: 0.16515500766155555 - val_acc: 0.9642857142857143 - val_loss: 0.10232419946737825\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 32 - train_acc: 0.9413265306122449 - train_loss: 0.15526939873788592 - val_acc: 0.9642857142857143 - val_loss: 0.17774153801909048\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 33 - train_acc: 0.9617346938775511 - train_loss: 0.1275077176590152 - val_acc: 0.9642857142857143 - val_loss: 0.14062714375664542\n",
      "EarlyStopping counter: 3 out of 50\n",
      "epoch: 34 - train_acc: 0.9464285714285714 - train_loss: 0.1617222198126283 - val_acc: 0.9642857142857143 - val_loss: 0.13794233075321155\n",
      "EarlyStopping counter: 4 out of 50\n",
      "epoch: 35 - train_acc: 0.9183673469387755 - train_loss: 0.20562843335879932 - val_acc: 0.8928571428571429 - val_loss: 0.20010995956328273\n",
      "EarlyStopping counter: 5 out of 50\n",
      "epoch: 36 - train_acc: 0.8928571428571429 - train_loss: 0.2493762465854247 - val_acc: 0.9285714285714286 - val_loss: 0.13985991450293747\n",
      "EarlyStopping counter: 6 out of 50\n",
      "epoch: 37 - train_acc: 0.9158163265306123 - train_loss: 0.2356729298786308 - val_acc: 0.8214285714285714 - val_loss: 0.3635383086900518\n",
      "EarlyStopping counter: 7 out of 50\n",
      "epoch: 38 - train_acc: 0.9464285714285714 - train_loss: 0.16411293174812502 - val_acc: 0.9642857142857143 - val_loss: 0.1669759417725336\n",
      "EarlyStopping counter: 8 out of 50\n",
      "epoch: 39 - train_acc: 0.9413265306122449 - train_loss: 0.1750040589400691 - val_acc: 0.9285714285714286 - val_loss: 0.10688170576700229\n",
      "EarlyStopping counter: 9 out of 50\n",
      "epoch: 40 - train_acc: 0.9566326530612245 - train_loss: 0.13007659258041565 - val_acc: 0.9285714285714286 - val_loss: 0.16654632761696442\n",
      "EarlyStopping counter: 10 out of 50\n",
      "epoch: 41 - train_acc: 0.9693877551020408 - train_loss: 0.1091573069570506 - val_acc: 0.8928571428571429 - val_loss: 0.1790964065109751\n",
      "EarlyStopping counter: 11 out of 50\n",
      "epoch: 42 - train_acc: 0.9719387755102041 - train_loss: 0.08444074171641486 - val_acc: 0.9285714285714286 - val_loss: 0.2044803353403059\n",
      "Validation loss decreased (0.102324 --> 0.080918).  Saving model ...\n",
      "epoch: 43 - train_acc: 0.9489795918367347 - train_loss: 0.14356889526419267 - val_acc: 0.9642857142857143 - val_loss: 0.08091772128069677\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 44 - train_acc: 0.9387755102040817 - train_loss: 0.1632254741353734 - val_acc: 0.8571428571428571 - val_loss: 0.48256757970247954\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 45 - train_acc: 0.9540816326530612 - train_loss: 0.12782746400676293 - val_acc: 0.9285714285714286 - val_loss: 0.23627072611171562\n",
      "EarlyStopping counter: 3 out of 50\n",
      "epoch: 46 - train_acc: 0.9668367346938775 - train_loss: 0.107790541149936 - val_acc: 0.8571428571428571 - val_loss: 0.30761771282908495\n",
      "EarlyStopping counter: 4 out of 50\n",
      "epoch: 47 - train_acc: 0.9285714285714286 - train_loss: 0.18738147706827618 - val_acc: 0.8928571428571429 - val_loss: 0.19027942172863171\n",
      "EarlyStopping counter: 5 out of 50\n",
      "epoch: 48 - train_acc: 0.9438775510204082 - train_loss: 0.13093142074929298 - val_acc: 0.9642857142857143 - val_loss: 0.11542756449562097\n",
      "Validation loss decreased (0.080918 --> 0.067570).  Saving model ...\n",
      "epoch: 49 - train_acc: 0.9464285714285714 - train_loss: 0.1345780352538444 - val_acc: 1.0 - val_loss: 0.06756996748679438\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 50 - train_acc: 0.9795918367346939 - train_loss: 0.08131857192011181 - val_acc: 0.9642857142857143 - val_loss: 0.11260208902225366\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 51 - train_acc: 0.9795918367346939 - train_loss: 0.07169589840771046 - val_acc: 0.9285714285714286 - val_loss: 0.31970234029012845\n",
      "EarlyStopping counter: 3 out of 50\n",
      "epoch: 52 - train_acc: 0.9642857142857143 - train_loss: 0.08922976389464243 - val_acc: 0.9285714285714286 - val_loss: 0.16875005348078334\n",
      "EarlyStopping counter: 4 out of 50\n",
      "epoch: 53 - train_acc: 0.9668367346938775 - train_loss: 0.09307047675998852 - val_acc: 0.9285714285714286 - val_loss: 0.11319557457503311\n",
      "EarlyStopping counter: 5 out of 50\n",
      "epoch: 54 - train_acc: 0.9719387755102041 - train_loss: 0.10218386738815857 - val_acc: 0.8571428571428571 - val_loss: 0.41531428121771374\n",
      "EarlyStopping counter: 6 out of 50\n",
      "epoch: 55 - train_acc: 0.9591836734693877 - train_loss: 0.12191129372765125 - val_acc: 0.8214285714285714 - val_loss: 0.4176189790661989\n",
      "EarlyStopping counter: 7 out of 50\n",
      "epoch: 56 - train_acc: 0.9668367346938775 - train_loss: 0.13589874522074585 - val_acc: 0.9285714285714286 - val_loss: 0.44044785062679326\n",
      "EarlyStopping counter: 8 out of 50\n",
      "epoch: 57 - train_acc: 0.9540816326530612 - train_loss: 0.11069938903339478 - val_acc: 0.9285714285714286 - val_loss: 0.1495642813483153\n",
      "EarlyStopping counter: 9 out of 50\n",
      "epoch: 58 - train_acc: 0.9617346938775511 - train_loss: 0.10299043067041441 - val_acc: 0.9285714285714286 - val_loss: 0.19456345094587446\n",
      "EarlyStopping counter: 10 out of 50\n",
      "epoch: 59 - train_acc: 0.9464285714285714 - train_loss: 0.13571222403087158 - val_acc: 0.9642857142857143 - val_loss: 0.14546916852058647\n",
      "EarlyStopping counter: 11 out of 50\n",
      "epoch: 60 - train_acc: 0.9668367346938775 - train_loss: 0.1079487180887092 - val_acc: 0.8571428571428571 - val_loss: 0.36871367890839657\n",
      "EarlyStopping counter: 12 out of 50\n",
      "epoch: 61 - train_acc: 0.9693877551020408 - train_loss: 0.09277826045124896 - val_acc: 0.8928571428571429 - val_loss: 0.3713402544981743\n",
      "EarlyStopping counter: 13 out of 50\n",
      "epoch: 62 - train_acc: 0.9668367346938775 - train_loss: 0.12138551580572168 - val_acc: 0.8571428571428571 - val_loss: 0.38873129935596357\n",
      "EarlyStopping counter: 14 out of 50\n",
      "epoch: 63 - train_acc: 0.9566326530612245 - train_loss: 0.12989170838537678 - val_acc: 0.8571428571428571 - val_loss: 0.40360094578079997\n",
      "EarlyStopping counter: 15 out of 50\n",
      "epoch: 64 - train_acc: 0.9617346938775511 - train_loss: 0.11492568398676159 - val_acc: 0.9285714285714286 - val_loss: 0.15289224609190052\n",
      "EarlyStopping counter: 16 out of 50\n",
      "epoch: 65 - train_acc: 0.9591836734693877 - train_loss: 0.1029744867123697 - val_acc: 0.9642857142857143 - val_loss: 0.08495097961354664\n",
      "EarlyStopping counter: 17 out of 50\n",
      "epoch: 66 - train_acc: 0.9770408163265306 - train_loss: 0.07831637409032659 - val_acc: 0.8928571428571429 - val_loss: 0.1619835886217078\n",
      "EarlyStopping counter: 18 out of 50\n",
      "epoch: 67 - train_acc: 0.9668367346938775 - train_loss: 0.07958866306747418 - val_acc: 0.8928571428571429 - val_loss: 0.25959405794579415\n",
      "EarlyStopping counter: 19 out of 50\n",
      "epoch: 68 - train_acc: 0.9821428571428571 - train_loss: 0.07607460169423666 - val_acc: 0.9285714285714286 - val_loss: 0.17670977197161897\n",
      "EarlyStopping counter: 20 out of 50\n",
      "epoch: 69 - train_acc: 0.9872448979591837 - train_loss: 0.06330270064514991 - val_acc: 0.9642857142857143 - val_loss: 0.07628432491992797\n",
      "EarlyStopping counter: 21 out of 50\n",
      "epoch: 70 - train_acc: 0.9897959183673469 - train_loss: 0.04262064868253832 - val_acc: 0.9285714285714286 - val_loss: 0.13111868015140568\n",
      "EarlyStopping counter: 22 out of 50\n",
      "epoch: 71 - train_acc: 0.9872448979591837 - train_loss: 0.04600374911503367 - val_acc: 0.9642857142857143 - val_loss: 0.08524663438856046\n",
      "EarlyStopping counter: 23 out of 50\n",
      "epoch: 72 - train_acc: 0.9719387755102041 - train_loss: 0.07766805084779722 - val_acc: 0.9642857142857143 - val_loss: 0.08171090304938632\n",
      "EarlyStopping counter: 24 out of 50\n",
      "epoch: 73 - train_acc: 0.9897959183673469 - train_loss: 0.04662693499867286 - val_acc: 0.9642857142857143 - val_loss: 0.1310713743428041\n",
      "EarlyStopping counter: 25 out of 50\n",
      "epoch: 74 - train_acc: 0.9974489795918368 - train_loss: 0.027236567772578754 - val_acc: 0.9642857142857143 - val_loss: 0.06941877224623236\n",
      "EarlyStopping counter: 26 out of 50\n",
      "epoch: 75 - train_acc: 0.9923469387755102 - train_loss: 0.03553708579596264 - val_acc: 0.9642857142857143 - val_loss: 0.11466338366424438\n",
      "EarlyStopping counter: 27 out of 50\n",
      "epoch: 76 - train_acc: 0.9795918367346939 - train_loss: 0.05428961995984087 - val_acc: 0.9642857142857143 - val_loss: 0.07737432079824592\n",
      "Validation loss decreased (0.067570 --> 0.025764).  Saving model ...\n",
      "epoch: 77 - train_acc: 0.9693877551020408 - train_loss: 0.07424489367803544 - val_acc: 1.0 - val_loss: 0.025763996860483064\n",
      "Validation loss decreased (0.025764 --> 0.023931).  Saving model ...\n",
      "epoch: 78 - train_acc: 0.9719387755102041 - train_loss: 0.07751964629916182 - val_acc: 1.0 - val_loss: 0.023930891569706556\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 79 - train_acc: 0.9770408163265306 - train_loss: 0.08751282828058174 - val_acc: 0.8214285714285714 - val_loss: 0.6036202638064886\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 80 - train_acc: 0.9642857142857143 - train_loss: 0.09687032761522299 - val_acc: 0.9642857142857143 - val_loss: 0.06423565413225048\n",
      "EarlyStopping counter: 3 out of 50\n",
      "epoch: 81 - train_acc: 0.9540816326530612 - train_loss: 0.12164381616078163 - val_acc: 0.9642857142857143 - val_loss: 0.1483769581840706\n",
      "EarlyStopping counter: 4 out of 50\n",
      "epoch: 82 - train_acc: 0.9489795918367347 - train_loss: 0.11870643475326804 - val_acc: 0.9285714285714286 - val_loss: 0.39666100244709385\n",
      "EarlyStopping counter: 5 out of 50\n",
      "epoch: 83 - train_acc: 0.9668367346938775 - train_loss: 0.10225620493487872 - val_acc: 0.8928571428571429 - val_loss: 0.27633688619661917\n",
      "EarlyStopping counter: 6 out of 50\n",
      "epoch: 84 - train_acc: 0.9617346938775511 - train_loss: 0.11062366117793733 - val_acc: 0.9285714285714286 - val_loss: 0.10231374848214009\n",
      "EarlyStopping counter: 7 out of 50\n",
      "epoch: 85 - train_acc: 0.9744897959183674 - train_loss: 0.0937245528849854 - val_acc: 0.9285714285714286 - val_loss: 0.11837971798548044\n",
      "EarlyStopping counter: 8 out of 50\n",
      "epoch: 86 - train_acc: 0.9617346938775511 - train_loss: 0.10121455833243102 - val_acc: 1.0 - val_loss: 0.036428876764364355\n",
      "EarlyStopping counter: 9 out of 50\n",
      "epoch: 87 - train_acc: 0.9617346938775511 - train_loss: 0.07826691406985883 - val_acc: 0.8928571428571429 - val_loss: 0.2921788491313373\n",
      "EarlyStopping counter: 10 out of 50\n",
      "epoch: 88 - train_acc: 0.9872448979591837 - train_loss: 0.05918724032876253 - val_acc: 0.9642857142857143 - val_loss: 0.10942091890636456\n",
      "EarlyStopping counter: 11 out of 50\n",
      "epoch: 89 - train_acc: 0.9770408163265306 - train_loss: 0.07205737922427823 - val_acc: 0.9285714285714286 - val_loss: 0.25243769660931725\n",
      "EarlyStopping counter: 12 out of 50\n",
      "epoch: 90 - train_acc: 0.9821428571428571 - train_loss: 0.04613417737047684 - val_acc: 0.8571428571428571 - val_loss: 0.7698013047453651\n",
      "EarlyStopping counter: 13 out of 50\n",
      "epoch: 91 - train_acc: 0.9872448979591837 - train_loss: 0.046879278945171285 - val_acc: 0.8928571428571429 - val_loss: 0.29096653902540853\n",
      "EarlyStopping counter: 14 out of 50\n",
      "epoch: 92 - train_acc: 0.9872448979591837 - train_loss: 0.04627605619314136 - val_acc: 0.9642857142857143 - val_loss: 0.19321369551462994\n",
      "EarlyStopping counter: 15 out of 50\n",
      "epoch: 93 - train_acc: 0.9923469387755102 - train_loss: 0.034992323671720775 - val_acc: 0.9285714285714286 - val_loss: 0.11988285596589743\n",
      "EarlyStopping counter: 16 out of 50\n",
      "epoch: 94 - train_acc: 0.9923469387755102 - train_loss: 0.03428634903676861 - val_acc: 0.9642857142857143 - val_loss: 0.06598470248528249\n",
      "EarlyStopping counter: 17 out of 50\n",
      "epoch: 95 - train_acc: 1.0 - train_loss: 0.011464497819496284 - val_acc: 0.9285714285714286 - val_loss: 0.14929558243102073\n",
      "EarlyStopping counter: 18 out of 50\n",
      "epoch: 96 - train_acc: 0.9974489795918368 - train_loss: 0.01996404554794237 - val_acc: 0.9285714285714286 - val_loss: 0.10114564267636128\n",
      "EarlyStopping counter: 19 out of 50\n",
      "epoch: 97 - train_acc: 0.9821428571428571 - train_loss: 0.0617487085003054 - val_acc: 0.8928571428571429 - val_loss: 0.8872487772111832\n",
      "EarlyStopping counter: 20 out of 50\n",
      "epoch: 98 - train_acc: 0.9744897959183674 - train_loss: 0.14563693941709063 - val_acc: 1.0 - val_loss: 0.04219836804227188\n",
      "EarlyStopping counter: 21 out of 50\n",
      "epoch: 99 - train_acc: 0.9566326530612245 - train_loss: 0.1489771872251003 - val_acc: 0.9285714285714286 - val_loss: 0.29658674627594406\n",
      "EarlyStopping counter: 22 out of 50\n",
      "epoch: 100 - train_acc: 0.9744897959183674 - train_loss: 0.07704962313655854 - val_acc: 0.9285714285714286 - val_loss: 0.4204268233084824\n",
      "EarlyStopping counter: 23 out of 50\n",
      "epoch: 101 - train_acc: 0.9923469387755102 - train_loss: 0.03656954531543365 - val_acc: 0.9642857142857143 - val_loss: 0.12819124973458254\n",
      "EarlyStopping counter: 24 out of 50\n",
      "epoch: 102 - train_acc: 0.9948979591836735 - train_loss: 0.02323623512015438 - val_acc: 0.9642857142857143 - val_loss: 0.2311706569837474\n",
      "EarlyStopping counter: 25 out of 50\n",
      "epoch: 103 - train_acc: 1.0 - train_loss: 0.022285679294139667 - val_acc: 0.9642857142857143 - val_loss: 0.09717972897180448\n",
      "EarlyStopping counter: 26 out of 50\n",
      "epoch: 104 - train_acc: 0.9846938775510204 - train_loss: 0.04225407084706367 - val_acc: 0.8928571428571429 - val_loss: 0.2388189160768694\n",
      "EarlyStopping counter: 27 out of 50\n",
      "epoch: 105 - train_acc: 0.9948979591836735 - train_loss: 0.029166267814320124 - val_acc: 0.9285714285714286 - val_loss: 0.38335725801854736\n",
      "EarlyStopping counter: 28 out of 50\n",
      "epoch: 106 - train_acc: 0.9872448979591837 - train_loss: 0.035815961526887756 - val_acc: 0.9642857142857143 - val_loss: 0.25321257340346487\n",
      "EarlyStopping counter: 29 out of 50\n",
      "epoch: 107 - train_acc: 0.9948979591836735 - train_loss: 0.02656265841030892 - val_acc: 0.9285714285714286 - val_loss: 0.29706892099289917\n",
      "EarlyStopping counter: 30 out of 50\n",
      "epoch: 108 - train_acc: 0.9846938775510204 - train_loss: 0.06039081671068461 - val_acc: 0.9285714285714286 - val_loss: 0.3527352340264372\n",
      "EarlyStopping counter: 31 out of 50\n",
      "epoch: 109 - train_acc: 0.9846938775510204 - train_loss: 0.060168960302115077 - val_acc: 0.8571428571428571 - val_loss: 0.25690333445985947\n",
      "EarlyStopping counter: 32 out of 50\n",
      "epoch: 110 - train_acc: 0.9923469387755102 - train_loss: 0.031315849537024994 - val_acc: 0.9642857142857143 - val_loss: 0.07845260012034834\n",
      "EarlyStopping counter: 33 out of 50\n",
      "epoch: 111 - train_acc: 0.9974489795918368 - train_loss: 0.026143402546361175 - val_acc: 0.8928571428571429 - val_loss: 0.25554146086922486\n",
      "EarlyStopping counter: 34 out of 50\n",
      "epoch: 112 - train_acc: 0.9974489795918368 - train_loss: 0.0164922808013837 - val_acc: 0.9285714285714286 - val_loss: 0.13440715550175383\n",
      "EarlyStopping counter: 35 out of 50\n",
      "epoch: 113 - train_acc: 0.9770408163265306 - train_loss: 0.06749229704591973 - val_acc: 0.9642857142857143 - val_loss: 0.1300532306602112\n",
      "EarlyStopping counter: 36 out of 50\n",
      "epoch: 114 - train_acc: 0.9770408163265306 - train_loss: 0.05758993236426884 - val_acc: 0.9285714285714286 - val_loss: 0.24943255795146546\n",
      "EarlyStopping counter: 37 out of 50\n",
      "epoch: 115 - train_acc: 0.9821428571428571 - train_loss: 0.05983106829646154 - val_acc: 0.9642857142857143 - val_loss: 0.27407088840071164\n",
      "EarlyStopping counter: 38 out of 50\n",
      "epoch: 116 - train_acc: 0.9923469387755102 - train_loss: 0.02362460846376622 - val_acc: 0.9285714285714286 - val_loss: 0.2025222528703639\n",
      "Validation loss decreased (0.023931 --> 0.009756).  Saving model ...\n",
      "epoch: 117 - train_acc: 0.9974489795918368 - train_loss: 0.015705134232233225 - val_acc: 1.0 - val_loss: 0.009755568587418637\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 118 - train_acc: 0.9948979591836735 - train_loss: 0.0244422714961417 - val_acc: 0.9285714285714286 - val_loss: 0.20964782449643227\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 119 - train_acc: 0.9897959183673469 - train_loss: 0.03592854144591759 - val_acc: 0.9642857142857143 - val_loss: 0.2899628616215917\n",
      "EarlyStopping counter: 3 out of 50\n",
      "epoch: 120 - train_acc: 0.9795918367346939 - train_loss: 0.04407585526658597 - val_acc: 0.9285714285714286 - val_loss: 0.3873329537931549\n",
      "EarlyStopping counter: 4 out of 50\n",
      "epoch: 121 - train_acc: 0.9770408163265306 - train_loss: 0.06720448312982619 - val_acc: 0.8928571428571429 - val_loss: 0.34609620322091433\n",
      "EarlyStopping counter: 5 out of 50\n",
      "epoch: 122 - train_acc: 0.9974489795918368 - train_loss: 0.01794061068476744 - val_acc: 0.9285714285714286 - val_loss: 0.3350703757991817\n",
      "EarlyStopping counter: 6 out of 50\n",
      "epoch: 123 - train_acc: 0.9872448979591837 - train_loss: 0.03386175397182491 - val_acc: 0.8571428571428571 - val_loss: 1.0209587298311782\n",
      "EarlyStopping counter: 7 out of 50\n",
      "epoch: 124 - train_acc: 0.9591836734693877 - train_loss: 0.1754523942670958 - val_acc: 0.6785714285714286 - val_loss: 1.043529144713031\n",
      "EarlyStopping counter: 8 out of 50\n",
      "epoch: 125 - train_acc: 0.9719387755102041 - train_loss: 0.06851647525301209 - val_acc: 0.9285714285714286 - val_loss: 0.21853163318891553\n",
      "EarlyStopping counter: 9 out of 50\n",
      "epoch: 126 - train_acc: 0.9897959183673469 - train_loss: 0.0429114708600017 - val_acc: 0.9285714285714286 - val_loss: 0.09966819492877665\n",
      "EarlyStopping counter: 10 out of 50\n",
      "epoch: 127 - train_acc: 0.9897959183673469 - train_loss: 0.06536817936283308 - val_acc: 0.8928571428571429 - val_loss: 0.45674033730229535\n",
      "EarlyStopping counter: 11 out of 50\n",
      "epoch: 128 - train_acc: 0.9795918367346939 - train_loss: 0.061057101721407506 - val_acc: 1.0 - val_loss: 0.07263366367594033\n",
      "EarlyStopping counter: 12 out of 50\n",
      "epoch: 129 - train_acc: 0.9872448979591837 - train_loss: 0.0344519208493161 - val_acc: 0.8928571428571429 - val_loss: 0.3418214446095335\n",
      "EarlyStopping counter: 13 out of 50\n",
      "epoch: 130 - train_acc: 0.9897959183673469 - train_loss: 0.02889483991238711 - val_acc: 1.0 - val_loss: 0.03817107087141334\n",
      "EarlyStopping counter: 14 out of 50\n",
      "epoch: 131 - train_acc: 0.9923469387755102 - train_loss: 0.024030520589465674 - val_acc: 0.9642857142857143 - val_loss: 0.06544174048977879\n",
      "EarlyStopping counter: 15 out of 50\n",
      "epoch: 132 - train_acc: 0.9948979591836735 - train_loss: 0.018181192452435487 - val_acc: 0.9642857142857143 - val_loss: 0.03993675177253566\n",
      "EarlyStopping counter: 16 out of 50\n",
      "epoch: 133 - train_acc: 0.9948979591836735 - train_loss: 0.022810416160611437 - val_acc: 1.0 - val_loss: 0.03640354931672821\n",
      "EarlyStopping counter: 17 out of 50\n",
      "epoch: 134 - train_acc: 0.9923469387755102 - train_loss: 0.017467635851734973 - val_acc: 1.0 - val_loss: 0.03196338718515042\n",
      "EarlyStopping counter: 18 out of 50\n",
      "epoch: 135 - train_acc: 0.9821428571428571 - train_loss: 0.04766758041651437 - val_acc: 0.9642857142857143 - val_loss: 0.16013887433290322\n",
      "EarlyStopping counter: 19 out of 50\n",
      "epoch: 136 - train_acc: 0.9923469387755102 - train_loss: 0.0353715754353804 - val_acc: 0.9285714285714286 - val_loss: 0.20758223168695\n",
      "EarlyStopping counter: 20 out of 50\n",
      "epoch: 137 - train_acc: 0.9897959183673469 - train_loss: 0.03506214267775389 - val_acc: 0.9642857142857143 - val_loss: 0.1555720957385394\n",
      "EarlyStopping counter: 21 out of 50\n",
      "epoch: 138 - train_acc: 0.9846938775510204 - train_loss: 0.050624590375514526 - val_acc: 0.9285714285714286 - val_loss: 0.2791337686351286\n",
      "EarlyStopping counter: 22 out of 50\n",
      "epoch: 139 - train_acc: 0.9719387755102041 - train_loss: 0.07482270359271108 - val_acc: 0.8928571428571429 - val_loss: 0.37115569911412405\n",
      "EarlyStopping counter: 23 out of 50\n",
      "epoch: 140 - train_acc: 0.9923469387755102 - train_loss: 0.025931914503783127 - val_acc: 0.8928571428571429 - val_loss: 0.21011317840531304\n",
      "EarlyStopping counter: 24 out of 50\n",
      "epoch: 141 - train_acc: 0.9693877551020408 - train_loss: 0.15434251382240644 - val_acc: 0.9285714285714286 - val_loss: 0.09141158350614065\n",
      "EarlyStopping counter: 25 out of 50\n",
      "epoch: 142 - train_acc: 0.9642857142857143 - train_loss: 0.08933744331392461 - val_acc: 0.9642857142857143 - val_loss: 0.17125060533791858\n",
      "EarlyStopping counter: 26 out of 50\n",
      "epoch: 143 - train_acc: 0.9770408163265306 - train_loss: 0.08452143172762643 - val_acc: 0.8928571428571429 - val_loss: 0.525584561167862\n",
      "EarlyStopping counter: 27 out of 50\n",
      "epoch: 144 - train_acc: 0.9744897959183674 - train_loss: 0.06633004948339732 - val_acc: 0.9285714285714286 - val_loss: 0.16353266582684794\n",
      "EarlyStopping counter: 28 out of 50\n",
      "epoch: 145 - train_acc: 0.9821428571428571 - train_loss: 0.05058314278932713 - val_acc: 0.9285714285714286 - val_loss: 0.09965077251960813\n",
      "EarlyStopping counter: 29 out of 50\n",
      "epoch: 146 - train_acc: 0.9846938775510204 - train_loss: 0.046357537176518984 - val_acc: 0.9285714285714286 - val_loss: 0.31261255170212415\n",
      "EarlyStopping counter: 30 out of 50\n",
      "epoch: 147 - train_acc: 0.9617346938775511 - train_loss: 0.126603657196473 - val_acc: 0.8571428571428571 - val_loss: 0.44528010928775985\n",
      "EarlyStopping counter: 31 out of 50\n",
      "epoch: 148 - train_acc: 0.9617346938775511 - train_loss: 0.07474591101313928 - val_acc: 0.9285714285714286 - val_loss: 0.11875020521141805\n",
      "EarlyStopping counter: 32 out of 50\n",
      "epoch: 149 - train_acc: 0.9821428571428571 - train_loss: 0.05268784562493726 - val_acc: 0.9642857142857143 - val_loss: 0.10305893737672633\n",
      "EarlyStopping counter: 33 out of 50\n",
      "epoch: 150 - train_acc: 0.9923469387755102 - train_loss: 0.024912463984901802 - val_acc: 0.9285714285714286 - val_loss: 0.13172678741141813\n",
      "EarlyStopping counter: 34 out of 50\n",
      "epoch: 151 - train_acc: 0.9948979591836735 - train_loss: 0.031813016044782504 - val_acc: 0.8928571428571429 - val_loss: 0.23672604660259391\n",
      "EarlyStopping counter: 35 out of 50\n",
      "epoch: 152 - train_acc: 0.9948979591836735 - train_loss: 0.018591005696157067 - val_acc: 0.9642857142857143 - val_loss: 0.06416896396824182\n",
      "EarlyStopping counter: 36 out of 50\n",
      "epoch: 153 - train_acc: 0.9948979591836735 - train_loss: 0.042471636482188976 - val_acc: 1.0 - val_loss: 0.05263908608360238\n",
      "EarlyStopping counter: 37 out of 50\n",
      "epoch: 154 - train_acc: 0.9821428571428571 - train_loss: 0.049120519500988336 - val_acc: 0.8928571428571429 - val_loss: 0.25971357060971567\n",
      "EarlyStopping counter: 38 out of 50\n",
      "epoch: 155 - train_acc: 0.9897959183673469 - train_loss: 0.03833371011202434 - val_acc: 0.9285714285714286 - val_loss: 0.3313364496555221\n",
      "EarlyStopping counter: 39 out of 50\n",
      "epoch: 156 - train_acc: 0.9872448979591837 - train_loss: 0.06835100160228269 - val_acc: 1.0 - val_loss: 0.028524412846254496\n",
      "EarlyStopping counter: 40 out of 50\n",
      "epoch: 157 - train_acc: 0.9846938775510204 - train_loss: 0.041757795647362594 - val_acc: 0.9642857142857143 - val_loss: 0.1436941872316781\n",
      "EarlyStopping counter: 41 out of 50\n",
      "epoch: 158 - train_acc: 0.9923469387755102 - train_loss: 0.037075248965992236 - val_acc: 0.9642857142857143 - val_loss: 0.1325397668265015\n",
      "EarlyStopping counter: 42 out of 50\n",
      "epoch: 159 - train_acc: 0.9795918367346939 - train_loss: 0.057315302313415355 - val_acc: 0.8928571428571429 - val_loss: 0.18296927398802054\n",
      "EarlyStopping counter: 43 out of 50\n",
      "epoch: 160 - train_acc: 0.9821428571428571 - train_loss: 0.041983367863162036 - val_acc: 0.9285714285714286 - val_loss: 0.16590730752719746\n",
      "EarlyStopping counter: 44 out of 50\n",
      "epoch: 161 - train_acc: 0.9897959183673469 - train_loss: 0.05368042472485429 - val_acc: 0.8214285714285714 - val_loss: 0.6123264811649601\n",
      "EarlyStopping counter: 45 out of 50\n",
      "epoch: 162 - train_acc: 0.9974489795918368 - train_loss: 0.01819223840503335 - val_acc: 0.9285714285714286 - val_loss: 0.49188534109764953\n",
      "EarlyStopping counter: 46 out of 50\n",
      "epoch: 163 - train_acc: 0.9948979591836735 - train_loss: 0.01790210910070146 - val_acc: 0.9285714285714286 - val_loss: 0.298386378080025\n",
      "EarlyStopping counter: 47 out of 50\n",
      "epoch: 164 - train_acc: 0.9974489795918368 - train_loss: 0.01128586367079522 - val_acc: 0.9642857142857143 - val_loss: 0.27889852726720793\n",
      "EarlyStopping counter: 48 out of 50\n",
      "epoch: 165 - train_acc: 0.9974489795918368 - train_loss: 0.009679999680035467 - val_acc: 0.9285714285714286 - val_loss: 0.42491655250532956\n",
      "EarlyStopping counter: 49 out of 50\n",
      "epoch: 166 - train_acc: 1.0 - train_loss: 0.007095223343035729 - val_acc: 0.9642857142857143 - val_loss: 0.3509439636116219\n",
      "EarlyStopping counter: 50 out of 50\n",
      "epoch: 167 - train_acc: 1.0 - train_loss: 0.00408169636900662 - val_acc: 0.9642857142857143 - val_loss: 0.27532023663527494\n",
      "=> Early stopped !!\n",
      "[total]\n",
      "Accuracy: 0.759090909090909\n",
      "Precision: 0.7674418604651163\n",
      "Recall: 0.8114754098360656\n",
      "F1: 0.7888446215139442\n",
      "[low]\n",
      "Accuracy: 0.75\n",
      "Precision: 0.7614678899082569\n",
      "Recall: 0.7904761904761904\n",
      "F1: 0.7757009345794392\n",
      "[high]\n",
      "Accuracy: 0.8\n",
      "Precision: 0.6923076923076923\n",
      "Recall: 1.0\n",
      "F1: 0.8181818181818181\n",
      "\n",
      "[Fold 6]: \n",
      "len(train_sampler)=392\n",
      "len(val_sampler)=28\n",
      "[  2   6  10  69  81  89  92 103 147 150 158 163 167 195 198 220 223 227\n",
      " 237 244 291 305 331 365 390 394 396 414]\n",
      "Validation loss decreased (inf --> 0.625041).  Saving model ...\n",
      "epoch: 0 - train_acc: 0.5867346938775511 - train_loss: 0.6677741812201626 - val_acc: 0.6785714285714286 - val_loss: 0.6250410141687488\n",
      "Validation loss decreased (0.625041 --> 0.590465).  Saving model ...\n",
      "epoch: 1 - train_acc: 0.6709183673469388 - train_loss: 0.6168569927432196 - val_acc: 0.7142857142857143 - val_loss: 0.5904648627309392\n",
      "Validation loss decreased (0.590465 --> 0.571422).  Saving model ...\n",
      "epoch: 2 - train_acc: 0.7066326530612245 - train_loss: 0.5814733093182777 - val_acc: 0.6428571428571429 - val_loss: 0.5714224044371634\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 3 - train_acc: 0.7244897959183674 - train_loss: 0.5618758155040969 - val_acc: 0.6785714285714286 - val_loss: 0.5948861735130696\n",
      "Validation loss decreased (0.571422 --> 0.457126).  Saving model ...\n",
      "epoch: 4 - train_acc: 0.7219387755102041 - train_loss: 0.5347394571656052 - val_acc: 0.7857142857142857 - val_loss: 0.4571257427417168\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 5 - train_acc: 0.7704081632653061 - train_loss: 0.480292527636132 - val_acc: 0.42857142857142855 - val_loss: 1.5518206763538727\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 6 - train_acc: 0.7448979591836735 - train_loss: 0.5201604638400394 - val_acc: 0.75 - val_loss: 0.4687993273293347\n",
      "EarlyStopping counter: 3 out of 50\n",
      "epoch: 7 - train_acc: 0.8010204081632653 - train_loss: 0.4508732961034044 - val_acc: 0.7142857142857143 - val_loss: 0.724359942216253\n",
      "EarlyStopping counter: 4 out of 50\n",
      "epoch: 8 - train_acc: 0.8010204081632653 - train_loss: 0.44629265056191153 - val_acc: 0.75 - val_loss: 0.5084553296115877\n",
      "EarlyStopping counter: 5 out of 50\n",
      "epoch: 9 - train_acc: 0.8061224489795918 - train_loss: 0.43179803630658053 - val_acc: 0.75 - val_loss: 0.6303553781930921\n",
      "EarlyStopping counter: 6 out of 50\n",
      "epoch: 10 - train_acc: 0.8188775510204082 - train_loss: 0.41374267109603724 - val_acc: 0.8571428571428571 - val_loss: 0.5214035903229758\n",
      "EarlyStopping counter: 7 out of 50\n",
      "epoch: 11 - train_acc: 0.826530612244898 - train_loss: 0.36798435178485733 - val_acc: 0.8214285714285714 - val_loss: 0.7834078968099553\n",
      "EarlyStopping counter: 8 out of 50\n",
      "epoch: 12 - train_acc: 0.8520408163265306 - train_loss: 0.3438579328152933 - val_acc: 0.6785714285714286 - val_loss: 0.722854837484921\n",
      "EarlyStopping counter: 9 out of 50\n",
      "epoch: 13 - train_acc: 0.8520408163265306 - train_loss: 0.35823078576886686 - val_acc: 0.8214285714285714 - val_loss: 0.5666146555951082\n",
      "EarlyStopping counter: 10 out of 50\n",
      "epoch: 14 - train_acc: 0.8647959183673469 - train_loss: 0.33185323010486745 - val_acc: 0.6785714285714286 - val_loss: 0.6793300114717044\n",
      "EarlyStopping counter: 11 out of 50\n",
      "epoch: 15 - train_acc: 0.8622448979591837 - train_loss: 0.36214015117555143 - val_acc: 0.8214285714285714 - val_loss: 0.7405425545127703\n",
      "EarlyStopping counter: 12 out of 50\n",
      "epoch: 16 - train_acc: 0.8698979591836735 - train_loss: 0.3221170571982736 - val_acc: 0.8214285714285714 - val_loss: 0.6024151251313672\n",
      "EarlyStopping counter: 13 out of 50\n",
      "epoch: 17 - train_acc: 0.8826530612244898 - train_loss: 0.28660701076851053 - val_acc: 0.6785714285714286 - val_loss: 0.904716406847252\n",
      "EarlyStopping counter: 14 out of 50\n",
      "epoch: 18 - train_acc: 0.9030612244897959 - train_loss: 0.2828922702170218 - val_acc: 0.7857142857142857 - val_loss: 0.4743013937405387\n",
      "EarlyStopping counter: 15 out of 50\n",
      "epoch: 19 - train_acc: 0.8979591836734694 - train_loss: 0.23901869323216743 - val_acc: 0.8214285714285714 - val_loss: 0.7144770439233221\n",
      "EarlyStopping counter: 16 out of 50\n",
      "epoch: 20 - train_acc: 0.875 - train_loss: 0.2910462250764203 - val_acc: 0.75 - val_loss: 0.7430930463972208\n",
      "EarlyStopping counter: 17 out of 50\n",
      "epoch: 21 - train_acc: 0.9030612244897959 - train_loss: 0.26897899150695387 - val_acc: 0.7142857142857143 - val_loss: 0.836674424143732\n",
      "EarlyStopping counter: 18 out of 50\n",
      "epoch: 22 - train_acc: 0.9030612244897959 - train_loss: 0.24786487303808677 - val_acc: 0.7142857142857143 - val_loss: 0.811533161012165\n",
      "EarlyStopping counter: 19 out of 50\n",
      "epoch: 23 - train_acc: 0.9311224489795918 - train_loss: 0.1922354621152392 - val_acc: 0.7857142857142857 - val_loss: 0.7826649169842997\n",
      "EarlyStopping counter: 20 out of 50\n",
      "epoch: 24 - train_acc: 0.9387755102040817 - train_loss: 0.1691433827271908 - val_acc: 0.6785714285714286 - val_loss: 1.0000045610940085\n",
      "EarlyStopping counter: 21 out of 50\n",
      "epoch: 25 - train_acc: 0.9336734693877551 - train_loss: 0.18842231714295993 - val_acc: 0.8214285714285714 - val_loss: 0.9475149597505192\n",
      "EarlyStopping counter: 22 out of 50\n",
      "epoch: 26 - train_acc: 0.9311224489795918 - train_loss: 0.18277736650414433 - val_acc: 0.5 - val_loss: 1.8884596162815517\n",
      "EarlyStopping counter: 23 out of 50\n",
      "epoch: 27 - train_acc: 0.8928571428571429 - train_loss: 0.2657287202511923 - val_acc: 0.8928571428571429 - val_loss: 0.8095773702298292\n",
      "EarlyStopping counter: 24 out of 50\n",
      "epoch: 28 - train_acc: 0.9081632653061225 - train_loss: 0.22657321096514305 - val_acc: 0.7142857142857143 - val_loss: 0.8986534699907875\n",
      "EarlyStopping counter: 25 out of 50\n",
      "epoch: 29 - train_acc: 0.9311224489795918 - train_loss: 0.19824487127042412 - val_acc: 0.7857142857142857 - val_loss: 0.5339212716255434\n",
      "EarlyStopping counter: 26 out of 50\n",
      "epoch: 30 - train_acc: 0.9540816326530612 - train_loss: 0.14675605935183722 - val_acc: 0.7857142857142857 - val_loss: 0.883993251291435\n",
      "EarlyStopping counter: 27 out of 50\n",
      "epoch: 31 - train_acc: 0.9209183673469388 - train_loss: 0.2035435220769075 - val_acc: 0.5714285714285714 - val_loss: 0.9879911476788761\n",
      "EarlyStopping counter: 28 out of 50\n",
      "epoch: 32 - train_acc: 0.9285714285714286 - train_loss: 0.1798401133382477 - val_acc: 0.6785714285714286 - val_loss: 1.2555691107857685\n",
      "EarlyStopping counter: 29 out of 50\n",
      "epoch: 33 - train_acc: 0.9540816326530612 - train_loss: 0.14547595361460566 - val_acc: 0.7857142857142857 - val_loss: 1.019010179162815\n",
      "EarlyStopping counter: 30 out of 50\n",
      "epoch: 34 - train_acc: 0.951530612244898 - train_loss: 0.12828398834110152 - val_acc: 0.75 - val_loss: 1.1445988590335852\n",
      "EarlyStopping counter: 31 out of 50\n",
      "epoch: 35 - train_acc: 0.9642857142857143 - train_loss: 0.12252932701809542 - val_acc: 0.6785714285714286 - val_loss: 0.7675083736478123\n",
      "EarlyStopping counter: 32 out of 50\n",
      "epoch: 36 - train_acc: 0.9438775510204082 - train_loss: 0.1587990260876126 - val_acc: 0.7857142857142857 - val_loss: 0.9420281151616503\n",
      "EarlyStopping counter: 33 out of 50\n",
      "epoch: 37 - train_acc: 0.9591836734693877 - train_loss: 0.13199554695999705 - val_acc: 0.7857142857142857 - val_loss: 0.7169982021162387\n",
      "EarlyStopping counter: 34 out of 50\n",
      "epoch: 38 - train_acc: 0.951530612244898 - train_loss: 0.12089250689448337 - val_acc: 0.7857142857142857 - val_loss: 1.1460918426673221\n",
      "EarlyStopping counter: 35 out of 50\n",
      "epoch: 39 - train_acc: 0.9311224489795918 - train_loss: 0.16532022958584808 - val_acc: 0.8571428571428571 - val_loss: 0.8209850959316806\n",
      "EarlyStopping counter: 36 out of 50\n",
      "epoch: 40 - train_acc: 0.9311224489795918 - train_loss: 0.21241760405431437 - val_acc: 0.8214285714285714 - val_loss: 1.1331248327186254\n",
      "EarlyStopping counter: 37 out of 50\n",
      "epoch: 41 - train_acc: 0.9617346938775511 - train_loss: 0.14944311439568375 - val_acc: 0.75 - val_loss: 1.0996418841782751\n",
      "EarlyStopping counter: 38 out of 50\n",
      "epoch: 42 - train_acc: 0.9540816326530612 - train_loss: 0.13967825855339788 - val_acc: 0.8928571428571429 - val_loss: 0.6486359620198346\n",
      "EarlyStopping counter: 39 out of 50\n",
      "epoch: 43 - train_acc: 0.9311224489795918 - train_loss: 0.17042289317236253 - val_acc: 0.6071428571428571 - val_loss: 1.093274564542249\n",
      "EarlyStopping counter: 40 out of 50\n",
      "epoch: 44 - train_acc: 0.9413265306122449 - train_loss: 0.159360926171731 - val_acc: 0.7857142857142857 - val_loss: 0.8737375823515001\n",
      "EarlyStopping counter: 41 out of 50\n",
      "epoch: 45 - train_acc: 0.951530612244898 - train_loss: 0.12189033198335318 - val_acc: 0.8571428571428571 - val_loss: 1.0040244606854656\n",
      "EarlyStopping counter: 42 out of 50\n",
      "epoch: 46 - train_acc: 0.9642857142857143 - train_loss: 0.11264490991870188 - val_acc: 0.7142857142857143 - val_loss: 0.9134103218620273\n",
      "EarlyStopping counter: 43 out of 50\n",
      "epoch: 47 - train_acc: 0.9719387755102041 - train_loss: 0.08934492920559808 - val_acc: 0.6428571428571429 - val_loss: 1.4096738946906229\n",
      "EarlyStopping counter: 44 out of 50\n",
      "epoch: 48 - train_acc: 0.9821428571428571 - train_loss: 0.07196725324112856 - val_acc: 0.7142857142857143 - val_loss: 1.0809602955620083\n",
      "EarlyStopping counter: 45 out of 50\n",
      "epoch: 49 - train_acc: 0.9821428571428571 - train_loss: 0.08137360253912666 - val_acc: 0.7142857142857143 - val_loss: 1.0171284067608384\n",
      "EarlyStopping counter: 46 out of 50\n",
      "epoch: 50 - train_acc: 0.9719387755102041 - train_loss: 0.08193790744429458 - val_acc: 0.75 - val_loss: 1.0850540172085656\n",
      "EarlyStopping counter: 47 out of 50\n",
      "epoch: 51 - train_acc: 0.9846938775510204 - train_loss: 0.06967545746259837 - val_acc: 0.75 - val_loss: 1.1872779844829626\n",
      "EarlyStopping counter: 48 out of 50\n",
      "epoch: 52 - train_acc: 0.9642857142857143 - train_loss: 0.0731831552631818 - val_acc: 0.8571428571428571 - val_loss: 1.0927863490632117\n",
      "EarlyStopping counter: 49 out of 50\n",
      "epoch: 53 - train_acc: 0.951530612244898 - train_loss: 0.10330593986838997 - val_acc: 0.7857142857142857 - val_loss: 1.098220369299341\n",
      "EarlyStopping counter: 50 out of 50\n",
      "epoch: 54 - train_acc: 0.9591836734693877 - train_loss: 0.10939952817784593 - val_acc: 0.75 - val_loss: 0.9674154256400672\n",
      "=> Early stopped !!\n",
      "[total]\n",
      "Accuracy: 0.5318181818181819\n",
      "Precision: 0.5575757575757576\n",
      "Recall: 0.7540983606557377\n",
      "F1: 0.6411149825783972\n",
      "[low]\n",
      "Accuracy: 0.5104166666666666\n",
      "Precision: 0.539568345323741\n",
      "Recall: 0.7142857142857143\n",
      "F1: 0.6147540983606558\n",
      "[high]\n",
      "Accuracy: 0.55\n",
      "Precision: 0.5\n",
      "Recall: 1.0\n",
      "F1: 0.6666666666666666\n",
      "\n",
      "[Fold 7]: \n",
      "len(train_sampler)=392\n",
      "len(val_sampler)=28\n",
      "[ 23  37  67  68  86  96  97 122 123 125 143 144 146 182 202 219 229 245\n",
      " 250 253 321 324 335 342 352 354 389 400]\n",
      "Validation loss decreased (inf --> 0.680077).  Saving model ...\n",
      "epoch: 0 - train_acc: 0.5943877551020408 - train_loss: 0.6760835083765512 - val_acc: 0.6071428571428571 - val_loss: 0.6800773997368893\n",
      "Validation loss decreased (0.680077 --> 0.636421).  Saving model ...\n",
      "epoch: 1 - train_acc: 0.6275510204081632 - train_loss: 0.6243326050173088 - val_acc: 0.6785714285714286 - val_loss: 0.636421095882694\n",
      "Validation loss decreased (0.636421 --> 0.623723).  Saving model ...\n",
      "epoch: 2 - train_acc: 0.6836734693877551 - train_loss: 0.6040764466375032 - val_acc: 0.6428571428571429 - val_loss: 0.6237226289716827\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 3 - train_acc: 0.6887755102040817 - train_loss: 0.5886645631467488 - val_acc: 0.7857142857142857 - val_loss: 0.6571543956672727\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 4 - train_acc: 0.7551020408163265 - train_loss: 0.5360995630812141 - val_acc: 0.7142857142857143 - val_loss: 0.6437951222174617\n",
      "EarlyStopping counter: 3 out of 50\n",
      "epoch: 5 - train_acc: 0.7372448979591837 - train_loss: 0.5434861341539693 - val_acc: 0.7142857142857143 - val_loss: 0.7208628319692943\n",
      "Validation loss decreased (0.623723 --> 0.622108).  Saving model ...\n",
      "epoch: 6 - train_acc: 0.7551020408163265 - train_loss: 0.5003461523387374 - val_acc: 0.7857142857142857 - val_loss: 0.6221083877177509\n",
      "Validation loss decreased (0.622108 --> 0.589444).  Saving model ...\n",
      "epoch: 7 - train_acc: 0.7295918367346939 - train_loss: 0.5318188134015298 - val_acc: 0.7857142857142857 - val_loss: 0.5894439735237256\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 8 - train_acc: 0.7780612244897959 - train_loss: 0.4861194462288441 - val_acc: 0.5714285714285714 - val_loss: 0.8451650484529813\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 9 - train_acc: 0.7959183673469388 - train_loss: 0.44556263365596155 - val_acc: 0.7142857142857143 - val_loss: 0.6799439069242675\n",
      "EarlyStopping counter: 3 out of 50\n",
      "epoch: 10 - train_acc: 0.8010204081632653 - train_loss: 0.42884455558910073 - val_acc: 0.7857142857142857 - val_loss: 0.6099462015158152\n",
      "EarlyStopping counter: 4 out of 50\n",
      "epoch: 11 - train_acc: 0.8137755102040817 - train_loss: 0.4048552019443529 - val_acc: 0.7142857142857143 - val_loss: 0.6662333713560709\n",
      "EarlyStopping counter: 5 out of 50\n",
      "epoch: 12 - train_acc: 0.8494897959183674 - train_loss: 0.37021490303998683 - val_acc: 0.6785714285714286 - val_loss: 0.6420214661354111\n",
      "EarlyStopping counter: 6 out of 50\n",
      "epoch: 13 - train_acc: 0.8367346938775511 - train_loss: 0.38134040460284907 - val_acc: 0.7857142857142857 - val_loss: 0.614952094823684\n",
      "EarlyStopping counter: 7 out of 50\n",
      "epoch: 14 - train_acc: 0.8673469387755102 - train_loss: 0.3318456377649018 - val_acc: 0.75 - val_loss: 0.747669354110588\n",
      "EarlyStopping counter: 8 out of 50\n",
      "epoch: 15 - train_acc: 0.8571428571428571 - train_loss: 0.3311912991082911 - val_acc: 0.75 - val_loss: 0.7723019095407277\n",
      "Validation loss decreased (0.589444 --> 0.555857).  Saving model ...\n",
      "epoch: 16 - train_acc: 0.8724489795918368 - train_loss: 0.32549735469228114 - val_acc: 0.8214285714285714 - val_loss: 0.5558572604694683\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 17 - train_acc: 0.8622448979591837 - train_loss: 0.3278099438362222 - val_acc: 0.8214285714285714 - val_loss: 0.7807561581715237\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 18 - train_acc: 0.8188775510204082 - train_loss: 0.35039858508104393 - val_acc: 0.7857142857142857 - val_loss: 0.7214701292385927\n",
      "EarlyStopping counter: 3 out of 50\n",
      "epoch: 19 - train_acc: 0.8852040816326531 - train_loss: 0.3220101371721822 - val_acc: 0.7857142857142857 - val_loss: 0.6839887812002513\n",
      "Validation loss decreased (0.555857 --> 0.514918).  Saving model ...\n",
      "epoch: 20 - train_acc: 0.875 - train_loss: 0.2840152895272273 - val_acc: 0.7857142857142857 - val_loss: 0.5149181696208248\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 21 - train_acc: 0.9107142857142857 - train_loss: 0.2489433996463832 - val_acc: 0.7857142857142857 - val_loss: 0.5856528084139572\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 22 - train_acc: 0.8775510204081632 - train_loss: 0.2878931781262591 - val_acc: 0.8214285714285714 - val_loss: 0.6739529373460907\n",
      "EarlyStopping counter: 3 out of 50\n",
      "epoch: 23 - train_acc: 0.9183673469387755 - train_loss: 0.21680232145296702 - val_acc: 0.7857142857142857 - val_loss: 0.6854848104923581\n",
      "EarlyStopping counter: 4 out of 50\n",
      "epoch: 24 - train_acc: 0.9285714285714286 - train_loss: 0.17365177467009804 - val_acc: 0.8214285714285714 - val_loss: 0.6445245934933275\n",
      "EarlyStopping counter: 5 out of 50\n",
      "epoch: 25 - train_acc: 0.923469387755102 - train_loss: 0.17701261089494033 - val_acc: 0.8214285714285714 - val_loss: 0.5529681718400178\n",
      "EarlyStopping counter: 6 out of 50\n",
      "epoch: 26 - train_acc: 0.9030612244897959 - train_loss: 0.22540987530715026 - val_acc: 0.8214285714285714 - val_loss: 0.6398153674774765\n",
      "EarlyStopping counter: 7 out of 50\n",
      "epoch: 27 - train_acc: 0.9209183673469388 - train_loss: 0.20335490787118762 - val_acc: 0.7857142857142857 - val_loss: 0.9056551312208034\n",
      "EarlyStopping counter: 8 out of 50\n",
      "epoch: 28 - train_acc: 0.9081632653061225 - train_loss: 0.2230410247962588 - val_acc: 0.75 - val_loss: 0.982936655494212\n",
      "EarlyStopping counter: 9 out of 50\n",
      "epoch: 29 - train_acc: 0.9158163265306123 - train_loss: 0.23862403840159502 - val_acc: 0.7857142857142857 - val_loss: 0.5794671273853989\n",
      "EarlyStopping counter: 10 out of 50\n",
      "epoch: 30 - train_acc: 0.9336734693877551 - train_loss: 0.18385372992661325 - val_acc: 0.8571428571428571 - val_loss: 0.5335605271771616\n",
      "EarlyStopping counter: 11 out of 50\n",
      "epoch: 31 - train_acc: 0.9591836734693877 - train_loss: 0.13488675216198323 - val_acc: 0.7857142857142857 - val_loss: 0.6183371556396446\n",
      "EarlyStopping counter: 12 out of 50\n",
      "epoch: 32 - train_acc: 0.951530612244898 - train_loss: 0.13385201901216595 - val_acc: 0.7857142857142857 - val_loss: 0.6898472021515318\n",
      "EarlyStopping counter: 13 out of 50\n",
      "epoch: 33 - train_acc: 0.9336734693877551 - train_loss: 0.1768396431090809 - val_acc: 0.7857142857142857 - val_loss: 0.8067870594507889\n",
      "EarlyStopping counter: 14 out of 50\n",
      "epoch: 34 - train_acc: 0.923469387755102 - train_loss: 0.21359777712378744 - val_acc: 0.75 - val_loss: 0.949895786886226\n",
      "EarlyStopping counter: 15 out of 50\n",
      "epoch: 35 - train_acc: 0.9413265306122449 - train_loss: 0.16398659096981277 - val_acc: 0.75 - val_loss: 1.1040430366259404\n",
      "EarlyStopping counter: 16 out of 50\n",
      "epoch: 36 - train_acc: 0.951530612244898 - train_loss: 0.13478031408084112 - val_acc: 0.7857142857142857 - val_loss: 0.9424650327853943\n",
      "EarlyStopping counter: 17 out of 50\n",
      "epoch: 37 - train_acc: 0.9668367346938775 - train_loss: 0.10761787304668684 - val_acc: 0.7857142857142857 - val_loss: 0.6286590238503511\n",
      "EarlyStopping counter: 18 out of 50\n",
      "epoch: 38 - train_acc: 0.951530612244898 - train_loss: 0.11994325665783787 - val_acc: 0.7857142857142857 - val_loss: 0.7956827526155367\n",
      "EarlyStopping counter: 19 out of 50\n",
      "epoch: 39 - train_acc: 0.9591836734693877 - train_loss: 0.12131979597810945 - val_acc: 0.8214285714285714 - val_loss: 0.7125920717658393\n",
      "EarlyStopping counter: 20 out of 50\n",
      "epoch: 40 - train_acc: 0.9438775510204082 - train_loss: 0.13716254206941014 - val_acc: 0.75 - val_loss: 1.2550302782460125\n",
      "EarlyStopping counter: 21 out of 50\n",
      "epoch: 41 - train_acc: 0.951530612244898 - train_loss: 0.11859104321581396 - val_acc: 0.7857142857142857 - val_loss: 0.6326381705302075\n",
      "EarlyStopping counter: 22 out of 50\n",
      "epoch: 42 - train_acc: 0.9770408163265306 - train_loss: 0.08123066593032234 - val_acc: 0.7857142857142857 - val_loss: 1.2092692202471724\n",
      "EarlyStopping counter: 23 out of 50\n",
      "epoch: 43 - train_acc: 0.9719387755102041 - train_loss: 0.08858268521190807 - val_acc: 0.7857142857142857 - val_loss: 0.670051362420637\n",
      "EarlyStopping counter: 24 out of 50\n",
      "epoch: 44 - train_acc: 0.9744897959183674 - train_loss: 0.08853647168108301 - val_acc: 0.75 - val_loss: 0.8229646694227591\n",
      "EarlyStopping counter: 25 out of 50\n",
      "epoch: 45 - train_acc: 0.9642857142857143 - train_loss: 0.10027477335981026 - val_acc: 0.8571428571428571 - val_loss: 0.8928497759703482\n",
      "EarlyStopping counter: 26 out of 50\n",
      "epoch: 46 - train_acc: 0.9617346938775511 - train_loss: 0.12858613964619547 - val_acc: 0.75 - val_loss: 1.171773638057787\n",
      "EarlyStopping counter: 27 out of 50\n",
      "epoch: 47 - train_acc: 0.9668367346938775 - train_loss: 0.09538018604450121 - val_acc: 0.8214285714285714 - val_loss: 0.767215618407001\n",
      "EarlyStopping counter: 28 out of 50\n",
      "epoch: 48 - train_acc: 0.9566326530612245 - train_loss: 0.10889203833120076 - val_acc: 0.8214285714285714 - val_loss: 0.6515685830533169\n",
      "EarlyStopping counter: 29 out of 50\n",
      "epoch: 49 - train_acc: 0.9566326530612245 - train_loss: 0.10122344653359391 - val_acc: 0.7142857142857143 - val_loss: 1.0639217103705105\n",
      "EarlyStopping counter: 30 out of 50\n",
      "epoch: 50 - train_acc: 0.951530612244898 - train_loss: 0.1164346561147908 - val_acc: 0.8214285714285714 - val_loss: 0.6170480451548108\n",
      "EarlyStopping counter: 31 out of 50\n",
      "epoch: 51 - train_acc: 0.9795918367346939 - train_loss: 0.07017116959010762 - val_acc: 0.7857142857142857 - val_loss: 0.5522662488365173\n",
      "EarlyStopping counter: 32 out of 50\n",
      "epoch: 52 - train_acc: 0.9719387755102041 - train_loss: 0.08977804059496261 - val_acc: 0.75 - val_loss: 0.8124546472464953\n",
      "EarlyStopping counter: 33 out of 50\n",
      "epoch: 53 - train_acc: 0.9744897959183674 - train_loss: 0.0951777573101299 - val_acc: 0.8214285714285714 - val_loss: 0.8009957611839613\n",
      "EarlyStopping counter: 34 out of 50\n",
      "epoch: 54 - train_acc: 0.9821428571428571 - train_loss: 0.06392028926856104 - val_acc: 0.8214285714285714 - val_loss: 1.0327058378358576\n",
      "EarlyStopping counter: 35 out of 50\n",
      "epoch: 55 - train_acc: 0.9974489795918368 - train_loss: 0.035718607442992815 - val_acc: 0.75 - val_loss: 1.1361137121652585\n",
      "EarlyStopping counter: 36 out of 50\n",
      "epoch: 56 - train_acc: 0.9719387755102041 - train_loss: 0.07746865959021461 - val_acc: 0.75 - val_loss: 1.2861995995414375\n",
      "EarlyStopping counter: 37 out of 50\n",
      "epoch: 57 - train_acc: 0.9591836734693877 - train_loss: 0.10736908742330889 - val_acc: 0.7857142857142857 - val_loss: 0.850513107381109\n",
      "EarlyStopping counter: 38 out of 50\n",
      "epoch: 58 - train_acc: 0.9566326530612245 - train_loss: 0.14068924082888157 - val_acc: 0.8214285714285714 - val_loss: 1.0019565998849689\n",
      "EarlyStopping counter: 39 out of 50\n",
      "epoch: 59 - train_acc: 0.9566326530612245 - train_loss: 0.1476107012990621 - val_acc: 0.75 - val_loss: 1.4240092601024856\n",
      "EarlyStopping counter: 40 out of 50\n",
      "epoch: 60 - train_acc: 0.9540816326530612 - train_loss: 0.12815576575386164 - val_acc: 0.75 - val_loss: 0.7703100577998225\n",
      "EarlyStopping counter: 41 out of 50\n",
      "epoch: 61 - train_acc: 0.9693877551020408 - train_loss: 0.08665620872574757 - val_acc: 0.8571428571428571 - val_loss: 0.562280248528079\n",
      "EarlyStopping counter: 42 out of 50\n",
      "epoch: 62 - train_acc: 0.9872448979591837 - train_loss: 0.06407116728249265 - val_acc: 0.8214285714285714 - val_loss: 0.6627397318637591\n",
      "Validation loss decreased (0.514918 --> 0.514734).  Saving model ...\n",
      "epoch: 63 - train_acc: 0.9923469387755102 - train_loss: 0.04663154562432848 - val_acc: 0.8214285714285714 - val_loss: 0.5147342651489036\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 64 - train_acc: 0.9744897959183674 - train_loss: 0.060304972073861596 - val_acc: 0.75 - val_loss: 0.8308493547505613\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 65 - train_acc: 0.9693877551020408 - train_loss: 0.09162971021209111 - val_acc: 0.8571428571428571 - val_loss: 1.0368135886969227\n",
      "EarlyStopping counter: 3 out of 50\n",
      "epoch: 66 - train_acc: 0.9719387755102041 - train_loss: 0.08557205959685263 - val_acc: 0.8214285714285714 - val_loss: 1.0947761450438307\n",
      "EarlyStopping counter: 4 out of 50\n",
      "epoch: 67 - train_acc: 0.9489795918367347 - train_loss: 0.14170016563497895 - val_acc: 0.75 - val_loss: 1.0562394915850224\n",
      "EarlyStopping counter: 5 out of 50\n",
      "epoch: 68 - train_acc: 0.9566326530612245 - train_loss: 0.07989632069915475 - val_acc: 0.6785714285714286 - val_loss: 0.7033480302365158\n",
      "EarlyStopping counter: 6 out of 50\n",
      "epoch: 69 - train_acc: 0.9872448979591837 - train_loss: 0.044635352673421766 - val_acc: 0.7857142857142857 - val_loss: 0.6089604825210166\n",
      "EarlyStopping counter: 7 out of 50\n",
      "epoch: 70 - train_acc: 0.9693877551020408 - train_loss: 0.07891229728877498 - val_acc: 0.8214285714285714 - val_loss: 0.5870021742546883\n",
      "EarlyStopping counter: 8 out of 50\n",
      "epoch: 71 - train_acc: 0.9770408163265306 - train_loss: 0.07088219943192114 - val_acc: 0.75 - val_loss: 0.6854532274106488\n",
      "EarlyStopping counter: 9 out of 50\n",
      "epoch: 72 - train_acc: 0.9642857142857143 - train_loss: 0.09373182331711966 - val_acc: 0.75 - val_loss: 1.080702061959876\n",
      "EarlyStopping counter: 10 out of 50\n",
      "epoch: 73 - train_acc: 0.9770408163265306 - train_loss: 0.07983675731746041 - val_acc: 0.8214285714285714 - val_loss: 0.6125402920822767\n",
      "EarlyStopping counter: 11 out of 50\n",
      "epoch: 74 - train_acc: 0.9821428571428571 - train_loss: 0.05898396194024022 - val_acc: 0.8571428571428571 - val_loss: 0.9431409432360518\n",
      "EarlyStopping counter: 12 out of 50\n",
      "epoch: 75 - train_acc: 0.9770408163265306 - train_loss: 0.06894376498797705 - val_acc: 0.7857142857142857 - val_loss: 1.138121931134585\n",
      "EarlyStopping counter: 13 out of 50\n",
      "epoch: 76 - train_acc: 0.9770408163265306 - train_loss: 0.0629162683468963 - val_acc: 0.8214285714285714 - val_loss: 0.6923428078264537\n",
      "EarlyStopping counter: 14 out of 50\n",
      "epoch: 77 - train_acc: 0.9948979591836735 - train_loss: 0.03122313834880968 - val_acc: 0.8571428571428571 - val_loss: 0.95862933441653\n",
      "EarlyStopping counter: 15 out of 50\n",
      "epoch: 78 - train_acc: 0.9693877551020408 - train_loss: 0.09485637573177173 - val_acc: 0.7142857142857143 - val_loss: 1.1473170328879427\n",
      "EarlyStopping counter: 16 out of 50\n",
      "epoch: 79 - train_acc: 0.9668367346938775 - train_loss: 0.08855549068860256 - val_acc: 0.7857142857142857 - val_loss: 0.783553714181995\n",
      "EarlyStopping counter: 17 out of 50\n",
      "epoch: 80 - train_acc: 0.9566326530612245 - train_loss: 0.15848153137290816 - val_acc: 0.8214285714285714 - val_loss: 1.4118707645602775\n",
      "Validation loss decreased (0.514734 --> 0.431905).  Saving model ...\n",
      "epoch: 81 - train_acc: 0.9566326530612245 - train_loss: 0.13602728151450771 - val_acc: 0.8571428571428571 - val_loss: 0.4319045686649885\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 82 - train_acc: 0.9464285714285714 - train_loss: 0.1449410529934115 - val_acc: 0.8214285714285714 - val_loss: 1.2374938419938402\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 83 - train_acc: 0.9795918367346939 - train_loss: 0.06332562319723603 - val_acc: 0.8214285714285714 - val_loss: 1.0079245773773151\n",
      "EarlyStopping counter: 3 out of 50\n",
      "epoch: 84 - train_acc: 0.9897959183673469 - train_loss: 0.04453163291661251 - val_acc: 0.8928571428571429 - val_loss: 0.5887562373108575\n",
      "EarlyStopping counter: 4 out of 50\n",
      "epoch: 85 - train_acc: 0.9872448979591837 - train_loss: 0.06390253399200317 - val_acc: 0.8571428571428571 - val_loss: 0.7271101125437051\n",
      "EarlyStopping counter: 5 out of 50\n",
      "epoch: 86 - train_acc: 0.9795918367346939 - train_loss: 0.053364899062758056 - val_acc: 0.7857142857142857 - val_loss: 1.6255713511165502\n",
      "EarlyStopping counter: 6 out of 50\n",
      "epoch: 87 - train_acc: 0.9872448979591837 - train_loss: 0.04283925593332226 - val_acc: 0.7857142857142857 - val_loss: 0.8497227344964465\n",
      "EarlyStopping counter: 7 out of 50\n",
      "epoch: 88 - train_acc: 0.9897959183673469 - train_loss: 0.03900906646685198 - val_acc: 0.75 - val_loss: 0.8153233754715629\n",
      "EarlyStopping counter: 8 out of 50\n",
      "epoch: 89 - train_acc: 0.9948979591836735 - train_loss: 0.021735789040403342 - val_acc: 0.7857142857142857 - val_loss: 0.863105643886275\n",
      "EarlyStopping counter: 9 out of 50\n",
      "epoch: 90 - train_acc: 0.9668367346938775 - train_loss: 0.0823279362090807 - val_acc: 0.8571428571428571 - val_loss: 0.4816569282842982\n",
      "EarlyStopping counter: 10 out of 50\n",
      "epoch: 91 - train_acc: 0.9897959183673469 - train_loss: 0.052510805563919113 - val_acc: 0.9285714285714286 - val_loss: 0.5097512783627874\n",
      "EarlyStopping counter: 11 out of 50\n",
      "epoch: 92 - train_acc: 0.9897959183673469 - train_loss: 0.036655222884235325 - val_acc: 0.8214285714285714 - val_loss: 0.7033332306293267\n",
      "EarlyStopping counter: 12 out of 50\n",
      "epoch: 93 - train_acc: 0.9974489795918368 - train_loss: 0.024645258110275896 - val_acc: 0.7857142857142857 - val_loss: 0.686563361076798\n",
      "EarlyStopping counter: 13 out of 50\n",
      "epoch: 94 - train_acc: 1.0 - train_loss: 0.02395150049371591 - val_acc: 0.8571428571428571 - val_loss: 0.6106295002222495\n",
      "EarlyStopping counter: 14 out of 50\n",
      "epoch: 95 - train_acc: 0.9948979591836735 - train_loss: 0.02668312480022855 - val_acc: 0.8214285714285714 - val_loss: 0.47311602429668276\n",
      "EarlyStopping counter: 15 out of 50\n",
      "epoch: 96 - train_acc: 0.9974489795918368 - train_loss: 0.021582739681100353 - val_acc: 0.7857142857142857 - val_loss: 0.7621807322170884\n",
      "EarlyStopping counter: 16 out of 50\n",
      "epoch: 97 - train_acc: 1.0 - train_loss: 0.016382474151981446 - val_acc: 0.7857142857142857 - val_loss: 0.610087456141813\n",
      "EarlyStopping counter: 17 out of 50\n",
      "epoch: 98 - train_acc: 1.0 - train_loss: 0.013199992983521513 - val_acc: 0.8214285714285714 - val_loss: 0.560962233279979\n",
      "EarlyStopping counter: 18 out of 50\n",
      "epoch: 99 - train_acc: 0.9948979591836735 - train_loss: 0.01293764127264719 - val_acc: 0.75 - val_loss: 0.838705714825668\n",
      "EarlyStopping counter: 19 out of 50\n",
      "epoch: 100 - train_acc: 0.9923469387755102 - train_loss: 0.03712914651940651 - val_acc: 0.7857142857142857 - val_loss: 0.7041038933106164\n",
      "EarlyStopping counter: 20 out of 50\n",
      "epoch: 101 - train_acc: 0.9948979591836735 - train_loss: 0.02428110344195696 - val_acc: 0.75 - val_loss: 0.7013001717958965\n",
      "EarlyStopping counter: 21 out of 50\n",
      "epoch: 102 - train_acc: 0.9923469387755102 - train_loss: 0.0356509504247148 - val_acc: 0.7857142857142857 - val_loss: 0.8716195635369464\n",
      "EarlyStopping counter: 22 out of 50\n",
      "epoch: 103 - train_acc: 0.9923469387755102 - train_loss: 0.02941170327764896 - val_acc: 0.7857142857142857 - val_loss: 0.9207489190493696\n",
      "EarlyStopping counter: 23 out of 50\n",
      "epoch: 104 - train_acc: 0.9770408163265306 - train_loss: 0.04906035864724657 - val_acc: 0.75 - val_loss: 0.8037823239311581\n",
      "EarlyStopping counter: 24 out of 50\n",
      "epoch: 105 - train_acc: 0.9872448979591837 - train_loss: 0.0693588466572821 - val_acc: 0.7142857142857143 - val_loss: 1.3376399190302415\n",
      "EarlyStopping counter: 25 out of 50\n",
      "epoch: 106 - train_acc: 0.9642857142857143 - train_loss: 0.1048079604977259 - val_acc: 0.7857142857142857 - val_loss: 0.6646034610908063\n",
      "EarlyStopping counter: 26 out of 50\n",
      "epoch: 107 - train_acc: 0.9693877551020408 - train_loss: 0.09668345421616241 - val_acc: 0.8214285714285714 - val_loss: 0.8176102485653473\n",
      "EarlyStopping counter: 27 out of 50\n",
      "epoch: 108 - train_acc: 0.9668367346938775 - train_loss: 0.09348161349728981 - val_acc: 0.8571428571428571 - val_loss: 0.6416840379420536\n",
      "EarlyStopping counter: 28 out of 50\n",
      "epoch: 109 - train_acc: 0.9821428571428571 - train_loss: 0.04460943094250173 - val_acc: 0.8571428571428571 - val_loss: 0.4999834341408595\n",
      "EarlyStopping counter: 29 out of 50\n",
      "epoch: 110 - train_acc: 0.9744897959183674 - train_loss: 0.04688624936727428 - val_acc: 0.7857142857142857 - val_loss: 0.8957173427517526\n",
      "EarlyStopping counter: 30 out of 50\n",
      "epoch: 111 - train_acc: 0.9872448979591837 - train_loss: 0.0377198268667283 - val_acc: 0.7857142857142857 - val_loss: 0.9468823767900292\n",
      "EarlyStopping counter: 31 out of 50\n",
      "epoch: 112 - train_acc: 0.9923469387755102 - train_loss: 0.02241021169045148 - val_acc: 0.7857142857142857 - val_loss: 1.1931768302494818\n",
      "EarlyStopping counter: 32 out of 50\n",
      "epoch: 113 - train_acc: 0.9872448979591837 - train_loss: 0.05233447809378861 - val_acc: 0.7857142857142857 - val_loss: 1.1635569241291615\n",
      "EarlyStopping counter: 33 out of 50\n",
      "epoch: 114 - train_acc: 0.9872448979591837 - train_loss: 0.03805973528220511 - val_acc: 0.7857142857142857 - val_loss: 1.0037064338874009\n",
      "EarlyStopping counter: 34 out of 50\n",
      "epoch: 115 - train_acc: 0.9948979591836735 - train_loss: 0.02022828334254992 - val_acc: 0.8214285714285714 - val_loss: 1.027589221594673\n",
      "EarlyStopping counter: 35 out of 50\n",
      "epoch: 116 - train_acc: 0.9923469387755102 - train_loss: 0.020892267809319586 - val_acc: 0.75 - val_loss: 1.2013594618803052\n",
      "EarlyStopping counter: 36 out of 50\n",
      "epoch: 117 - train_acc: 0.9795918367346939 - train_loss: 0.0434682793147581 - val_acc: 0.7857142857142857 - val_loss: 1.0725138695536234\n",
      "EarlyStopping counter: 37 out of 50\n",
      "epoch: 118 - train_acc: 0.9923469387755102 - train_loss: 0.022235146025725946 - val_acc: 0.75 - val_loss: 0.9144324594166854\n",
      "EarlyStopping counter: 38 out of 50\n",
      "epoch: 119 - train_acc: 0.9897959183673469 - train_loss: 0.02345135848946952 - val_acc: 0.7857142857142857 - val_loss: 0.9304047788539406\n",
      "EarlyStopping counter: 39 out of 50\n",
      "epoch: 120 - train_acc: 0.9872448979591837 - train_loss: 0.041210552422339494 - val_acc: 0.75 - val_loss: 0.7635237002294922\n",
      "EarlyStopping counter: 40 out of 50\n",
      "epoch: 121 - train_acc: 0.9948979591836735 - train_loss: 0.01808552462111933 - val_acc: 0.8214285714285714 - val_loss: 1.0241138107185073\n",
      "EarlyStopping counter: 41 out of 50\n",
      "epoch: 122 - train_acc: 1.0 - train_loss: 0.00904231165214353 - val_acc: 0.7857142857142857 - val_loss: 0.9015413031293835\n",
      "EarlyStopping counter: 42 out of 50\n",
      "epoch: 123 - train_acc: 1.0 - train_loss: 0.0071016756065574345 - val_acc: 0.75 - val_loss: 1.1244462533029456\n",
      "EarlyStopping counter: 43 out of 50\n",
      "epoch: 124 - train_acc: 0.9897959183673469 - train_loss: 0.04031664395238588 - val_acc: 0.7857142857142857 - val_loss: 1.3452625019883235\n",
      "EarlyStopping counter: 44 out of 50\n",
      "epoch: 125 - train_acc: 0.9591836734693877 - train_loss: 0.07473720745278256 - val_acc: 0.75 - val_loss: 1.0401852368952484\n",
      "EarlyStopping counter: 45 out of 50\n",
      "epoch: 126 - train_acc: 0.9770408163265306 - train_loss: 0.0860561619574766 - val_acc: 0.75 - val_loss: 0.8158569482194207\n",
      "EarlyStopping counter: 46 out of 50\n",
      "epoch: 127 - train_acc: 0.9566326530612245 - train_loss: 0.12192342888561067 - val_acc: 0.7142857142857143 - val_loss: 0.9566171222255829\n",
      "EarlyStopping counter: 47 out of 50\n",
      "epoch: 128 - train_acc: 0.9464285714285714 - train_loss: 0.16979521291455502 - val_acc: 0.7857142857142857 - val_loss: 1.5488390913702041\n",
      "Validation loss decreased (0.431905 --> 0.367762).  Saving model ...\n",
      "epoch: 129 - train_acc: 0.9387755102040817 - train_loss: 0.21863818444418065 - val_acc: 0.9285714285714286 - val_loss: 0.3677622583753695\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 130 - train_acc: 0.9693877551020408 - train_loss: 0.08474282674582806 - val_acc: 0.8571428571428571 - val_loss: 0.6363228733699973\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 131 - train_acc: 0.9719387755102041 - train_loss: 0.06632633479213786 - val_acc: 0.7857142857142857 - val_loss: 0.7888006007555045\n",
      "Validation loss decreased (0.367762 --> 0.355172).  Saving model ...\n",
      "epoch: 132 - train_acc: 0.9923469387755102 - train_loss: 0.02756350739295851 - val_acc: 0.9285714285714286 - val_loss: 0.35517204973625127\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 133 - train_acc: 0.9974489795918368 - train_loss: 0.028617051487132505 - val_acc: 0.8214285714285714 - val_loss: 0.5059425309501743\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 134 - train_acc: 0.9897959183673469 - train_loss: 0.030023596998151752 - val_acc: 0.8214285714285714 - val_loss: 0.6221121924164545\n",
      "EarlyStopping counter: 3 out of 50\n",
      "epoch: 135 - train_acc: 0.9923469387755102 - train_loss: 0.0243261093002556 - val_acc: 0.8571428571428571 - val_loss: 0.596663928541461\n",
      "EarlyStopping counter: 4 out of 50\n",
      "epoch: 136 - train_acc: 0.9974489795918368 - train_loss: 0.030689032456406563 - val_acc: 0.7857142857142857 - val_loss: 0.6375149396620701\n",
      "EarlyStopping counter: 5 out of 50\n",
      "epoch: 137 - train_acc: 0.9948979591836735 - train_loss: 0.022715790359186194 - val_acc: 0.75 - val_loss: 0.7873880565706579\n",
      "EarlyStopping counter: 6 out of 50\n",
      "epoch: 138 - train_acc: 0.9923469387755102 - train_loss: 0.02226578018608951 - val_acc: 0.75 - val_loss: 0.698074551456686\n",
      "EarlyStopping counter: 7 out of 50\n",
      "epoch: 139 - train_acc: 0.9923469387755102 - train_loss: 0.03188556687248351 - val_acc: 0.8571428571428571 - val_loss: 0.7046248128191376\n",
      "EarlyStopping counter: 8 out of 50\n",
      "epoch: 140 - train_acc: 0.9795918367346939 - train_loss: 0.059942856265590046 - val_acc: 0.7857142857142857 - val_loss: 0.8560033736169759\n",
      "EarlyStopping counter: 9 out of 50\n",
      "epoch: 141 - train_acc: 0.9821428571428571 - train_loss: 0.0491670604724155 - val_acc: 0.7857142857142857 - val_loss: 0.9629811883845834\n",
      "EarlyStopping counter: 10 out of 50\n",
      "epoch: 142 - train_acc: 0.9795918367346939 - train_loss: 0.04207793914225488 - val_acc: 0.7857142857142857 - val_loss: 0.8328335034773697\n",
      "EarlyStopping counter: 11 out of 50\n",
      "epoch: 143 - train_acc: 0.9897959183673469 - train_loss: 0.021993296825163558 - val_acc: 0.75 - val_loss: 0.9141677407525812\n",
      "EarlyStopping counter: 12 out of 50\n",
      "epoch: 144 - train_acc: 0.9668367346938775 - train_loss: 0.09181188530059102 - val_acc: 0.8571428571428571 - val_loss: 0.671808258897676\n",
      "EarlyStopping counter: 13 out of 50\n",
      "epoch: 145 - train_acc: 0.9795918367346939 - train_loss: 0.058811407037426114 - val_acc: 0.8214285714285714 - val_loss: 0.6874845333463752\n",
      "EarlyStopping counter: 14 out of 50\n",
      "epoch: 146 - train_acc: 0.9846938775510204 - train_loss: 0.04548125465715198 - val_acc: 0.75 - val_loss: 1.0780200808708547\n",
      "EarlyStopping counter: 15 out of 50\n",
      "epoch: 147 - train_acc: 0.9948979591836735 - train_loss: 0.03395107248068086 - val_acc: 0.7857142857142857 - val_loss: 1.0619608655092616\n",
      "EarlyStopping counter: 16 out of 50\n",
      "epoch: 148 - train_acc: 0.9948979591836735 - train_loss: 0.02522326987087068 - val_acc: 0.8214285714285714 - val_loss: 0.6706559156892691\n",
      "EarlyStopping counter: 17 out of 50\n",
      "epoch: 149 - train_acc: 0.9897959183673469 - train_loss: 0.024105854176393705 - val_acc: 0.7857142857142857 - val_loss: 0.8308835334131368\n",
      "EarlyStopping counter: 18 out of 50\n",
      "epoch: 150 - train_acc: 0.9948979591836735 - train_loss: 0.016163711888812524 - val_acc: 0.7857142857142857 - val_loss: 0.7977416055933562\n",
      "EarlyStopping counter: 19 out of 50\n",
      "epoch: 151 - train_acc: 0.9897959183673469 - train_loss: 0.020368872679956188 - val_acc: 0.8214285714285714 - val_loss: 0.7959305973337805\n",
      "EarlyStopping counter: 20 out of 50\n",
      "epoch: 152 - train_acc: 0.9923469387755102 - train_loss: 0.016647319869722583 - val_acc: 0.75 - val_loss: 1.112161455923742\n",
      "EarlyStopping counter: 21 out of 50\n",
      "epoch: 153 - train_acc: 0.9923469387755102 - train_loss: 0.02324257201351744 - val_acc: 0.75 - val_loss: 1.0601409860100315\n",
      "EarlyStopping counter: 22 out of 50\n",
      "epoch: 154 - train_acc: 1.0 - train_loss: 0.007768702114497484 - val_acc: 0.7857142857142857 - val_loss: 0.9819588767661913\n",
      "EarlyStopping counter: 23 out of 50\n",
      "epoch: 155 - train_acc: 0.9897959183673469 - train_loss: 0.022111749946114843 - val_acc: 0.7857142857142857 - val_loss: 0.9832873525624544\n",
      "EarlyStopping counter: 24 out of 50\n",
      "epoch: 156 - train_acc: 0.9974489795918368 - train_loss: 0.013587134826642614 - val_acc: 0.8571428571428571 - val_loss: 0.9904829526662856\n",
      "EarlyStopping counter: 25 out of 50\n",
      "epoch: 157 - train_acc: 0.9948979591836735 - train_loss: 0.014233660706243831 - val_acc: 0.7857142857142857 - val_loss: 1.5721126992282435\n",
      "EarlyStopping counter: 26 out of 50\n",
      "epoch: 158 - train_acc: 1.0 - train_loss: 0.014574772979179371 - val_acc: 0.75 - val_loss: 1.0530295900491626\n",
      "EarlyStopping counter: 27 out of 50\n",
      "epoch: 159 - train_acc: 1.0 - train_loss: 0.009632803698333352 - val_acc: 0.7857142857142857 - val_loss: 0.9545231065782851\n",
      "EarlyStopping counter: 28 out of 50\n",
      "epoch: 160 - train_acc: 0.9948979591836735 - train_loss: 0.012395719056295816 - val_acc: 0.7857142857142857 - val_loss: 0.8441975122232531\n",
      "EarlyStopping counter: 29 out of 50\n",
      "epoch: 161 - train_acc: 0.9948979591836735 - train_loss: 0.01716368384813144 - val_acc: 0.7857142857142857 - val_loss: 0.8495876701397569\n",
      "EarlyStopping counter: 30 out of 50\n",
      "epoch: 162 - train_acc: 0.9846938775510204 - train_loss: 0.033599156864737675 - val_acc: 0.7857142857142857 - val_loss: 1.092475136749214\n",
      "EarlyStopping counter: 31 out of 50\n",
      "epoch: 163 - train_acc: 0.9872448979591837 - train_loss: 0.03229369858280439 - val_acc: 0.7857142857142857 - val_loss: 0.9564370228371973\n",
      "EarlyStopping counter: 32 out of 50\n",
      "epoch: 164 - train_acc: 0.9872448979591837 - train_loss: 0.026952745399865967 - val_acc: 0.7857142857142857 - val_loss: 0.9742263294481485\n",
      "EarlyStopping counter: 33 out of 50\n",
      "epoch: 165 - train_acc: 0.9974489795918368 - train_loss: 0.01579210787563943 - val_acc: 0.7857142857142857 - val_loss: 1.1859831742314686\n",
      "EarlyStopping counter: 34 out of 50\n",
      "epoch: 166 - train_acc: 0.9974489795918368 - train_loss: 0.02597752285835514 - val_acc: 0.8214285714285714 - val_loss: 0.7090837024123771\n",
      "EarlyStopping counter: 35 out of 50\n",
      "epoch: 167 - train_acc: 0.9872448979591837 - train_loss: 0.054048275272498995 - val_acc: 0.8214285714285714 - val_loss: 0.6858046969135272\n",
      "EarlyStopping counter: 36 out of 50\n",
      "epoch: 168 - train_acc: 0.9821428571428571 - train_loss: 0.0478340743558105 - val_acc: 0.8214285714285714 - val_loss: 0.7820196448521375\n",
      "EarlyStopping counter: 37 out of 50\n",
      "epoch: 169 - train_acc: 0.9974489795918368 - train_loss: 0.026015972240415006 - val_acc: 0.8214285714285714 - val_loss: 0.7734138653960638\n",
      "EarlyStopping counter: 38 out of 50\n",
      "epoch: 170 - train_acc: 0.9795918367346939 - train_loss: 0.04917345645829391 - val_acc: 0.7857142857142857 - val_loss: 1.5490691863180341\n",
      "EarlyStopping counter: 39 out of 50\n",
      "epoch: 171 - train_acc: 0.9948979591836735 - train_loss: 0.016297244258684083 - val_acc: 0.7857142857142857 - val_loss: 0.5725309259896426\n",
      "EarlyStopping counter: 40 out of 50\n",
      "epoch: 172 - train_acc: 0.9872448979591837 - train_loss: 0.02566163284849878 - val_acc: 0.7857142857142857 - val_loss: 1.263352442155189\n",
      "EarlyStopping counter: 41 out of 50\n",
      "epoch: 173 - train_acc: 0.9948979591836735 - train_loss: 0.016241191968315884 - val_acc: 0.8214285714285714 - val_loss: 0.9114752855397049\n",
      "EarlyStopping counter: 42 out of 50\n",
      "epoch: 174 - train_acc: 0.9923469387755102 - train_loss: 0.01683351478886484 - val_acc: 0.7142857142857143 - val_loss: 0.9412204390187844\n",
      "EarlyStopping counter: 43 out of 50\n",
      "epoch: 175 - train_acc: 1.0 - train_loss: 0.007464623423663371 - val_acc: 0.7142857142857143 - val_loss: 0.9271926474768475\n",
      "EarlyStopping counter: 44 out of 50\n",
      "epoch: 176 - train_acc: 1.0 - train_loss: 0.008353156740559881 - val_acc: 0.7142857142857143 - val_loss: 1.1319089794943613\n",
      "EarlyStopping counter: 45 out of 50\n",
      "epoch: 177 - train_acc: 0.9974489795918368 - train_loss: 0.024079895572395812 - val_acc: 0.8214285714285714 - val_loss: 0.9694403335730577\n",
      "EarlyStopping counter: 46 out of 50\n",
      "epoch: 178 - train_acc: 0.9948979591836735 - train_loss: 0.02911352850733486 - val_acc: 0.8571428571428571 - val_loss: 0.7125795055216428\n",
      "EarlyStopping counter: 47 out of 50\n",
      "epoch: 179 - train_acc: 0.9923469387755102 - train_loss: 0.024822768809574215 - val_acc: 0.8928571428571429 - val_loss: 0.5880730250120229\n",
      "EarlyStopping counter: 48 out of 50\n",
      "epoch: 180 - train_acc: 0.9923469387755102 - train_loss: 0.02103722905450416 - val_acc: 0.7857142857142857 - val_loss: 0.6988622595058482\n",
      "EarlyStopping counter: 49 out of 50\n",
      "epoch: 181 - train_acc: 0.9948979591836735 - train_loss: 0.028038140478256597 - val_acc: 0.8214285714285714 - val_loss: 0.467838966682607\n",
      "EarlyStopping counter: 50 out of 50\n",
      "epoch: 182 - train_acc: 0.9846938775510204 - train_loss: 0.044250512829314526 - val_acc: 0.7857142857142857 - val_loss: 0.9776907516772464\n",
      "=> Early stopped !!\n",
      "[total]\n",
      "Accuracy: 0.7545454545454545\n",
      "Precision: 0.7833333333333333\n",
      "Recall: 0.7704918032786885\n",
      "F1: 0.7768595041322315\n",
      "[low]\n",
      "Accuracy: 0.7552083333333334\n",
      "Precision: 0.7843137254901961\n",
      "Recall: 0.7619047619047619\n",
      "F1: 0.7729468599033816\n",
      "[high]\n",
      "Accuracy: 0.7\n",
      "Precision: 0.6363636363636364\n",
      "Recall: 0.7777777777777778\n",
      "F1: 0.7000000000000001\n",
      "\n",
      "[Fold 8]: \n",
      "len(train_sampler)=392\n",
      "len(val_sampler)=28\n",
      "[ 11  38 112 117 129 179 183 185 233 234 255 258 274 275 285 286 287 299\n",
      " 304 307 320 327 355 360 387 395 398 404]\n",
      "Validation loss decreased (inf --> 0.748940).  Saving model ...\n",
      "epoch: 0 - train_acc: 0.5714285714285714 - train_loss: 0.6774303426601449 - val_acc: 0.35714285714285715 - val_loss: 0.7489395227638671\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 1 - train_acc: 0.6658163265306123 - train_loss: 0.6045477195763763 - val_acc: 0.35714285714285715 - val_loss: 0.810255429495228\n",
      "Validation loss decreased (0.748940 --> 0.579701).  Saving model ...\n",
      "epoch: 2 - train_acc: 0.6989795918367347 - train_loss: 0.5793618812145447 - val_acc: 0.6785714285714286 - val_loss: 0.5797011833024495\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 3 - train_acc: 0.7142857142857143 - train_loss: 0.5818622730549651 - val_acc: 0.6071428571428571 - val_loss: 0.7907060717259478\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 4 - train_acc: 0.7602040816326531 - train_loss: 0.5214445805089424 - val_acc: 0.5714285714285714 - val_loss: 0.6868574247113315\n",
      "Validation loss decreased (0.579701 --> 0.453758).  Saving model ...\n",
      "epoch: 5 - train_acc: 0.7806122448979592 - train_loss: 0.4758154107180885 - val_acc: 0.9285714285714286 - val_loss: 0.4537584684151232\n",
      "Validation loss decreased (0.453758 --> 0.426389).  Saving model ...\n",
      "epoch: 6 - train_acc: 0.7755102040816326 - train_loss: 0.47750040957801787 - val_acc: 0.75 - val_loss: 0.42638869959583714\n",
      "Validation loss decreased (0.426389 --> 0.371899).  Saving model ...\n",
      "epoch: 7 - train_acc: 0.8163265306122449 - train_loss: 0.4442487426809598 - val_acc: 0.7857142857142857 - val_loss: 0.37189910913620405\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 8 - train_acc: 0.8137755102040817 - train_loss: 0.421784399941684 - val_acc: 0.7857142857142857 - val_loss: 0.5292168632428611\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 9 - train_acc: 0.8367346938775511 - train_loss: 0.39307005576437903 - val_acc: 0.6785714285714286 - val_loss: 0.8253501775428387\n",
      "EarlyStopping counter: 3 out of 50\n",
      "epoch: 10 - train_acc: 0.8494897959183674 - train_loss: 0.3766893630375502 - val_acc: 0.75 - val_loss: 0.6178107267651075\n",
      "EarlyStopping counter: 4 out of 50\n",
      "epoch: 11 - train_acc: 0.8443877551020408 - train_loss: 0.38165554193600676 - val_acc: 0.8571428571428571 - val_loss: 0.4519749637738837\n",
      "EarlyStopping counter: 5 out of 50\n",
      "epoch: 12 - train_acc: 0.8545918367346939 - train_loss: 0.3485911190138099 - val_acc: 0.7857142857142857 - val_loss: 0.7343496216235311\n",
      "EarlyStopping counter: 6 out of 50\n",
      "epoch: 13 - train_acc: 0.8367346938775511 - train_loss: 0.35851334368737625 - val_acc: 0.6785714285714286 - val_loss: 0.7128227457068947\n",
      "EarlyStopping counter: 7 out of 50\n",
      "epoch: 14 - train_acc: 0.8775510204081632 - train_loss: 0.303478790357193 - val_acc: 0.8571428571428571 - val_loss: 0.6131005421136835\n",
      "EarlyStopping counter: 8 out of 50\n",
      "epoch: 15 - train_acc: 0.875 - train_loss: 0.2996571926130994 - val_acc: 0.6428571428571429 - val_loss: 1.4753292810595164\n",
      "EarlyStopping counter: 9 out of 50\n",
      "epoch: 16 - train_acc: 0.8494897959183674 - train_loss: 0.3796697291117478 - val_acc: 0.6785714285714286 - val_loss: 0.8701052118724056\n",
      "EarlyStopping counter: 10 out of 50\n",
      "epoch: 17 - train_acc: 0.8341836734693877 - train_loss: 0.37281981277666437 - val_acc: 0.75 - val_loss: 0.48865730787872247\n",
      "EarlyStopping counter: 11 out of 50\n",
      "epoch: 18 - train_acc: 0.8596938775510204 - train_loss: 0.32134380335795026 - val_acc: 0.7857142857142857 - val_loss: 0.4606423451562315\n",
      "EarlyStopping counter: 12 out of 50\n",
      "epoch: 19 - train_acc: 0.875 - train_loss: 0.32305512027509675 - val_acc: 0.7857142857142857 - val_loss: 0.4540972916430249\n",
      "EarlyStopping counter: 13 out of 50\n",
      "epoch: 20 - train_acc: 0.8928571428571429 - train_loss: 0.25614605134351215 - val_acc: 0.8214285714285714 - val_loss: 0.983766698659773\n",
      "EarlyStopping counter: 14 out of 50\n",
      "epoch: 21 - train_acc: 0.9158163265306123 - train_loss: 0.2503343284387964 - val_acc: 0.8571428571428571 - val_loss: 0.650814188667271\n",
      "EarlyStopping counter: 15 out of 50\n",
      "epoch: 22 - train_acc: 0.8877551020408163 - train_loss: 0.2819823538497611 - val_acc: 0.8571428571428571 - val_loss: 0.6441445488316859\n",
      "EarlyStopping counter: 16 out of 50\n",
      "epoch: 23 - train_acc: 0.9260204081632653 - train_loss: 0.21753741478827937 - val_acc: 0.8214285714285714 - val_loss: 1.2446677535508608\n",
      "EarlyStopping counter: 17 out of 50\n",
      "epoch: 24 - train_acc: 0.9489795918367347 - train_loss: 0.18314938259543045 - val_acc: 0.8571428571428571 - val_loss: 0.8879877505090273\n",
      "EarlyStopping counter: 18 out of 50\n",
      "epoch: 25 - train_acc: 0.9107142857142857 - train_loss: 0.21889440807454266 - val_acc: 0.8214285714285714 - val_loss: 0.9774649099055328\n",
      "EarlyStopping counter: 19 out of 50\n",
      "epoch: 26 - train_acc: 0.9209183673469388 - train_loss: 0.20830065734469538 - val_acc: 0.7857142857142857 - val_loss: 0.5617199807102062\n",
      "EarlyStopping counter: 20 out of 50\n",
      "epoch: 27 - train_acc: 0.9362244897959183 - train_loss: 0.17665449527423407 - val_acc: 0.8214285714285714 - val_loss: 1.1075134962205528\n",
      "EarlyStopping counter: 21 out of 50\n",
      "epoch: 28 - train_acc: 0.9413265306122449 - train_loss: 0.1799564580455731 - val_acc: 0.75 - val_loss: 0.8707665473481071\n",
      "EarlyStopping counter: 22 out of 50\n",
      "epoch: 29 - train_acc: 0.923469387755102 - train_loss: 0.19793673231287454 - val_acc: 0.8571428571428571 - val_loss: 1.0859908700334509\n",
      "EarlyStopping counter: 23 out of 50\n",
      "epoch: 30 - train_acc: 0.9438775510204082 - train_loss: 0.1639656321633226 - val_acc: 0.6785714285714286 - val_loss: 2.271909345527808\n",
      "EarlyStopping counter: 24 out of 50\n",
      "epoch: 31 - train_acc: 0.9056122448979592 - train_loss: 0.19853752729008273 - val_acc: 0.7857142857142857 - val_loss: 1.3203297100944278\n",
      "EarlyStopping counter: 25 out of 50\n",
      "epoch: 32 - train_acc: 0.923469387755102 - train_loss: 0.17703306742279945 - val_acc: 0.7857142857142857 - val_loss: 0.8877817510676291\n",
      "EarlyStopping counter: 26 out of 50\n",
      "epoch: 33 - train_acc: 0.9209183673469388 - train_loss: 0.21551042460244071 - val_acc: 0.8571428571428571 - val_loss: 0.6307524453021603\n",
      "EarlyStopping counter: 27 out of 50\n",
      "epoch: 34 - train_acc: 0.9005102040816326 - train_loss: 0.22669106565322847 - val_acc: 0.8214285714285714 - val_loss: 0.6225641812870594\n",
      "EarlyStopping counter: 28 out of 50\n",
      "epoch: 35 - train_acc: 0.9642857142857143 - train_loss: 0.12568396305011875 - val_acc: 0.7857142857142857 - val_loss: 0.8834677301710647\n",
      "EarlyStopping counter: 29 out of 50\n",
      "epoch: 36 - train_acc: 0.9693877551020408 - train_loss: 0.09864360649060372 - val_acc: 0.75 - val_loss: 0.7547029675487256\n",
      "EarlyStopping counter: 30 out of 50\n",
      "epoch: 37 - train_acc: 0.9693877551020408 - train_loss: 0.0920877283925568 - val_acc: 0.7857142857142857 - val_loss: 1.0703983537034218\n",
      "EarlyStopping counter: 31 out of 50\n",
      "epoch: 38 - train_acc: 0.9795918367346939 - train_loss: 0.07827108912319726 - val_acc: 0.75 - val_loss: 1.2906409632592588\n",
      "EarlyStopping counter: 32 out of 50\n",
      "epoch: 39 - train_acc: 0.9540816326530612 - train_loss: 0.14708015786959425 - val_acc: 0.8214285714285714 - val_loss: 1.8303733852562427\n",
      "EarlyStopping counter: 33 out of 50\n",
      "epoch: 40 - train_acc: 0.9132653061224489 - train_loss: 0.2007044842752629 - val_acc: 0.7142857142857143 - val_loss: 0.5321287442171814\n",
      "EarlyStopping counter: 34 out of 50\n",
      "epoch: 41 - train_acc: 0.9464285714285714 - train_loss: 0.16175795152952613 - val_acc: 0.8214285714285714 - val_loss: 1.258193905751003\n",
      "EarlyStopping counter: 35 out of 50\n",
      "epoch: 42 - train_acc: 0.951530612244898 - train_loss: 0.1385922137393346 - val_acc: 0.8214285714285714 - val_loss: 1.3368462472850955\n",
      "EarlyStopping counter: 36 out of 50\n",
      "epoch: 43 - train_acc: 0.9693877551020408 - train_loss: 0.09726710709587467 - val_acc: 0.8214285714285714 - val_loss: 1.3802415239811858\n",
      "EarlyStopping counter: 37 out of 50\n",
      "epoch: 44 - train_acc: 0.9719387755102041 - train_loss: 0.11287964632583149 - val_acc: 0.7142857142857143 - val_loss: 1.1857774448654037\n",
      "EarlyStopping counter: 38 out of 50\n",
      "epoch: 45 - train_acc: 0.9642857142857143 - train_loss: 0.10701164136490153 - val_acc: 0.75 - val_loss: 1.7035187305578083\n",
      "EarlyStopping counter: 39 out of 50\n",
      "epoch: 46 - train_acc: 0.9770408163265306 - train_loss: 0.07820165036265703 - val_acc: 0.8214285714285714 - val_loss: 1.1628491302803812\n",
      "EarlyStopping counter: 40 out of 50\n",
      "epoch: 47 - train_acc: 0.9668367346938775 - train_loss: 0.082981835199693 - val_acc: 0.7857142857142857 - val_loss: 2.2766603777797862\n",
      "EarlyStopping counter: 41 out of 50\n",
      "epoch: 48 - train_acc: 0.9489795918367347 - train_loss: 0.1322221805020212 - val_acc: 0.8214285714285714 - val_loss: 1.0702231394092516\n",
      "EarlyStopping counter: 42 out of 50\n",
      "epoch: 49 - train_acc: 0.9566326530612245 - train_loss: 0.13600026484303068 - val_acc: 0.6785714285714286 - val_loss: 2.1412740335175826\n",
      "EarlyStopping counter: 43 out of 50\n",
      "epoch: 50 - train_acc: 0.9642857142857143 - train_loss: 0.10031918744427129 - val_acc: 0.7857142857142857 - val_loss: 1.530407079612507\n",
      "EarlyStopping counter: 44 out of 50\n",
      "epoch: 51 - train_acc: 0.9668367346938775 - train_loss: 0.10087669654221539 - val_acc: 0.75 - val_loss: 0.7481936711025883\n",
      "EarlyStopping counter: 45 out of 50\n",
      "epoch: 52 - train_acc: 0.9744897959183674 - train_loss: 0.10235675733484643 - val_acc: 0.6785714285714286 - val_loss: 1.785093246119542\n",
      "EarlyStopping counter: 46 out of 50\n",
      "epoch: 53 - train_acc: 0.9566326530612245 - train_loss: 0.1033020683834934 - val_acc: 0.7857142857142857 - val_loss: 1.6342450559056887\n",
      "EarlyStopping counter: 47 out of 50\n",
      "epoch: 54 - train_acc: 0.9617346938775511 - train_loss: 0.08918631114546464 - val_acc: 0.75 - val_loss: 1.3314961084376034\n",
      "EarlyStopping counter: 48 out of 50\n",
      "epoch: 55 - train_acc: 0.9770408163265306 - train_loss: 0.05909806457825907 - val_acc: 0.8571428571428571 - val_loss: 1.4512451617614495\n",
      "EarlyStopping counter: 49 out of 50\n",
      "epoch: 56 - train_acc: 0.9591836734693877 - train_loss: 0.12861521651747246 - val_acc: 0.75 - val_loss: 2.1091852710049115\n",
      "EarlyStopping counter: 50 out of 50\n",
      "epoch: 57 - train_acc: 0.9489795918367347 - train_loss: 0.13440845888669156 - val_acc: 0.75 - val_loss: 1.6278633272266716\n",
      "=> Early stopped !!\n",
      "[total]\n",
      "Accuracy: 0.6272727272727273\n",
      "Precision: 0.7325581395348837\n",
      "Recall: 0.5163934426229508\n",
      "F1: 0.6057692307692308\n",
      "[low]\n",
      "Accuracy: 0.625\n",
      "Precision: 0.7391304347826086\n",
      "Recall: 0.4857142857142857\n",
      "F1: 0.5862068965517241\n",
      "[high]\n",
      "Accuracy: 0.55\n",
      "Precision: 0.5\n",
      "Recall: 0.5555555555555556\n",
      "F1: 0.5263157894736842\n",
      "\n",
      "[Fold 9]: \n",
      "len(train_sampler)=392\n",
      "len(val_sampler)=28\n",
      "[ 12  28  35  44  65  85 107 115 120 127 133 136 159 164 186 197 224 232\n",
      " 242 261 283 314 340 364 381 386 401 418]\n",
      "Validation loss decreased (inf --> 0.661767).  Saving model ...\n",
      "epoch: 0 - train_acc: 0.5714285714285714 - train_loss: 0.6803851764699781 - val_acc: 0.6428571428571429 - val_loss: 0.6617665933648582\n",
      "Validation loss decreased (0.661767 --> 0.636912).  Saving model ...\n",
      "epoch: 1 - train_acc: 0.6785714285714286 - train_loss: 0.6250605797654436 - val_acc: 0.6428571428571429 - val_loss: 0.6369119031539646\n",
      "Validation loss decreased (0.636912 --> 0.535289).  Saving model ...\n",
      "epoch: 2 - train_acc: 0.6964285714285714 - train_loss: 0.5997885900682972 - val_acc: 0.6785714285714286 - val_loss: 0.5352891873073304\n",
      "Validation loss decreased (0.535289 --> 0.468821).  Saving model ...\n",
      "epoch: 3 - train_acc: 0.6836734693877551 - train_loss: 0.5878674598863489 - val_acc: 0.75 - val_loss: 0.4688206009569234\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 4 - train_acc: 0.7270408163265306 - train_loss: 0.5613948252974198 - val_acc: 0.75 - val_loss: 0.5563091894376759\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 5 - train_acc: 0.7755102040816326 - train_loss: 0.5156618643137441 - val_acc: 0.5714285714285714 - val_loss: 0.8060934357930112\n",
      "Validation loss decreased (0.468821 --> 0.365706).  Saving model ...\n",
      "epoch: 6 - train_acc: 0.8137755102040817 - train_loss: 0.46712721192113654 - val_acc: 0.8571428571428571 - val_loss: 0.36570637351692925\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 7 - train_acc: 0.7346938775510204 - train_loss: 0.5480962574885572 - val_acc: 0.8928571428571429 - val_loss: 0.3714534071866149\n",
      "Validation loss decreased (0.365706 --> 0.321959).  Saving model ...\n",
      "epoch: 8 - train_acc: 0.7857142857142857 - train_loss: 0.47209132951582694 - val_acc: 0.8928571428571429 - val_loss: 0.3219587400477066\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 9 - train_acc: 0.7959183673469388 - train_loss: 0.47294106883898135 - val_acc: 0.8928571428571429 - val_loss: 0.34761498920341727\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 10 - train_acc: 0.8086734693877551 - train_loss: 0.4397087683189468 - val_acc: 0.7857142857142857 - val_loss: 0.5188413443962034\n",
      "EarlyStopping counter: 3 out of 50\n",
      "epoch: 11 - train_acc: 0.7806122448979592 - train_loss: 0.48132357450458346 - val_acc: 0.8214285714285714 - val_loss: 0.3854689090702005\n",
      "Validation loss decreased (0.321959 --> 0.265646).  Saving model ...\n",
      "epoch: 12 - train_acc: 0.8035714285714286 - train_loss: 0.4356256013072706 - val_acc: 0.8928571428571429 - val_loss: 0.2656464120302627\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 13 - train_acc: 0.8418367346938775 - train_loss: 0.38964451197329125 - val_acc: 0.75 - val_loss: 0.4257057526112007\n",
      "Validation loss decreased (0.265646 --> 0.227043).  Saving model ...\n",
      "epoch: 14 - train_acc: 0.8214285714285714 - train_loss: 0.3824359316237456 - val_acc: 0.8928571428571429 - val_loss: 0.22704309178847765\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 15 - train_acc: 0.8494897959183674 - train_loss: 0.3195693746434019 - val_acc: 0.7857142857142857 - val_loss: 0.37388927871736805\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 16 - train_acc: 0.8698979591836735 - train_loss: 0.30473377492373027 - val_acc: 0.8571428571428571 - val_loss: 0.2892539969437865\n",
      "Validation loss decreased (0.227043 --> 0.208200).  Saving model ...\n",
      "epoch: 17 - train_acc: 0.8392857142857143 - train_loss: 0.38821462443744814 - val_acc: 0.9285714285714286 - val_loss: 0.20820015304965975\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 18 - train_acc: 0.8698979591836735 - train_loss: 0.3042728783735979 - val_acc: 0.8571428571428571 - val_loss: 0.3470975553072335\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 19 - train_acc: 0.8622448979591837 - train_loss: 0.32785513622332474 - val_acc: 0.8571428571428571 - val_loss: 0.3319285255378176\n",
      "EarlyStopping counter: 3 out of 50\n",
      "epoch: 20 - train_acc: 0.8724489795918368 - train_loss: 0.3146404014923897 - val_acc: 0.8571428571428571 - val_loss: 0.3172140014102703\n",
      "EarlyStopping counter: 4 out of 50\n",
      "epoch: 21 - train_acc: 0.8596938775510204 - train_loss: 0.3241270703466757 - val_acc: 0.8571428571428571 - val_loss: 0.28098283835455506\n",
      "EarlyStopping counter: 5 out of 50\n",
      "epoch: 22 - train_acc: 0.8928571428571429 - train_loss: 0.2666596645092846 - val_acc: 0.8214285714285714 - val_loss: 0.3574472091485337\n",
      "EarlyStopping counter: 6 out of 50\n",
      "epoch: 23 - train_acc: 0.9030612244897959 - train_loss: 0.2509366035978159 - val_acc: 0.7857142857142857 - val_loss: 0.3061316126186332\n",
      "EarlyStopping counter: 7 out of 50\n",
      "epoch: 24 - train_acc: 0.8903061224489796 - train_loss: 0.22137796043963476 - val_acc: 0.8928571428571429 - val_loss: 0.22548735165563705\n",
      "EarlyStopping counter: 8 out of 50\n",
      "epoch: 25 - train_acc: 0.9260204081632653 - train_loss: 0.19375204081773947 - val_acc: 0.8571428571428571 - val_loss: 0.2467572452490988\n",
      "EarlyStopping counter: 9 out of 50\n",
      "epoch: 26 - train_acc: 0.9209183673469388 - train_loss: 0.22952217749107143 - val_acc: 0.9285714285714286 - val_loss: 0.2665086603958954\n",
      "EarlyStopping counter: 10 out of 50\n",
      "epoch: 27 - train_acc: 0.9056122448979592 - train_loss: 0.24404056756465423 - val_acc: 0.8214285714285714 - val_loss: 0.37687070364217856\n",
      "EarlyStopping counter: 11 out of 50\n",
      "epoch: 28 - train_acc: 0.8979591836734694 - train_loss: 0.264391821410238 - val_acc: 0.9285714285714286 - val_loss: 0.20890799851402597\n",
      "EarlyStopping counter: 12 out of 50\n",
      "epoch: 29 - train_acc: 0.9158163265306123 - train_loss: 0.22030668562728864 - val_acc: 0.8571428571428571 - val_loss: 0.24519319167129539\n",
      "EarlyStopping counter: 13 out of 50\n",
      "epoch: 30 - train_acc: 0.9209183673469388 - train_loss: 0.2172975399931727 - val_acc: 0.8928571428571429 - val_loss: 0.2486327260855786\n",
      "Validation loss decreased (0.208200 --> 0.138529).  Saving model ...\n",
      "epoch: 31 - train_acc: 0.9438775510204082 - train_loss: 0.15412946553516615 - val_acc: 0.9285714285714286 - val_loss: 0.13852875162205508\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 32 - train_acc: 0.9489795918367347 - train_loss: 0.18150428254316725 - val_acc: 0.8928571428571429 - val_loss: 0.24729883445460105\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 33 - train_acc: 0.9438775510204082 - train_loss: 0.1787688198120343 - val_acc: 0.8571428571428571 - val_loss: 0.36958410054871665\n",
      "EarlyStopping counter: 3 out of 50\n",
      "epoch: 34 - train_acc: 0.951530612244898 - train_loss: 0.15124926691678922 - val_acc: 0.8928571428571429 - val_loss: 0.30466951031334155\n",
      "EarlyStopping counter: 4 out of 50\n",
      "epoch: 35 - train_acc: 0.9413265306122449 - train_loss: 0.14298716632017427 - val_acc: 0.8571428571428571 - val_loss: 0.3722983929290778\n",
      "EarlyStopping counter: 5 out of 50\n",
      "epoch: 36 - train_acc: 0.9438775510204082 - train_loss: 0.14061832774289645 - val_acc: 0.8214285714285714 - val_loss: 0.31502272450213953\n",
      "EarlyStopping counter: 6 out of 50\n",
      "epoch: 37 - train_acc: 0.9540816326530612 - train_loss: 0.1487204001963618 - val_acc: 0.7142857142857143 - val_loss: 0.37286806686118984\n",
      "EarlyStopping counter: 7 out of 50\n",
      "epoch: 38 - train_acc: 0.9183673469387755 - train_loss: 0.16597190648008572 - val_acc: 0.8214285714285714 - val_loss: 0.3321405509045163\n",
      "EarlyStopping counter: 8 out of 50\n",
      "epoch: 39 - train_acc: 0.9413265306122449 - train_loss: 0.15553888137097518 - val_acc: 0.9285714285714286 - val_loss: 0.23871714482901008\n",
      "EarlyStopping counter: 9 out of 50\n",
      "epoch: 40 - train_acc: 0.951530612244898 - train_loss: 0.1305370095767149 - val_acc: 0.9285714285714286 - val_loss: 0.26537522791810786\n",
      "EarlyStopping counter: 10 out of 50\n",
      "epoch: 41 - train_acc: 0.9540816326530612 - train_loss: 0.1104610154355006 - val_acc: 0.8571428571428571 - val_loss: 0.35763478297093065\n",
      "EarlyStopping counter: 11 out of 50\n",
      "epoch: 42 - train_acc: 0.951530612244898 - train_loss: 0.1368113051865992 - val_acc: 0.8214285714285714 - val_loss: 0.34271503420364613\n",
      "EarlyStopping counter: 12 out of 50\n",
      "epoch: 43 - train_acc: 0.9464285714285714 - train_loss: 0.13558534998599617 - val_acc: 0.8214285714285714 - val_loss: 0.33131475473055816\n",
      "EarlyStopping counter: 13 out of 50\n",
      "epoch: 44 - train_acc: 0.9540816326530612 - train_loss: 0.12323725912163365 - val_acc: 0.8571428571428571 - val_loss: 0.25178472040851524\n",
      "EarlyStopping counter: 14 out of 50\n",
      "epoch: 45 - train_acc: 0.9617346938775511 - train_loss: 0.10470328173025713 - val_acc: 0.8571428571428571 - val_loss: 0.3103986057454624\n",
      "EarlyStopping counter: 15 out of 50\n",
      "epoch: 46 - train_acc: 0.9668367346938775 - train_loss: 0.09571461096258378 - val_acc: 0.8214285714285714 - val_loss: 0.28268085252566433\n",
      "EarlyStopping counter: 16 out of 50\n",
      "epoch: 47 - train_acc: 0.9719387755102041 - train_loss: 0.08735352073631264 - val_acc: 0.8928571428571429 - val_loss: 0.1795736696174282\n",
      "EarlyStopping counter: 17 out of 50\n",
      "epoch: 48 - train_acc: 0.9617346938775511 - train_loss: 0.10058421996046167 - val_acc: 0.8928571428571429 - val_loss: 0.2222619691103911\n",
      "EarlyStopping counter: 18 out of 50\n",
      "epoch: 49 - train_acc: 0.9770408163265306 - train_loss: 0.07958245530469368 - val_acc: 0.8571428571428571 - val_loss: 0.2755640976702336\n",
      "EarlyStopping counter: 19 out of 50\n",
      "epoch: 50 - train_acc: 0.9617346938775511 - train_loss: 0.12801024442622025 - val_acc: 0.7142857142857143 - val_loss: 0.462182865076494\n",
      "EarlyStopping counter: 20 out of 50\n",
      "epoch: 51 - train_acc: 0.9719387755102041 - train_loss: 0.073071056315829 - val_acc: 0.8928571428571429 - val_loss: 0.3151201039643574\n",
      "EarlyStopping counter: 21 out of 50\n",
      "epoch: 52 - train_acc: 0.9540816326530612 - train_loss: 0.12557595500043245 - val_acc: 0.7857142857142857 - val_loss: 0.5235543904741548\n",
      "EarlyStopping counter: 22 out of 50\n",
      "epoch: 53 - train_acc: 0.9642857142857143 - train_loss: 0.10638455520504479 - val_acc: 0.8928571428571429 - val_loss: 0.4970367742213938\n",
      "EarlyStopping counter: 23 out of 50\n",
      "epoch: 54 - train_acc: 0.9795918367346939 - train_loss: 0.060524940017657694 - val_acc: 0.75 - val_loss: 0.4315635810103036\n",
      "EarlyStopping counter: 24 out of 50\n",
      "epoch: 55 - train_acc: 0.9846938775510204 - train_loss: 0.04880133059598752 - val_acc: 0.7857142857142857 - val_loss: 0.5386337555145527\n",
      "EarlyStopping counter: 25 out of 50\n",
      "epoch: 56 - train_acc: 0.9821428571428571 - train_loss: 0.058852415864194 - val_acc: 0.8571428571428571 - val_loss: 0.2885773849483848\n",
      "EarlyStopping counter: 26 out of 50\n",
      "epoch: 57 - train_acc: 0.9693877551020408 - train_loss: 0.09194814683563685 - val_acc: 0.8571428571428571 - val_loss: 0.2555153592685426\n",
      "EarlyStopping counter: 27 out of 50\n",
      "epoch: 58 - train_acc: 0.9923469387755102 - train_loss: 0.059460260861946876 - val_acc: 0.8571428571428571 - val_loss: 0.308525230238019\n",
      "EarlyStopping counter: 28 out of 50\n",
      "epoch: 59 - train_acc: 0.9719387755102041 - train_loss: 0.09632628304455586 - val_acc: 0.75 - val_loss: 0.4479646480061745\n",
      "EarlyStopping counter: 29 out of 50\n",
      "epoch: 60 - train_acc: 0.9668367346938775 - train_loss: 0.08857760620695485 - val_acc: 0.8571428571428571 - val_loss: 0.44717710584277914\n",
      "EarlyStopping counter: 30 out of 50\n",
      "epoch: 61 - train_acc: 0.9770408163265306 - train_loss: 0.07456255312319228 - val_acc: 0.8214285714285714 - val_loss: 0.325873057798539\n",
      "EarlyStopping counter: 31 out of 50\n",
      "epoch: 62 - train_acc: 0.9566326530612245 - train_loss: 0.13083948703578174 - val_acc: 0.8214285714285714 - val_loss: 0.5244948939701928\n",
      "EarlyStopping counter: 32 out of 50\n",
      "epoch: 63 - train_acc: 0.9566326530612245 - train_loss: 0.1574989881410711 - val_acc: 0.8928571428571429 - val_loss: 0.17521433446972962\n",
      "EarlyStopping counter: 33 out of 50\n",
      "epoch: 64 - train_acc: 0.9489795918367347 - train_loss: 0.14413605131757243 - val_acc: 0.9285714285714286 - val_loss: 0.1412195159760337\n",
      "EarlyStopping counter: 34 out of 50\n",
      "epoch: 65 - train_acc: 0.9770408163265306 - train_loss: 0.07862027075820059 - val_acc: 0.8571428571428571 - val_loss: 0.39574849171415805\n",
      "EarlyStopping counter: 35 out of 50\n",
      "epoch: 66 - train_acc: 0.9846938775510204 - train_loss: 0.05426987354230215 - val_acc: 0.8214285714285714 - val_loss: 0.33067705054800867\n",
      "EarlyStopping counter: 36 out of 50\n",
      "epoch: 67 - train_acc: 0.9693877551020408 - train_loss: 0.08629144329138042 - val_acc: 0.8571428571428571 - val_loss: 0.3515555395553993\n",
      "EarlyStopping counter: 37 out of 50\n",
      "epoch: 68 - train_acc: 0.9821428571428571 - train_loss: 0.07462047029472549 - val_acc: 0.9285714285714286 - val_loss: 0.26226903855206923\n",
      "EarlyStopping counter: 38 out of 50\n",
      "epoch: 69 - train_acc: 0.9770408163265306 - train_loss: 0.07079784337445012 - val_acc: 0.8571428571428571 - val_loss: 0.2739124419237191\n",
      "EarlyStopping counter: 39 out of 50\n",
      "epoch: 70 - train_acc: 0.9770408163265306 - train_loss: 0.0807586909418736 - val_acc: 0.8214285714285714 - val_loss: 0.3376336797726114\n",
      "EarlyStopping counter: 40 out of 50\n",
      "epoch: 71 - train_acc: 0.9642857142857143 - train_loss: 0.08582297869198992 - val_acc: 0.7857142857142857 - val_loss: 0.31836915260725684\n",
      "EarlyStopping counter: 41 out of 50\n",
      "epoch: 72 - train_acc: 0.9846938775510204 - train_loss: 0.048493699180698666 - val_acc: 0.7857142857142857 - val_loss: 0.4851200171310695\n",
      "EarlyStopping counter: 42 out of 50\n",
      "epoch: 73 - train_acc: 0.9948979591836735 - train_loss: 0.0476175971504264 - val_acc: 0.8214285714285714 - val_loss: 0.4386641445693169\n",
      "EarlyStopping counter: 43 out of 50\n",
      "epoch: 74 - train_acc: 0.9795918367346939 - train_loss: 0.06358700969748671 - val_acc: 0.7857142857142857 - val_loss: 0.6896592199650984\n",
      "EarlyStopping counter: 44 out of 50\n",
      "epoch: 75 - train_acc: 0.9846938775510204 - train_loss: 0.05830275170190031 - val_acc: 0.7857142857142857 - val_loss: 0.33115186167541166\n",
      "EarlyStopping counter: 45 out of 50\n",
      "epoch: 76 - train_acc: 0.9693877551020408 - train_loss: 0.0647048160629816 - val_acc: 0.8571428571428571 - val_loss: 0.3224784057171237\n",
      "EarlyStopping counter: 46 out of 50\n",
      "epoch: 77 - train_acc: 0.9770408163265306 - train_loss: 0.05424415351092934 - val_acc: 0.8571428571428571 - val_loss: 0.2958259486310882\n",
      "EarlyStopping counter: 47 out of 50\n",
      "epoch: 78 - train_acc: 0.9744897959183674 - train_loss: 0.0668490835353241 - val_acc: 0.8571428571428571 - val_loss: 0.3889611632196984\n",
      "EarlyStopping counter: 48 out of 50\n",
      "epoch: 79 - train_acc: 0.9795918367346939 - train_loss: 0.05404858694205119 - val_acc: 0.8214285714285714 - val_loss: 0.6151709719052117\n",
      "EarlyStopping counter: 49 out of 50\n",
      "epoch: 80 - train_acc: 0.9821428571428571 - train_loss: 0.06898105656440129 - val_acc: 0.75 - val_loss: 0.5193126462293643\n",
      "EarlyStopping counter: 50 out of 50\n",
      "epoch: 81 - train_acc: 0.9744897959183674 - train_loss: 0.06176657334497121 - val_acc: 0.8214285714285714 - val_loss: 0.4743498837183082\n",
      "=> Early stopped !!\n",
      "[total]\n",
      "Accuracy: 0.7045454545454546\n",
      "Precision: 0.6938775510204082\n",
      "Recall: 0.8360655737704918\n",
      "F1: 0.758364312267658\n",
      "[low]\n",
      "Accuracy: 0.6822916666666666\n",
      "Precision: 0.671875\n",
      "Recall: 0.819047619047619\n",
      "F1: 0.7381974248927039\n",
      "[high]\n",
      "Accuracy: 0.8\n",
      "Precision: 0.7272727272727273\n",
      "Recall: 0.8888888888888888\n",
      "F1: 0.8\n",
      "\n",
      "[Fold 10]: \n",
      "len(train_sampler)=392\n",
      "len(val_sampler)=28\n",
      "[  4  41  51  95  98 100 142 170 171 178 206 213 215 221 226 240 254 256\n",
      " 282 292 316 318 322 336 349 376 405 407]\n",
      "Validation loss decreased (inf --> 0.772440).  Saving model ...\n",
      "epoch: 0 - train_acc: 0.5714285714285714 - train_loss: 0.6761582793881241 - val_acc: 0.5357142857142857 - val_loss: 0.772440220654883\n",
      "Validation loss decreased (0.772440 --> 0.673183).  Saving model ...\n",
      "epoch: 1 - train_acc: 0.6530612244897959 - train_loss: 0.6123330457032299 - val_acc: 0.6785714285714286 - val_loss: 0.6731832472428829\n",
      "Validation loss decreased (0.673183 --> 0.607573).  Saving model ...\n",
      "epoch: 2 - train_acc: 0.7397959183673469 - train_loss: 0.5629256945502205 - val_acc: 0.6428571428571429 - val_loss: 0.6075734741754781\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 3 - train_acc: 0.7142857142857143 - train_loss: 0.5443966623016039 - val_acc: 0.5357142857142857 - val_loss: 0.8676788624985806\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 4 - train_acc: 0.7678571428571429 - train_loss: 0.5041182335160211 - val_acc: 0.42857142857142855 - val_loss: 0.8723077559365505\n",
      "Validation loss decreased (0.607573 --> 0.594229).  Saving model ...\n",
      "epoch: 5 - train_acc: 0.7653061224489796 - train_loss: 0.5180490100603011 - val_acc: 0.7142857142857143 - val_loss: 0.5942285960049276\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 6 - train_acc: 0.7704081632653061 - train_loss: 0.4855558196541554 - val_acc: 0.6785714285714286 - val_loss: 0.7609996753775012\n",
      "Validation loss decreased (0.594229 --> 0.540318).  Saving model ...\n",
      "epoch: 7 - train_acc: 0.8035714285714286 - train_loss: 0.4594975675732436 - val_acc: 0.6428571428571429 - val_loss: 0.5403179798181768\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 8 - train_acc: 0.8214285714285714 - train_loss: 0.3962385433616689 - val_acc: 0.6071428571428571 - val_loss: 0.595928594334554\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 9 - train_acc: 0.7908163265306123 - train_loss: 0.44321901149396564 - val_acc: 0.5714285714285714 - val_loss: 0.806070812626825\n",
      "EarlyStopping counter: 3 out of 50\n",
      "epoch: 10 - train_acc: 0.8188775510204082 - train_loss: 0.43431877776263605 - val_acc: 0.6071428571428571 - val_loss: 0.6955891885710316\n",
      "EarlyStopping counter: 4 out of 50\n",
      "epoch: 11 - train_acc: 0.8367346938775511 - train_loss: 0.3754557365326562 - val_acc: 0.6071428571428571 - val_loss: 0.7459906451759502\n",
      "EarlyStopping counter: 5 out of 50\n",
      "epoch: 12 - train_acc: 0.8698979591836735 - train_loss: 0.33896291243717214 - val_acc: 0.6071428571428571 - val_loss: 0.618906677380897\n",
      "EarlyStopping counter: 6 out of 50\n",
      "epoch: 13 - train_acc: 0.826530612244898 - train_loss: 0.3682550659361344 - val_acc: 0.6785714285714286 - val_loss: 0.6206252153985736\n",
      "EarlyStopping counter: 7 out of 50\n",
      "epoch: 14 - train_acc: 0.8520408163265306 - train_loss: 0.3601248214666317 - val_acc: 0.6071428571428571 - val_loss: 0.7924220174606063\n",
      "EarlyStopping counter: 8 out of 50\n",
      "epoch: 15 - train_acc: 0.8698979591836735 - train_loss: 0.30246863789714484 - val_acc: 0.6785714285714286 - val_loss: 0.5747899208894293\n",
      "EarlyStopping counter: 9 out of 50\n",
      "epoch: 16 - train_acc: 0.8673469387755102 - train_loss: 0.30106160574872215 - val_acc: 0.6428571428571429 - val_loss: 0.9213844577043369\n",
      "EarlyStopping counter: 10 out of 50\n",
      "epoch: 17 - train_acc: 0.8852040816326531 - train_loss: 0.2592292145750701 - val_acc: 0.5714285714285714 - val_loss: 0.820874206178466\n",
      "EarlyStopping counter: 11 out of 50\n",
      "epoch: 18 - train_acc: 0.8954081632653061 - train_loss: 0.24909700678331825 - val_acc: 0.6071428571428571 - val_loss: 0.7921538297658157\n",
      "EarlyStopping counter: 12 out of 50\n",
      "epoch: 19 - train_acc: 0.9209183673469388 - train_loss: 0.22137462903075913 - val_acc: 0.5714285714285714 - val_loss: 0.8915190114430263\n",
      "EarlyStopping counter: 13 out of 50\n",
      "epoch: 20 - train_acc: 0.9209183673469388 - train_loss: 0.24154342022513867 - val_acc: 0.5714285714285714 - val_loss: 1.0068912520342865\n",
      "EarlyStopping counter: 14 out of 50\n",
      "epoch: 21 - train_acc: 0.8928571428571429 - train_loss: 0.2860620758925895 - val_acc: 0.5357142857142857 - val_loss: 0.9352053788745438\n",
      "EarlyStopping counter: 15 out of 50\n",
      "epoch: 22 - train_acc: 0.8724489795918368 - train_loss: 0.31213055832025904 - val_acc: 0.7857142857142857 - val_loss: 0.7277732326834121\n",
      "EarlyStopping counter: 16 out of 50\n",
      "epoch: 23 - train_acc: 0.8877551020408163 - train_loss: 0.2545264432616384 - val_acc: 0.6428571428571429 - val_loss: 0.8588515220626498\n",
      "EarlyStopping counter: 17 out of 50\n",
      "epoch: 24 - train_acc: 0.8979591836734694 - train_loss: 0.2605227525317344 - val_acc: 0.6071428571428571 - val_loss: 0.7331425136021194\n",
      "EarlyStopping counter: 18 out of 50\n",
      "epoch: 25 - train_acc: 0.9285714285714286 - train_loss: 0.21144410061579763 - val_acc: 0.6785714285714286 - val_loss: 0.6153733832603732\n",
      "EarlyStopping counter: 19 out of 50\n",
      "epoch: 26 - train_acc: 0.9209183673469388 - train_loss: 0.20053457842908434 - val_acc: 0.6071428571428571 - val_loss: 0.7328216835194135\n",
      "EarlyStopping counter: 20 out of 50\n",
      "epoch: 27 - train_acc: 0.9311224489795918 - train_loss: 0.2076380599222309 - val_acc: 0.6071428571428571 - val_loss: 1.1397913555795287\n",
      "EarlyStopping counter: 21 out of 50\n",
      "epoch: 28 - train_acc: 0.951530612244898 - train_loss: 0.16788393787716424 - val_acc: 0.5 - val_loss: 1.1001782635853696\n",
      "EarlyStopping counter: 22 out of 50\n",
      "epoch: 29 - train_acc: 0.923469387755102 - train_loss: 0.18676004790563797 - val_acc: 0.6785714285714286 - val_loss: 1.0312974204470093\n",
      "EarlyStopping counter: 23 out of 50\n",
      "epoch: 30 - train_acc: 0.951530612244898 - train_loss: 0.15799356113755547 - val_acc: 0.5714285714285714 - val_loss: 0.9606637885483896\n",
      "EarlyStopping counter: 24 out of 50\n",
      "epoch: 31 - train_acc: 0.9540816326530612 - train_loss: 0.15405361482237792 - val_acc: 0.6785714285714286 - val_loss: 0.7756140618560778\n",
      "EarlyStopping counter: 25 out of 50\n",
      "epoch: 32 - train_acc: 0.9744897959183674 - train_loss: 0.12168050423160451 - val_acc: 0.6428571428571429 - val_loss: 0.8845630493259102\n",
      "EarlyStopping counter: 26 out of 50\n",
      "epoch: 33 - train_acc: 0.9591836734693877 - train_loss: 0.11704119542860192 - val_acc: 0.6071428571428571 - val_loss: 0.9955331155480527\n",
      "EarlyStopping counter: 27 out of 50\n",
      "epoch: 34 - train_acc: 0.9311224489795918 - train_loss: 0.17935782120705485 - val_acc: 0.7142857142857143 - val_loss: 1.0921878424861848\n",
      "EarlyStopping counter: 28 out of 50\n",
      "epoch: 35 - train_acc: 0.9336734693877551 - train_loss: 0.17314999422524566 - val_acc: 0.75 - val_loss: 0.6128258108715681\n",
      "EarlyStopping counter: 29 out of 50\n",
      "epoch: 36 - train_acc: 0.9107142857142857 - train_loss: 0.22941565223361166 - val_acc: 0.6071428571428571 - val_loss: 1.8052251080925055\n",
      "EarlyStopping counter: 30 out of 50\n",
      "epoch: 37 - train_acc: 0.9107142857142857 - train_loss: 0.2169356603004948 - val_acc: 0.6785714285714286 - val_loss: 0.8687453708548221\n",
      "EarlyStopping counter: 31 out of 50\n",
      "epoch: 38 - train_acc: 0.9540816326530612 - train_loss: 0.1345108719660581 - val_acc: 0.6428571428571429 - val_loss: 1.2310745958628355\n",
      "EarlyStopping counter: 32 out of 50\n",
      "epoch: 39 - train_acc: 0.9489795918367347 - train_loss: 0.12401191896904934 - val_acc: 0.6785714285714286 - val_loss: 0.8230526585789375\n",
      "EarlyStopping counter: 33 out of 50\n",
      "epoch: 40 - train_acc: 0.9744897959183674 - train_loss: 0.09529748658156953 - val_acc: 0.6785714285714286 - val_loss: 0.7054107762099416\n",
      "EarlyStopping counter: 34 out of 50\n",
      "epoch: 41 - train_acc: 0.9438775510204082 - train_loss: 0.14508179441468722 - val_acc: 0.7142857142857143 - val_loss: 0.8324648418098184\n",
      "EarlyStopping counter: 35 out of 50\n",
      "epoch: 42 - train_acc: 0.9744897959183674 - train_loss: 0.10744144407654306 - val_acc: 0.75 - val_loss: 0.7965925406061503\n",
      "EarlyStopping counter: 36 out of 50\n",
      "epoch: 43 - train_acc: 0.9897959183673469 - train_loss: 0.06570833258559795 - val_acc: 0.7857142857142857 - val_loss: 0.7546512222511655\n",
      "EarlyStopping counter: 37 out of 50\n",
      "epoch: 44 - train_acc: 0.9795918367346939 - train_loss: 0.06460196872734979 - val_acc: 0.6785714285714286 - val_loss: 1.218118463453592\n",
      "EarlyStopping counter: 38 out of 50\n",
      "epoch: 45 - train_acc: 0.9413265306122449 - train_loss: 0.13979577617729813 - val_acc: 0.5714285714285714 - val_loss: 1.593716973854781\n",
      "EarlyStopping counter: 39 out of 50\n",
      "epoch: 46 - train_acc: 0.9336734693877551 - train_loss: 0.1765602284079301 - val_acc: 0.6428571428571429 - val_loss: 1.0141001323065464\n",
      "EarlyStopping counter: 40 out of 50\n",
      "epoch: 47 - train_acc: 0.9566326530612245 - train_loss: 0.14285689270742488 - val_acc: 0.7142857142857143 - val_loss: 1.2808748934729541\n",
      "EarlyStopping counter: 41 out of 50\n",
      "epoch: 48 - train_acc: 0.9336734693877551 - train_loss: 0.15213596989645137 - val_acc: 0.5714285714285714 - val_loss: 2.7475911815462264\n",
      "EarlyStopping counter: 42 out of 50\n",
      "epoch: 49 - train_acc: 0.9336734693877551 - train_loss: 0.14537636882649665 - val_acc: 0.6071428571428571 - val_loss: 1.2982257981077145\n",
      "EarlyStopping counter: 43 out of 50\n",
      "epoch: 50 - train_acc: 0.9744897959183674 - train_loss: 0.09220374137315972 - val_acc: 0.6428571428571429 - val_loss: 1.0271975059980916\n",
      "EarlyStopping counter: 44 out of 50\n",
      "epoch: 51 - train_acc: 0.9719387755102041 - train_loss: 0.11611577513548141 - val_acc: 0.7142857142857143 - val_loss: 1.023507124593787\n",
      "EarlyStopping counter: 45 out of 50\n",
      "epoch: 52 - train_acc: 0.9693877551020408 - train_loss: 0.09504468049221873 - val_acc: 0.5714285714285714 - val_loss: 1.04608455697623\n",
      "EarlyStopping counter: 46 out of 50\n",
      "epoch: 53 - train_acc: 0.9872448979591837 - train_loss: 0.06633412765909066 - val_acc: 0.6428571428571429 - val_loss: 1.175689974038518\n",
      "EarlyStopping counter: 47 out of 50\n",
      "epoch: 54 - train_acc: 0.9566326530612245 - train_loss: 0.11582603118279304 - val_acc: 0.7142857142857143 - val_loss: 0.8854291500455617\n",
      "EarlyStopping counter: 48 out of 50\n",
      "epoch: 55 - train_acc: 0.9693877551020408 - train_loss: 0.09130259096477332 - val_acc: 0.6071428571428571 - val_loss: 0.92971077103546\n",
      "EarlyStopping counter: 49 out of 50\n",
      "epoch: 56 - train_acc: 0.9591836734693877 - train_loss: 0.1521045418331352 - val_acc: 0.6428571428571429 - val_loss: 0.8046087831302169\n",
      "EarlyStopping counter: 50 out of 50\n",
      "epoch: 57 - train_acc: 0.9668367346938775 - train_loss: 0.08936235257983524 - val_acc: 0.7857142857142857 - val_loss: 1.0623253481297499\n",
      "=> Early stopped !!\n",
      "[total]\n",
      "Accuracy: 0.6681818181818182\n",
      "Precision: 0.7289719626168224\n",
      "Recall: 0.639344262295082\n",
      "F1: 0.6812227074235807\n",
      "[low]\n",
      "Accuracy: 0.65625\n",
      "Precision: 0.7241379310344828\n",
      "Recall: 0.6\n",
      "F1: 0.65625\n",
      "[high]\n",
      "Accuracy: 0.7\n",
      "Precision: 0.6153846153846154\n",
      "Recall: 0.8888888888888888\n",
      "F1: 0.7272727272727273\n",
      "\n",
      "[Fold 11]: \n",
      "len(train_sampler)=392\n",
      "len(val_sampler)=28\n",
      "[  8  14  27  32  40  47  61  62  64 128 135 138 156 162 200 216 230 260\n",
      " 267 279 288 300 341 357 374 380 397 409]\n",
      "Validation loss decreased (inf --> 0.651419).  Saving model ...\n",
      "epoch: 0 - train_acc: 0.6020408163265306 - train_loss: 0.6773032572395611 - val_acc: 0.6071428571428571 - val_loss: 0.651419161989828\n",
      "Validation loss decreased (0.651419 --> 0.617444).  Saving model ...\n",
      "epoch: 1 - train_acc: 0.6785714285714286 - train_loss: 0.6253241531889979 - val_acc: 0.6071428571428571 - val_loss: 0.6174439347893266\n",
      "Validation loss decreased (0.617444 --> 0.545023).  Saving model ...\n",
      "epoch: 2 - train_acc: 0.6683673469387755 - train_loss: 0.6102630129543363 - val_acc: 0.7142857142857143 - val_loss: 0.5450227332930652\n",
      "Validation loss decreased (0.545023 --> 0.488700).  Saving model ...\n",
      "epoch: 3 - train_acc: 0.7397959183673469 - train_loss: 0.5386657166534136 - val_acc: 0.7857142857142857 - val_loss: 0.48869965604614707\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 4 - train_acc: 0.7602040816326531 - train_loss: 0.5110666860451525 - val_acc: 0.75 - val_loss: 0.6899901684401994\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 5 - train_acc: 0.7423469387755102 - train_loss: 0.520662371922796 - val_acc: 0.6428571428571429 - val_loss: 0.83359127835449\n",
      "EarlyStopping counter: 3 out of 50\n",
      "epoch: 6 - train_acc: 0.7372448979591837 - train_loss: 0.5008636284848519 - val_acc: 0.6428571428571429 - val_loss: 0.656056074096307\n",
      "EarlyStopping counter: 4 out of 50\n",
      "epoch: 7 - train_acc: 0.7397959183673469 - train_loss: 0.5153136583947271 - val_acc: 0.8571428571428571 - val_loss: 0.5070380042725884\n",
      "EarlyStopping counter: 5 out of 50\n",
      "epoch: 8 - train_acc: 0.7959183673469388 - train_loss: 0.437719784412646 - val_acc: 0.75 - val_loss: 0.48893681755095\n",
      "Validation loss decreased (0.488700 --> 0.417954).  Saving model ...\n",
      "epoch: 9 - train_acc: 0.7959183673469388 - train_loss: 0.4324110243082795 - val_acc: 0.8214285714285714 - val_loss: 0.41795380787569963\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 10 - train_acc: 0.798469387755102 - train_loss: 0.4454348386070127 - val_acc: 0.7142857142857143 - val_loss: 0.5927827746254373\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 11 - train_acc: 0.8290816326530612 - train_loss: 0.3835087610289461 - val_acc: 0.7857142857142857 - val_loss: 0.5432162784484804\n",
      "EarlyStopping counter: 3 out of 50\n",
      "epoch: 12 - train_acc: 0.8392857142857143 - train_loss: 0.37141930987848976 - val_acc: 0.8214285714285714 - val_loss: 0.515800717248442\n",
      "EarlyStopping counter: 4 out of 50\n",
      "epoch: 13 - train_acc: 0.8392857142857143 - train_loss: 0.3618912639662244 - val_acc: 0.7857142857142857 - val_loss: 0.5052352552263126\n",
      "EarlyStopping counter: 5 out of 50\n",
      "epoch: 14 - train_acc: 0.8647959183673469 - train_loss: 0.32475720072491504 - val_acc: 0.8571428571428571 - val_loss: 0.4717256969179323\n",
      "EarlyStopping counter: 6 out of 50\n",
      "epoch: 15 - train_acc: 0.8647959183673469 - train_loss: 0.3202366685822737 - val_acc: 0.75 - val_loss: 0.5469694319338804\n",
      "EarlyStopping counter: 7 out of 50\n",
      "epoch: 16 - train_acc: 0.8647959183673469 - train_loss: 0.314079820085052 - val_acc: 0.7142857142857143 - val_loss: 0.5252239435309988\n",
      "EarlyStopping counter: 8 out of 50\n",
      "epoch: 17 - train_acc: 0.9158163265306123 - train_loss: 0.2639290862532765 - val_acc: 0.8571428571428571 - val_loss: 0.4635127389848807\n",
      "EarlyStopping counter: 9 out of 50\n",
      "epoch: 18 - train_acc: 0.8928571428571429 - train_loss: 0.26898422753923523 - val_acc: 0.75 - val_loss: 0.44357020761088306\n",
      "EarlyStopping counter: 10 out of 50\n",
      "epoch: 19 - train_acc: 0.8545918367346939 - train_loss: 0.3471951630271717 - val_acc: 0.75 - val_loss: 0.6036297500087557\n",
      "EarlyStopping counter: 11 out of 50\n",
      "epoch: 20 - train_acc: 0.9081632653061225 - train_loss: 0.24323873705021593 - val_acc: 0.6785714285714286 - val_loss: 0.7072739829560503\n",
      "Validation loss decreased (0.417954 --> 0.399199).  Saving model ...\n",
      "epoch: 21 - train_acc: 0.9005102040816326 - train_loss: 0.26609094571144454 - val_acc: 0.8214285714285714 - val_loss: 0.399199179224799\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 22 - train_acc: 0.9081632653061225 - train_loss: 0.2393073236417568 - val_acc: 0.8214285714285714 - val_loss: 0.560817578420101\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 23 - train_acc: 0.9183673469387755 - train_loss: 0.21226698199872404 - val_acc: 0.75 - val_loss: 0.9015278153371173\n",
      "EarlyStopping counter: 3 out of 50\n",
      "epoch: 24 - train_acc: 0.9056122448979592 - train_loss: 0.23727466164320196 - val_acc: 0.75 - val_loss: 0.5280069521989579\n",
      "EarlyStopping counter: 4 out of 50\n",
      "epoch: 25 - train_acc: 0.9005102040816326 - train_loss: 0.24792836129526924 - val_acc: 0.7142857142857143 - val_loss: 1.0439363927592726\n",
      "EarlyStopping counter: 5 out of 50\n",
      "epoch: 26 - train_acc: 0.9081632653061225 - train_loss: 0.23406331019277526 - val_acc: 0.7142857142857143 - val_loss: 0.5340477747118799\n",
      "EarlyStopping counter: 6 out of 50\n",
      "epoch: 27 - train_acc: 0.9158163265306123 - train_loss: 0.24821661750365695 - val_acc: 0.8571428571428571 - val_loss: 0.43761383652063274\n",
      "EarlyStopping counter: 7 out of 50\n",
      "epoch: 28 - train_acc: 0.9056122448979592 - train_loss: 0.22139230060063775 - val_acc: 0.75 - val_loss: 0.6494868590428757\n",
      "EarlyStopping counter: 8 out of 50\n",
      "epoch: 29 - train_acc: 0.9132653061224489 - train_loss: 0.20739733179288344 - val_acc: 0.7857142857142857 - val_loss: 0.40528034814005093\n",
      "EarlyStopping counter: 9 out of 50\n",
      "epoch: 30 - train_acc: 0.9081632653061225 - train_loss: 0.23424119778378819 - val_acc: 0.75 - val_loss: 0.870864822560557\n",
      "EarlyStopping counter: 10 out of 50\n",
      "epoch: 31 - train_acc: 0.9132653061224489 - train_loss: 0.21942806684376237 - val_acc: 0.8571428571428571 - val_loss: 0.42285032468149686\n",
      "EarlyStopping counter: 11 out of 50\n",
      "epoch: 32 - train_acc: 0.9438775510204082 - train_loss: 0.18131423072828717 - val_acc: 0.7142857142857143 - val_loss: 0.6435523300311778\n",
      "EarlyStopping counter: 12 out of 50\n",
      "epoch: 33 - train_acc: 0.9464285714285714 - train_loss: 0.13679039034012386 - val_acc: 0.7857142857142857 - val_loss: 0.4289206544966666\n",
      "EarlyStopping counter: 13 out of 50\n",
      "epoch: 34 - train_acc: 0.9693877551020408 - train_loss: 0.11767059503660933 - val_acc: 0.8214285714285714 - val_loss: 0.46016204596842314\n",
      "EarlyStopping counter: 14 out of 50\n",
      "epoch: 35 - train_acc: 0.9540816326530612 - train_loss: 0.13489634239361215 - val_acc: 0.75 - val_loss: 0.5761832362131842\n",
      "EarlyStopping counter: 15 out of 50\n",
      "epoch: 36 - train_acc: 0.9744897959183674 - train_loss: 0.10542120231851768 - val_acc: 0.75 - val_loss: 0.5655834025969652\n",
      "EarlyStopping counter: 16 out of 50\n",
      "epoch: 37 - train_acc: 0.951530612244898 - train_loss: 0.12532701724903061 - val_acc: 0.7857142857142857 - val_loss: 0.7492525472756609\n",
      "EarlyStopping counter: 17 out of 50\n",
      "epoch: 38 - train_acc: 0.9795918367346939 - train_loss: 0.07808039745124 - val_acc: 0.8214285714285714 - val_loss: 0.5239334834115386\n",
      "EarlyStopping counter: 18 out of 50\n",
      "epoch: 39 - train_acc: 0.9438775510204082 - train_loss: 0.15352312456281644 - val_acc: 0.7857142857142857 - val_loss: 0.6441926575847875\n",
      "Validation loss decreased (0.399199 --> 0.299823).  Saving model ...\n",
      "epoch: 40 - train_acc: 0.9591836734693877 - train_loss: 0.1074495935899974 - val_acc: 0.8214285714285714 - val_loss: 0.2998228006789544\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 41 - train_acc: 0.9438775510204082 - train_loss: 0.14052734603884917 - val_acc: 0.7857142857142857 - val_loss: 0.40573740796507146\n",
      "Validation loss decreased (0.299823 --> 0.234334).  Saving model ...\n",
      "epoch: 42 - train_acc: 0.9693877551020408 - train_loss: 0.09673870469099503 - val_acc: 0.8571428571428571 - val_loss: 0.2343336723030684\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 43 - train_acc: 0.9642857142857143 - train_loss: 0.09372656752934783 - val_acc: 0.8571428571428571 - val_loss: 0.3864496765630741\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 44 - train_acc: 0.9566326530612245 - train_loss: 0.11533499776124667 - val_acc: 0.7142857142857143 - val_loss: 0.9886439379961325\n",
      "EarlyStopping counter: 3 out of 50\n",
      "epoch: 45 - train_acc: 0.9566326530612245 - train_loss: 0.12689885807718068 - val_acc: 0.7857142857142857 - val_loss: 0.48579378479880864\n",
      "EarlyStopping counter: 4 out of 50\n",
      "epoch: 46 - train_acc: 0.9668367346938775 - train_loss: 0.09191185128728176 - val_acc: 0.8214285714285714 - val_loss: 0.6661161836078289\n",
      "EarlyStopping counter: 5 out of 50\n",
      "epoch: 47 - train_acc: 0.9770408163265306 - train_loss: 0.08555852647136096 - val_acc: 0.8571428571428571 - val_loss: 0.49319723871688137\n",
      "EarlyStopping counter: 6 out of 50\n",
      "epoch: 48 - train_acc: 0.9744897959183674 - train_loss: 0.10056527904203502 - val_acc: 0.7857142857142857 - val_loss: 0.47080403126486114\n",
      "EarlyStopping counter: 7 out of 50\n",
      "epoch: 49 - train_acc: 0.9668367346938775 - train_loss: 0.09596102068005766 - val_acc: 0.7857142857142857 - val_loss: 0.9618484343733366\n",
      "EarlyStopping counter: 8 out of 50\n",
      "epoch: 50 - train_acc: 0.9362244897959183 - train_loss: 0.1857605154986032 - val_acc: 0.6785714285714286 - val_loss: 1.1834738976920665\n",
      "EarlyStopping counter: 9 out of 50\n",
      "epoch: 51 - train_acc: 0.9362244897959183 - train_loss: 0.2097862509545036 - val_acc: 0.7142857142857143 - val_loss: 0.7774318609912847\n",
      "EarlyStopping counter: 10 out of 50\n",
      "epoch: 52 - train_acc: 0.9489795918367347 - train_loss: 0.14354641141320587 - val_acc: 0.8214285714285714 - val_loss: 0.47276935993183317\n",
      "EarlyStopping counter: 11 out of 50\n",
      "epoch: 53 - train_acc: 0.9617346938775511 - train_loss: 0.1231369733006318 - val_acc: 0.7857142857142857 - val_loss: 0.5639160454412075\n",
      "EarlyStopping counter: 12 out of 50\n",
      "epoch: 54 - train_acc: 0.9617346938775511 - train_loss: 0.11297636644536325 - val_acc: 0.8214285714285714 - val_loss: 0.6242780898804872\n",
      "EarlyStopping counter: 13 out of 50\n",
      "epoch: 55 - train_acc: 0.9948979591836735 - train_loss: 0.057859638380933574 - val_acc: 0.7857142857142857 - val_loss: 0.5045852192266779\n",
      "EarlyStopping counter: 14 out of 50\n",
      "epoch: 56 - train_acc: 0.9617346938775511 - train_loss: 0.09765392348370004 - val_acc: 0.7857142857142857 - val_loss: 0.5097768328431306\n",
      "EarlyStopping counter: 15 out of 50\n",
      "epoch: 57 - train_acc: 0.9795918367346939 - train_loss: 0.08925578270408373 - val_acc: 0.7857142857142857 - val_loss: 0.8995023119651241\n",
      "EarlyStopping counter: 16 out of 50\n",
      "epoch: 58 - train_acc: 0.9617346938775511 - train_loss: 0.12528164000027975 - val_acc: 0.75 - val_loss: 0.9540915364095098\n",
      "EarlyStopping counter: 17 out of 50\n",
      "epoch: 59 - train_acc: 0.9821428571428571 - train_loss: 0.06458957614319341 - val_acc: 0.75 - val_loss: 0.6995909383184933\n",
      "EarlyStopping counter: 18 out of 50\n",
      "epoch: 60 - train_acc: 0.9744897959183674 - train_loss: 0.06915831831844123 - val_acc: 0.8214285714285714 - val_loss: 0.7348264643833995\n",
      "EarlyStopping counter: 19 out of 50\n",
      "epoch: 61 - train_acc: 0.9719387755102041 - train_loss: 0.07536093367568406 - val_acc: 0.8214285714285714 - val_loss: 0.6525419966767018\n",
      "EarlyStopping counter: 20 out of 50\n",
      "epoch: 62 - train_acc: 0.9744897959183674 - train_loss: 0.06017634669377592 - val_acc: 0.7857142857142857 - val_loss: 0.4972351279982412\n",
      "EarlyStopping counter: 21 out of 50\n",
      "epoch: 63 - train_acc: 0.9719387755102041 - train_loss: 0.06703161833597887 - val_acc: 0.6785714285714286 - val_loss: 0.6549681560892646\n",
      "EarlyStopping counter: 22 out of 50\n",
      "epoch: 64 - train_acc: 0.9693877551020408 - train_loss: 0.0764049066897057 - val_acc: 0.8214285714285714 - val_loss: 0.795675249092741\n",
      "EarlyStopping counter: 23 out of 50\n",
      "epoch: 65 - train_acc: 0.9693877551020408 - train_loss: 0.07833896438194432 - val_acc: 0.8214285714285714 - val_loss: 0.7339662860162809\n",
      "EarlyStopping counter: 24 out of 50\n",
      "epoch: 66 - train_acc: 0.9642857142857143 - train_loss: 0.1154081260184748 - val_acc: 0.8571428571428571 - val_loss: 0.504864565890351\n",
      "EarlyStopping counter: 25 out of 50\n",
      "epoch: 67 - train_acc: 0.9591836734693877 - train_loss: 0.11828096972675518 - val_acc: 0.7142857142857143 - val_loss: 0.8859147925972799\n",
      "EarlyStopping counter: 26 out of 50\n",
      "epoch: 68 - train_acc: 0.9617346938775511 - train_loss: 0.12795626080137582 - val_acc: 0.7857142857142857 - val_loss: 0.9468614717043482\n",
      "EarlyStopping counter: 27 out of 50\n",
      "epoch: 69 - train_acc: 0.9693877551020408 - train_loss: 0.08707443446107833 - val_acc: 0.75 - val_loss: 0.8031823570445071\n",
      "EarlyStopping counter: 28 out of 50\n",
      "epoch: 70 - train_acc: 0.9693877551020408 - train_loss: 0.13983296273531615 - val_acc: 0.75 - val_loss: 0.7676533140439721\n",
      "EarlyStopping counter: 29 out of 50\n",
      "epoch: 71 - train_acc: 0.9387755102040817 - train_loss: 0.14269291037486684 - val_acc: 0.7857142857142857 - val_loss: 0.7745910898349745\n",
      "EarlyStopping counter: 30 out of 50\n",
      "epoch: 72 - train_acc: 0.9668367346938775 - train_loss: 0.12698237187605982 - val_acc: 0.8571428571428571 - val_loss: 0.3474003608703693\n",
      "EarlyStopping counter: 31 out of 50\n",
      "epoch: 73 - train_acc: 0.9489795918367347 - train_loss: 0.15700278885128022 - val_acc: 0.8214285714285714 - val_loss: 0.4438798294719176\n",
      "EarlyStopping counter: 32 out of 50\n",
      "epoch: 74 - train_acc: 0.9770408163265306 - train_loss: 0.06087457273085057 - val_acc: 0.8214285714285714 - val_loss: 0.35627397240850844\n",
      "EarlyStopping counter: 33 out of 50\n",
      "epoch: 75 - train_acc: 0.9923469387755102 - train_loss: 0.04612758230787099 - val_acc: 0.75 - val_loss: 0.5184701021695959\n",
      "EarlyStopping counter: 34 out of 50\n",
      "epoch: 76 - train_acc: 0.9923469387755102 - train_loss: 0.03420890689481196 - val_acc: 0.8214285714285714 - val_loss: 0.3641002810377422\n",
      "EarlyStopping counter: 35 out of 50\n",
      "epoch: 77 - train_acc: 0.9872448979591837 - train_loss: 0.033804226507158225 - val_acc: 0.8214285714285714 - val_loss: 0.6428162682671434\n",
      "EarlyStopping counter: 36 out of 50\n",
      "epoch: 78 - train_acc: 0.9770408163265306 - train_loss: 0.050950390137800804 - val_acc: 0.8214285714285714 - val_loss: 0.5071934993639842\n",
      "EarlyStopping counter: 37 out of 50\n",
      "epoch: 79 - train_acc: 0.9897959183673469 - train_loss: 0.04273271892094059 - val_acc: 0.8214285714285714 - val_loss: 0.453196317571467\n",
      "EarlyStopping counter: 38 out of 50\n",
      "epoch: 80 - train_acc: 0.9948979591836735 - train_loss: 0.032541980883618885 - val_acc: 0.75 - val_loss: 0.6603891018743213\n",
      "EarlyStopping counter: 39 out of 50\n",
      "epoch: 81 - train_acc: 0.9897959183673469 - train_loss: 0.04860795232014828 - val_acc: 0.75 - val_loss: 0.8015548770466336\n",
      "EarlyStopping counter: 40 out of 50\n",
      "epoch: 82 - train_acc: 0.9974489795918368 - train_loss: 0.02573960135141025 - val_acc: 0.8214285714285714 - val_loss: 0.5443435524852773\n",
      "EarlyStopping counter: 41 out of 50\n",
      "epoch: 83 - train_acc: 0.9897959183673469 - train_loss: 0.031125317603399268 - val_acc: 0.8571428571428571 - val_loss: 0.5460920838551195\n",
      "EarlyStopping counter: 42 out of 50\n",
      "epoch: 84 - train_acc: 0.9872448979591837 - train_loss: 0.04169182573404298 - val_acc: 0.6785714285714286 - val_loss: 0.8297902352049811\n",
      "EarlyStopping counter: 43 out of 50\n",
      "epoch: 85 - train_acc: 0.9642857142857143 - train_loss: 0.10990196545606662 - val_acc: 0.75 - val_loss: 1.0748246448561891\n",
      "EarlyStopping counter: 44 out of 50\n",
      "epoch: 86 - train_acc: 0.9719387755102041 - train_loss: 0.07330089850321447 - val_acc: 0.7142857142857143 - val_loss: 0.943165977197689\n",
      "EarlyStopping counter: 45 out of 50\n",
      "epoch: 87 - train_acc: 0.9846938775510204 - train_loss: 0.05366988861528589 - val_acc: 0.8571428571428571 - val_loss: 0.35477486871068703\n",
      "EarlyStopping counter: 46 out of 50\n",
      "epoch: 88 - train_acc: 0.9821428571428571 - train_loss: 0.05839613339984955 - val_acc: 0.7857142857142857 - val_loss: 0.7571068403718234\n",
      "EarlyStopping counter: 47 out of 50\n",
      "epoch: 89 - train_acc: 0.9821428571428571 - train_loss: 0.04170938644839246 - val_acc: 0.7142857142857143 - val_loss: 0.6590490096459758\n",
      "EarlyStopping counter: 48 out of 50\n",
      "epoch: 90 - train_acc: 0.9693877551020408 - train_loss: 0.07538287373729491 - val_acc: 0.7857142857142857 - val_loss: 0.6252971134155488\n",
      "EarlyStopping counter: 49 out of 50\n",
      "epoch: 91 - train_acc: 0.9872448979591837 - train_loss: 0.04591111635027958 - val_acc: 0.8571428571428571 - val_loss: 0.5362681378252554\n",
      "EarlyStopping counter: 50 out of 50\n",
      "epoch: 92 - train_acc: 0.9846938775510204 - train_loss: 0.038551350354128926 - val_acc: 0.7857142857142857 - val_loss: 0.43353681293158813\n",
      "=> Early stopped !!\n",
      "[total]\n",
      "Accuracy: 0.7363636363636363\n",
      "Precision: 0.7857142857142857\n",
      "Recall: 0.7213114754098361\n",
      "F1: 0.7521367521367521\n",
      "[low]\n",
      "Accuracy: 0.734375\n",
      "Precision: 0.7872340425531915\n",
      "Recall: 0.7047619047619048\n",
      "F1: 0.7437185929648241\n",
      "[high]\n",
      "Accuracy: 0.8\n",
      "Precision: 0.6923076923076923\n",
      "Recall: 1.0\n",
      "F1: 0.8181818181818181\n",
      "\n",
      "[Fold 12]: \n",
      "len(train_sampler)=392\n",
      "len(val_sampler)=28\n",
      "[  1  34  43  49  52  53  80  91 105 161 190 201 205 207 212 217 236 251\n",
      " 259 263 269 295 303 309 339 383 384 410]\n",
      "Validation loss decreased (inf --> 0.706092).  Saving model ...\n",
      "epoch: 0 - train_acc: 0.6071428571428571 - train_loss: 0.6733631425788494 - val_acc: 0.39285714285714285 - val_loss: 0.7060916601317107\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 1 - train_acc: 0.6862244897959183 - train_loss: 0.604434494430046 - val_acc: 0.5 - val_loss: 0.7381526729351485\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 2 - train_acc: 0.7117346938775511 - train_loss: 0.5607795727235578 - val_acc: 0.5714285714285714 - val_loss: 0.8170578531022509\n",
      "EarlyStopping counter: 3 out of 50\n",
      "epoch: 3 - train_acc: 0.6836734693877551 - train_loss: 0.5806237302959412 - val_acc: 0.5 - val_loss: 0.8595858674626041\n",
      "EarlyStopping counter: 4 out of 50\n",
      "epoch: 4 - train_acc: 0.7321428571428571 - train_loss: 0.5378918249430529 - val_acc: 0.6071428571428571 - val_loss: 0.7471525611091454\n",
      "EarlyStopping counter: 5 out of 50\n",
      "epoch: 5 - train_acc: 0.798469387755102 - train_loss: 0.4786681596590374 - val_acc: 0.5357142857142857 - val_loss: 0.8778357284220826\n",
      "EarlyStopping counter: 6 out of 50\n",
      "epoch: 6 - train_acc: 0.7678571428571429 - train_loss: 0.47486970138874507 - val_acc: 0.5 - val_loss: 0.7506042354033724\n",
      "EarlyStopping counter: 7 out of 50\n",
      "epoch: 7 - train_acc: 0.7857142857142857 - train_loss: 0.4651992944353365 - val_acc: 0.7142857142857143 - val_loss: 0.706964913967598\n",
      "EarlyStopping counter: 8 out of 50\n",
      "epoch: 8 - train_acc: 0.826530612244898 - train_loss: 0.43095452584353044 - val_acc: 0.6428571428571429 - val_loss: 0.7384280505627121\n",
      "EarlyStopping counter: 9 out of 50\n",
      "epoch: 9 - train_acc: 0.826530612244898 - train_loss: 0.40960939486190745 - val_acc: 0.6428571428571429 - val_loss: 0.7355816172270324\n",
      "EarlyStopping counter: 10 out of 50\n",
      "epoch: 10 - train_acc: 0.8545918367346939 - train_loss: 0.35163680743517123 - val_acc: 0.6785714285714286 - val_loss: 0.992281824683532\n",
      "EarlyStopping counter: 11 out of 50\n",
      "epoch: 11 - train_acc: 0.8137755102040817 - train_loss: 0.38962509003225076 - val_acc: 0.75 - val_loss: 0.7901021494006615\n",
      "EarlyStopping counter: 12 out of 50\n",
      "epoch: 12 - train_acc: 0.8622448979591837 - train_loss: 0.32335744358803237 - val_acc: 0.75 - val_loss: 0.8843549191361468\n",
      "EarlyStopping counter: 13 out of 50\n",
      "epoch: 13 - train_acc: 0.8545918367346939 - train_loss: 0.3358626527562808 - val_acc: 0.75 - val_loss: 0.7979712747102387\n",
      "EarlyStopping counter: 14 out of 50\n",
      "epoch: 14 - train_acc: 0.8698979591836735 - train_loss: 0.32159332135545404 - val_acc: 0.75 - val_loss: 0.724013911895552\n",
      "EarlyStopping counter: 15 out of 50\n",
      "epoch: 15 - train_acc: 0.8443877551020408 - train_loss: 0.36028281043085336 - val_acc: 0.6785714285714286 - val_loss: 0.819714061261136\n",
      "Validation loss decreased (0.706092 --> 0.704481).  Saving model ...\n",
      "epoch: 16 - train_acc: 0.8520408163265306 - train_loss: 0.346582080655476 - val_acc: 0.75 - val_loss: 0.7044811098095147\n",
      "Validation loss decreased (0.704481 --> 0.651440).  Saving model ...\n",
      "epoch: 17 - train_acc: 0.8622448979591837 - train_loss: 0.29716626534515084 - val_acc: 0.6785714285714286 - val_loss: 0.6514396273688341\n",
      "Validation loss decreased (0.651440 --> 0.606241).  Saving model ...\n",
      "epoch: 18 - train_acc: 0.8469387755102041 - train_loss: 0.3284443031340326 - val_acc: 0.7142857142857143 - val_loss: 0.606240674282182\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 19 - train_acc: 0.9005102040816326 - train_loss: 0.25309080558981173 - val_acc: 0.7142857142857143 - val_loss: 0.6277795950946246\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 20 - train_acc: 0.9183673469387755 - train_loss: 0.2307525554128605 - val_acc: 0.7142857142857143 - val_loss: 0.6463419532273322\n",
      "EarlyStopping counter: 3 out of 50\n",
      "epoch: 21 - train_acc: 0.9209183673469388 - train_loss: 0.2299855965730102 - val_acc: 0.7142857142857143 - val_loss: 0.7620467736573493\n",
      "EarlyStopping counter: 4 out of 50\n",
      "epoch: 22 - train_acc: 0.9030612244897959 - train_loss: 0.2595495526013278 - val_acc: 0.6785714285714286 - val_loss: 1.0752012316396597\n",
      "EarlyStopping counter: 5 out of 50\n",
      "epoch: 23 - train_acc: 0.9209183673469388 - train_loss: 0.23448727556362986 - val_acc: 0.75 - val_loss: 0.6973215282767071\n",
      "EarlyStopping counter: 6 out of 50\n",
      "epoch: 24 - train_acc: 0.9183673469387755 - train_loss: 0.20622649469949073 - val_acc: 0.7142857142857143 - val_loss: 0.8175891482293778\n",
      "EarlyStopping counter: 7 out of 50\n",
      "epoch: 25 - train_acc: 0.9285714285714286 - train_loss: 0.19726784619738497 - val_acc: 0.7857142857142857 - val_loss: 0.7880515353845239\n",
      "EarlyStopping counter: 8 out of 50\n",
      "epoch: 26 - train_acc: 0.8775510204081632 - train_loss: 0.2903789445363116 - val_acc: 0.75 - val_loss: 0.7707661099317232\n",
      "EarlyStopping counter: 9 out of 50\n",
      "epoch: 27 - train_acc: 0.9362244897959183 - train_loss: 0.20557276049083303 - val_acc: 0.7142857142857143 - val_loss: 0.871509964871591\n",
      "EarlyStopping counter: 10 out of 50\n",
      "epoch: 28 - train_acc: 0.9336734693877551 - train_loss: 0.20066248670844017 - val_acc: 0.6785714285714286 - val_loss: 0.9342666047959212\n",
      "EarlyStopping counter: 11 out of 50\n",
      "epoch: 29 - train_acc: 0.951530612244898 - train_loss: 0.1466573321780637 - val_acc: 0.7142857142857143 - val_loss: 0.7673481414691399\n",
      "EarlyStopping counter: 12 out of 50\n",
      "epoch: 30 - train_acc: 0.9489795918367347 - train_loss: 0.15477465725895787 - val_acc: 0.7142857142857143 - val_loss: 1.0548736795805085\n",
      "EarlyStopping counter: 13 out of 50\n",
      "epoch: 31 - train_acc: 0.9489795918367347 - train_loss: 0.13764734381527693 - val_acc: 0.7142857142857143 - val_loss: 1.3742132632179087\n",
      "EarlyStopping counter: 14 out of 50\n",
      "epoch: 32 - train_acc: 0.923469387755102 - train_loss: 0.17961938777056521 - val_acc: 0.7142857142857143 - val_loss: 1.382134418574656\n",
      "EarlyStopping counter: 15 out of 50\n",
      "epoch: 33 - train_acc: 0.9540816326530612 - train_loss: 0.14247273806402705 - val_acc: 0.7142857142857143 - val_loss: 1.2344374604888286\n",
      "EarlyStopping counter: 16 out of 50\n",
      "epoch: 34 - train_acc: 0.9285714285714286 - train_loss: 0.16806550903111864 - val_acc: 0.6785714285714286 - val_loss: 0.9798999085197166\n",
      "EarlyStopping counter: 17 out of 50\n",
      "epoch: 35 - train_acc: 0.9413265306122449 - train_loss: 0.1634120530522624 - val_acc: 0.7142857142857143 - val_loss: 0.9074466883550274\n",
      "EarlyStopping counter: 18 out of 50\n",
      "epoch: 36 - train_acc: 0.9438775510204082 - train_loss: 0.14529803232952698 - val_acc: 0.7142857142857143 - val_loss: 0.949030162440304\n",
      "EarlyStopping counter: 19 out of 50\n",
      "epoch: 37 - train_acc: 0.9209183673469388 - train_loss: 0.21193882243735523 - val_acc: 0.75 - val_loss: 0.705758236275639\n",
      "Validation loss decreased (0.606241 --> 0.558800).  Saving model ...\n",
      "epoch: 38 - train_acc: 0.9285714285714286 - train_loss: 0.19096710206731365 - val_acc: 0.8571428571428571 - val_loss: 0.5587996346041798\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 39 - train_acc: 0.9540816326530612 - train_loss: 0.17412119973534324 - val_acc: 0.7142857142857143 - val_loss: 0.729506659866396\n",
      "Validation loss decreased (0.558800 --> 0.472801).  Saving model ...\n",
      "epoch: 40 - train_acc: 0.9540816326530612 - train_loss: 0.13610590885404183 - val_acc: 0.8214285714285714 - val_loss: 0.47280086702704827\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 41 - train_acc: 0.9566326530612245 - train_loss: 0.13195095485546313 - val_acc: 0.8214285714285714 - val_loss: 0.5304661495424311\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 42 - train_acc: 0.9617346938775511 - train_loss: 0.09703800083270579 - val_acc: 0.75 - val_loss: 0.5550752861400448\n",
      "EarlyStopping counter: 3 out of 50\n",
      "epoch: 43 - train_acc: 0.9642857142857143 - train_loss: 0.1145269557731935 - val_acc: 0.75 - val_loss: 0.6952076902585473\n",
      "EarlyStopping counter: 4 out of 50\n",
      "epoch: 44 - train_acc: 0.951530612244898 - train_loss: 0.1557721192638928 - val_acc: 0.75 - val_loss: 0.7064166598105531\n",
      "EarlyStopping counter: 5 out of 50\n",
      "epoch: 45 - train_acc: 0.9387755102040817 - train_loss: 0.13771331630255765 - val_acc: 0.75 - val_loss: 1.0280588092262097\n",
      "EarlyStopping counter: 6 out of 50\n",
      "epoch: 46 - train_acc: 0.9744897959183674 - train_loss: 0.11828125122849087 - val_acc: 0.75 - val_loss: 0.7781228517521345\n",
      "EarlyStopping counter: 7 out of 50\n",
      "epoch: 47 - train_acc: 0.9591836734693877 - train_loss: 0.11595578178451346 - val_acc: 0.7142857142857143 - val_loss: 0.5624254242728117\n",
      "EarlyStopping counter: 8 out of 50\n",
      "epoch: 48 - train_acc: 0.9617346938775511 - train_loss: 0.11434899116579118 - val_acc: 0.7142857142857143 - val_loss: 1.0894456651362499\n",
      "EarlyStopping counter: 9 out of 50\n",
      "epoch: 49 - train_acc: 0.9744897959183674 - train_loss: 0.08266080265068525 - val_acc: 0.6785714285714286 - val_loss: 0.9367073067711971\n",
      "EarlyStopping counter: 10 out of 50\n",
      "epoch: 50 - train_acc: 0.9719387755102041 - train_loss: 0.09398809368963904 - val_acc: 0.7142857142857143 - val_loss: 0.9088778826242369\n",
      "EarlyStopping counter: 11 out of 50\n",
      "epoch: 51 - train_acc: 0.9591836734693877 - train_loss: 0.10896635928325811 - val_acc: 0.7857142857142857 - val_loss: 0.9381903515807016\n",
      "EarlyStopping counter: 12 out of 50\n",
      "epoch: 52 - train_acc: 0.9591836734693877 - train_loss: 0.11331256287339242 - val_acc: 0.75 - val_loss: 0.7592597306545077\n",
      "EarlyStopping counter: 13 out of 50\n",
      "epoch: 53 - train_acc: 0.9744897959183674 - train_loss: 0.08225331087828647 - val_acc: 0.7142857142857143 - val_loss: 0.6169971390665228\n",
      "EarlyStopping counter: 14 out of 50\n",
      "epoch: 54 - train_acc: 0.9872448979591837 - train_loss: 0.0712038392611297 - val_acc: 0.8214285714285714 - val_loss: 1.0696496386253636\n",
      "EarlyStopping counter: 15 out of 50\n",
      "epoch: 55 - train_acc: 0.9668367346938775 - train_loss: 0.09843782406363336 - val_acc: 0.7142857142857143 - val_loss: 1.3325042660736357\n",
      "EarlyStopping counter: 16 out of 50\n",
      "epoch: 56 - train_acc: 0.9617346938775511 - train_loss: 0.08728999789282225 - val_acc: 0.7857142857142857 - val_loss: 1.0445346382825824\n",
      "EarlyStopping counter: 17 out of 50\n",
      "epoch: 57 - train_acc: 0.9693877551020408 - train_loss: 0.09703096617646034 - val_acc: 0.7857142857142857 - val_loss: 0.7582866735648597\n",
      "EarlyStopping counter: 18 out of 50\n",
      "epoch: 58 - train_acc: 0.9744897959183674 - train_loss: 0.06504452325104429 - val_acc: 0.8214285714285714 - val_loss: 0.5465143963277557\n",
      "EarlyStopping counter: 19 out of 50\n",
      "epoch: 59 - train_acc: 0.9591836734693877 - train_loss: 0.10269453276093429 - val_acc: 0.6785714285714286 - val_loss: 0.8461433582498357\n",
      "EarlyStopping counter: 20 out of 50\n",
      "epoch: 60 - train_acc: 0.9770408163265306 - train_loss: 0.07281702419150388 - val_acc: 0.7857142857142857 - val_loss: 0.6244374398286898\n",
      "EarlyStopping counter: 21 out of 50\n",
      "epoch: 61 - train_acc: 0.9872448979591837 - train_loss: 0.051013117471532275 - val_acc: 0.7142857142857143 - val_loss: 0.7936872194573525\n",
      "EarlyStopping counter: 22 out of 50\n",
      "epoch: 62 - train_acc: 0.9897959183673469 - train_loss: 0.03170660687834735 - val_acc: 0.7142857142857143 - val_loss: 0.928353406184456\n",
      "EarlyStopping counter: 23 out of 50\n",
      "epoch: 63 - train_acc: 0.9872448979591837 - train_loss: 0.04013413658610854 - val_acc: 0.75 - val_loss: 0.781119103058629\n",
      "EarlyStopping counter: 24 out of 50\n",
      "epoch: 64 - train_acc: 0.9846938775510204 - train_loss: 0.05759291156412486 - val_acc: 0.8214285714285714 - val_loss: 0.9412649070986722\n",
      "EarlyStopping counter: 25 out of 50\n",
      "epoch: 65 - train_acc: 0.9872448979591837 - train_loss: 0.0695013220959784 - val_acc: 0.7142857142857143 - val_loss: 1.3785397878730326\n",
      "EarlyStopping counter: 26 out of 50\n",
      "epoch: 66 - train_acc: 0.9770408163265306 - train_loss: 0.06874285856074254 - val_acc: 0.75 - val_loss: 1.0704187101926026\n",
      "EarlyStopping counter: 27 out of 50\n",
      "epoch: 67 - train_acc: 0.9770408163265306 - train_loss: 0.07661154292436606 - val_acc: 0.75 - val_loss: 1.1348869833361745\n",
      "EarlyStopping counter: 28 out of 50\n",
      "epoch: 68 - train_acc: 0.9693877551020408 - train_loss: 0.08837752559535347 - val_acc: 0.75 - val_loss: 0.9029025761468106\n",
      "EarlyStopping counter: 29 out of 50\n",
      "epoch: 69 - train_acc: 0.9693877551020408 - train_loss: 0.08928839178640043 - val_acc: 0.75 - val_loss: 1.1375859259059098\n",
      "EarlyStopping counter: 30 out of 50\n",
      "epoch: 70 - train_acc: 0.9489795918367347 - train_loss: 0.15093636024205506 - val_acc: 0.6428571428571429 - val_loss: 1.9033145988674147\n",
      "EarlyStopping counter: 31 out of 50\n",
      "epoch: 71 - train_acc: 0.9591836734693877 - train_loss: 0.11922418943030291 - val_acc: 0.75 - val_loss: 0.5459309225116601\n",
      "EarlyStopping counter: 32 out of 50\n",
      "epoch: 72 - train_acc: 0.9795918367346939 - train_loss: 0.06646321207395209 - val_acc: 0.75 - val_loss: 0.7377093384762341\n",
      "EarlyStopping counter: 33 out of 50\n",
      "epoch: 73 - train_acc: 0.9846938775510204 - train_loss: 0.05712811411563394 - val_acc: 0.7857142857142857 - val_loss: 1.0092514442451965\n",
      "EarlyStopping counter: 34 out of 50\n",
      "epoch: 74 - train_acc: 0.9770408163265306 - train_loss: 0.10876823236714082 - val_acc: 0.75 - val_loss: 0.9003755101375305\n",
      "EarlyStopping counter: 35 out of 50\n",
      "epoch: 75 - train_acc: 0.9591836734693877 - train_loss: 0.10247597894657041 - val_acc: 0.7142857142857143 - val_loss: 0.8684832527045669\n",
      "EarlyStopping counter: 36 out of 50\n",
      "epoch: 76 - train_acc: 0.9693877551020408 - train_loss: 0.0841230446843131 - val_acc: 0.6785714285714286 - val_loss: 0.9705937830688763\n",
      "EarlyStopping counter: 37 out of 50\n",
      "epoch: 77 - train_acc: 0.9693877551020408 - train_loss: 0.08604206613784669 - val_acc: 0.6785714285714286 - val_loss: 0.521760946016753\n",
      "EarlyStopping counter: 38 out of 50\n",
      "epoch: 78 - train_acc: 0.9821428571428571 - train_loss: 0.0570857143141163 - val_acc: 0.75 - val_loss: 0.6925503515939153\n",
      "EarlyStopping counter: 39 out of 50\n",
      "epoch: 79 - train_acc: 0.9719387755102041 - train_loss: 0.0831601238721441 - val_acc: 0.75 - val_loss: 0.8043353659887092\n",
      "EarlyStopping counter: 40 out of 50\n",
      "epoch: 80 - train_acc: 0.9821428571428571 - train_loss: 0.05605519493534795 - val_acc: 0.7142857142857143 - val_loss: 0.8249290943374898\n",
      "EarlyStopping counter: 41 out of 50\n",
      "epoch: 81 - train_acc: 1.0 - train_loss: 0.019473675486461513 - val_acc: 0.75 - val_loss: 0.6987850105406834\n",
      "EarlyStopping counter: 42 out of 50\n",
      "epoch: 82 - train_acc: 0.9744897959183674 - train_loss: 0.050038672403360296 - val_acc: 0.7857142857142857 - val_loss: 0.8971345573608822\n",
      "EarlyStopping counter: 43 out of 50\n",
      "epoch: 83 - train_acc: 0.9974489795918368 - train_loss: 0.029303736914360452 - val_acc: 0.7142857142857143 - val_loss: 0.8330601735546586\n",
      "EarlyStopping counter: 44 out of 50\n",
      "epoch: 84 - train_acc: 0.9744897959183674 - train_loss: 0.08708144691789696 - val_acc: 0.75 - val_loss: 0.9342851690982471\n",
      "EarlyStopping counter: 45 out of 50\n",
      "epoch: 85 - train_acc: 0.9719387755102041 - train_loss: 0.07230246511676318 - val_acc: 0.7857142857142857 - val_loss: 0.8674572729015089\n",
      "EarlyStopping counter: 46 out of 50\n",
      "epoch: 86 - train_acc: 0.9795918367346939 - train_loss: 0.13940207768817814 - val_acc: 0.7857142857142857 - val_loss: 0.6587134070467857\n",
      "EarlyStopping counter: 47 out of 50\n",
      "epoch: 87 - train_acc: 0.9489795918367347 - train_loss: 0.16567287573843456 - val_acc: 0.6785714285714286 - val_loss: 0.7903156889530901\n",
      "EarlyStopping counter: 48 out of 50\n",
      "epoch: 88 - train_acc: 0.9846938775510204 - train_loss: 0.04881106047769302 - val_acc: 0.75 - val_loss: 0.6778766392419109\n",
      "EarlyStopping counter: 49 out of 50\n",
      "epoch: 89 - train_acc: 0.9846938775510204 - train_loss: 0.04974624096410023 - val_acc: 0.7857142857142857 - val_loss: 0.7924474073355492\n",
      "EarlyStopping counter: 50 out of 50\n",
      "epoch: 90 - train_acc: 0.9872448979591837 - train_loss: 0.03626117237540478 - val_acc: 0.6785714285714286 - val_loss: 0.833328538171938\n",
      "=> Early stopped !!\n",
      "[total]\n",
      "Accuracy: 0.75\n",
      "Precision: 0.7445255474452555\n",
      "Recall: 0.8360655737704918\n",
      "F1: 0.7876447876447877\n",
      "[low]\n",
      "Accuracy: 0.7447916666666666\n",
      "Precision: 0.7413793103448276\n",
      "Recall: 0.819047619047619\n",
      "F1: 0.7782805429864253\n",
      "[high]\n",
      "Accuracy: 0.75\n",
      "Precision: 0.6428571428571429\n",
      "Recall: 1.0\n",
      "F1: 0.782608695652174\n",
      "\n",
      "[Fold 13]: \n",
      "len(train_sampler)=392\n",
      "len(val_sampler)=28\n",
      "[ 13  48  50  54  58  88 134 166 169 174 187 189 235 241 243 252 264 273\n",
      " 306 315 319 328 344 345 363 403 413 417]\n",
      "Validation loss decreased (inf --> 0.902319).  Saving model ...\n",
      "epoch: 0 - train_acc: 0.5969387755102041 - train_loss: 0.6546431091949155 - val_acc: 0.4642857142857143 - val_loss: 0.9023192858736123\n",
      "Validation loss decreased (0.902319 --> 0.871159).  Saving model ...\n",
      "epoch: 1 - train_acc: 0.6811224489795918 - train_loss: 0.5991813176166293 - val_acc: 0.5 - val_loss: 0.8711588661049214\n",
      "Validation loss decreased (0.871159 --> 0.609414).  Saving model ...\n",
      "epoch: 2 - train_acc: 0.6836734693877551 - train_loss: 0.5796016667598305 - val_acc: 0.6785714285714286 - val_loss: 0.6094136029340335\n",
      "Validation loss decreased (0.609414 --> 0.520276).  Saving model ...\n",
      "epoch: 3 - train_acc: 0.75 - train_loss: 0.5423668053961108 - val_acc: 0.7857142857142857 - val_loss: 0.5202761186791773\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 4 - train_acc: 0.7882653061224489 - train_loss: 0.5217095292555617 - val_acc: 0.6785714285714286 - val_loss: 0.6454687535868353\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 5 - train_acc: 0.7627551020408163 - train_loss: 0.5181920238870331 - val_acc: 0.7142857142857143 - val_loss: 0.6635503285181137\n",
      "EarlyStopping counter: 3 out of 50\n",
      "epoch: 6 - train_acc: 0.7576530612244898 - train_loss: 0.49295525632734605 - val_acc: 0.5714285714285714 - val_loss: 0.8027899397669693\n",
      "EarlyStopping counter: 4 out of 50\n",
      "epoch: 7 - train_acc: 0.7882653061224489 - train_loss: 0.4452411436978265 - val_acc: 0.5714285714285714 - val_loss: 0.9120659171233834\n",
      "EarlyStopping counter: 5 out of 50\n",
      "epoch: 8 - train_acc: 0.7933673469387755 - train_loss: 0.4519589595650405 - val_acc: 0.7142857142857143 - val_loss: 0.5493762463978783\n",
      "EarlyStopping counter: 6 out of 50\n",
      "epoch: 9 - train_acc: 0.8163265306122449 - train_loss: 0.40548298794753823 - val_acc: 0.5714285714285714 - val_loss: 0.7905558443560059\n",
      "EarlyStopping counter: 7 out of 50\n",
      "epoch: 10 - train_acc: 0.7959183673469388 - train_loss: 0.43400075268997335 - val_acc: 0.6428571428571429 - val_loss: 0.5841048525800663\n",
      "EarlyStopping counter: 8 out of 50\n",
      "epoch: 11 - train_acc: 0.826530612244898 - train_loss: 0.3813842865420087 - val_acc: 0.6785714285714286 - val_loss: 0.6309646256543908\n",
      "EarlyStopping counter: 9 out of 50\n",
      "epoch: 12 - train_acc: 0.8418367346938775 - train_loss: 0.36672790942139594 - val_acc: 0.75 - val_loss: 0.5400985979813153\n",
      "EarlyStopping counter: 10 out of 50\n",
      "epoch: 13 - train_acc: 0.8596938775510204 - train_loss: 0.3349056557159723 - val_acc: 0.5357142857142857 - val_loss: 0.7111735832290327\n",
      "EarlyStopping counter: 11 out of 50\n",
      "epoch: 14 - train_acc: 0.875 - train_loss: 0.32092931088309107 - val_acc: 0.5714285714285714 - val_loss: 0.6893371854059165\n",
      "EarlyStopping counter: 12 out of 50\n",
      "epoch: 15 - train_acc: 0.8596938775510204 - train_loss: 0.3058732997976288 - val_acc: 0.6071428571428571 - val_loss: 0.8731746898504894\n",
      "EarlyStopping counter: 13 out of 50\n",
      "epoch: 16 - train_acc: 0.8903061224489796 - train_loss: 0.29917046628241745 - val_acc: 0.6785714285714286 - val_loss: 0.6179568680579915\n",
      "Validation loss decreased (0.520276 --> 0.483548).  Saving model ...\n",
      "epoch: 17 - train_acc: 0.8622448979591837 - train_loss: 0.28985294744800305 - val_acc: 0.7857142857142857 - val_loss: 0.48354781304974936\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 18 - train_acc: 0.8903061224489796 - train_loss: 0.27329909360325755 - val_acc: 0.6428571428571429 - val_loss: 0.9838376242697129\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 19 - train_acc: 0.9107142857142857 - train_loss: 0.25028477366387675 - val_acc: 0.7857142857142857 - val_loss: 0.6854771983048167\n",
      "EarlyStopping counter: 3 out of 50\n",
      "epoch: 20 - train_acc: 0.8979591836734694 - train_loss: 0.2403092357116626 - val_acc: 0.7142857142857143 - val_loss: 0.6051893975074571\n",
      "EarlyStopping counter: 4 out of 50\n",
      "epoch: 21 - train_acc: 0.9107142857142857 - train_loss: 0.21141030019639792 - val_acc: 0.7142857142857143 - val_loss: 0.6580211053331348\n",
      "EarlyStopping counter: 5 out of 50\n",
      "epoch: 22 - train_acc: 0.9005102040816326 - train_loss: 0.24988372034937698 - val_acc: 0.6785714285714286 - val_loss: 0.7766380582942014\n",
      "EarlyStopping counter: 6 out of 50\n",
      "epoch: 23 - train_acc: 0.9005102040816326 - train_loss: 0.2224971704910944 - val_acc: 0.6785714285714286 - val_loss: 0.8532553400133615\n",
      "EarlyStopping counter: 7 out of 50\n",
      "epoch: 24 - train_acc: 0.8903061224489796 - train_loss: 0.25538984858677616 - val_acc: 0.6071428571428571 - val_loss: 0.7221104438560036\n",
      "EarlyStopping counter: 8 out of 50\n",
      "epoch: 25 - train_acc: 0.8928571428571429 - train_loss: 0.22211781927430008 - val_acc: 0.75 - val_loss: 0.6501620158099828\n",
      "EarlyStopping counter: 9 out of 50\n",
      "epoch: 26 - train_acc: 0.9260204081632653 - train_loss: 0.21707363223254728 - val_acc: 0.75 - val_loss: 0.527596227058846\n",
      "EarlyStopping counter: 10 out of 50\n",
      "epoch: 27 - train_acc: 0.9209183673469388 - train_loss: 0.19202653068409534 - val_acc: 0.75 - val_loss: 0.6052821327734563\n",
      "Validation loss decreased (0.483548 --> 0.399929).  Saving model ...\n",
      "epoch: 28 - train_acc: 0.9336734693877551 - train_loss: 0.18049237744220858 - val_acc: 0.8214285714285714 - val_loss: 0.3999291994118904\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 29 - train_acc: 0.9209183673469388 - train_loss: 0.1879502296519006 - val_acc: 0.75 - val_loss: 0.5998790011524657\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 30 - train_acc: 0.9209183673469388 - train_loss: 0.18289525915990337 - val_acc: 0.7857142857142857 - val_loss: 0.6638645051385668\n",
      "EarlyStopping counter: 3 out of 50\n",
      "epoch: 31 - train_acc: 0.9081632653061225 - train_loss: 0.21927752299837674 - val_acc: 0.5714285714285714 - val_loss: 1.6492063972917304\n",
      "EarlyStopping counter: 4 out of 50\n",
      "epoch: 32 - train_acc: 0.9260204081632653 - train_loss: 0.18513686254985876 - val_acc: 0.75 - val_loss: 0.9565092173435756\n",
      "EarlyStopping counter: 5 out of 50\n",
      "epoch: 33 - train_acc: 0.9056122448979592 - train_loss: 0.21684544072757103 - val_acc: 0.6785714285714286 - val_loss: 1.221181687488935\n",
      "EarlyStopping counter: 6 out of 50\n",
      "epoch: 34 - train_acc: 0.9311224489795918 - train_loss: 0.1903480357714814 - val_acc: 0.7857142857142857 - val_loss: 0.49695780440292237\n",
      "EarlyStopping counter: 7 out of 50\n",
      "epoch: 35 - train_acc: 0.9642857142857143 - train_loss: 0.12299530675845995 - val_acc: 0.7142857142857143 - val_loss: 1.0425407673974978\n",
      "EarlyStopping counter: 8 out of 50\n",
      "epoch: 36 - train_acc: 0.9591836734693877 - train_loss: 0.12335277784560769 - val_acc: 0.75 - val_loss: 0.6424745587617358\n",
      "EarlyStopping counter: 9 out of 50\n",
      "epoch: 37 - train_acc: 0.9362244897959183 - train_loss: 0.1536366394148685 - val_acc: 0.6071428571428571 - val_loss: 1.2996485673611504\n",
      "EarlyStopping counter: 10 out of 50\n",
      "epoch: 38 - train_acc: 0.9464285714285714 - train_loss: 0.16436024699794047 - val_acc: 0.7857142857142857 - val_loss: 0.7846363732776878\n",
      "EarlyStopping counter: 11 out of 50\n",
      "epoch: 39 - train_acc: 0.9591836734693877 - train_loss: 0.11220437393623468 - val_acc: 0.8571428571428571 - val_loss: 0.5802688153188451\n",
      "EarlyStopping counter: 12 out of 50\n",
      "epoch: 40 - train_acc: 0.9795918367346939 - train_loss: 0.0887648649079032 - val_acc: 0.6428571428571429 - val_loss: 0.9032895427675813\n",
      "EarlyStopping counter: 13 out of 50\n",
      "epoch: 41 - train_acc: 0.9719387755102041 - train_loss: 0.09614573966647416 - val_acc: 0.7142857142857143 - val_loss: 0.7956156255975949\n",
      "EarlyStopping counter: 14 out of 50\n",
      "epoch: 42 - train_acc: 0.9642857142857143 - train_loss: 0.09859650637708695 - val_acc: 0.75 - val_loss: 0.7314065481395754\n",
      "EarlyStopping counter: 15 out of 50\n",
      "epoch: 43 - train_acc: 0.9872448979591837 - train_loss: 0.06566734159878454 - val_acc: 0.6428571428571429 - val_loss: 0.7996329480772035\n",
      "EarlyStopping counter: 16 out of 50\n",
      "epoch: 44 - train_acc: 0.9668367346938775 - train_loss: 0.10694102551381873 - val_acc: 0.6428571428571429 - val_loss: 0.7521754674747948\n",
      "EarlyStopping counter: 17 out of 50\n",
      "epoch: 45 - train_acc: 0.951530612244898 - train_loss: 0.12076752408795297 - val_acc: 0.75 - val_loss: 0.9624975432456394\n",
      "EarlyStopping counter: 18 out of 50\n",
      "epoch: 46 - train_acc: 0.9617346938775511 - train_loss: 0.09983725951189959 - val_acc: 0.6785714285714286 - val_loss: 0.9313744670641898\n",
      "EarlyStopping counter: 19 out of 50\n",
      "epoch: 47 - train_acc: 0.9770408163265306 - train_loss: 0.08486156291312344 - val_acc: 0.7142857142857143 - val_loss: 1.342866786411498\n",
      "EarlyStopping counter: 20 out of 50\n",
      "epoch: 48 - train_acc: 0.9591836734693877 - train_loss: 0.09536234168883673 - val_acc: 0.7142857142857143 - val_loss: 1.1113720224553845\n",
      "EarlyStopping counter: 21 out of 50\n",
      "epoch: 49 - train_acc: 0.9744897959183674 - train_loss: 0.0804005988615265 - val_acc: 0.75 - val_loss: 0.9884876839115058\n",
      "EarlyStopping counter: 22 out of 50\n",
      "epoch: 50 - train_acc: 0.9642857142857143 - train_loss: 0.11216823684141032 - val_acc: 0.6428571428571429 - val_loss: 0.9631131239181785\n",
      "EarlyStopping counter: 23 out of 50\n",
      "epoch: 51 - train_acc: 0.9617346938775511 - train_loss: 0.11069588850635041 - val_acc: 0.75 - val_loss: 0.8541213443040288\n",
      "EarlyStopping counter: 24 out of 50\n",
      "epoch: 52 - train_acc: 0.9540816326530612 - train_loss: 0.1413910656049152 - val_acc: 0.6785714285714286 - val_loss: 1.3157011605917854\n",
      "EarlyStopping counter: 25 out of 50\n",
      "epoch: 53 - train_acc: 0.9464285714285714 - train_loss: 0.12832335160745778 - val_acc: 0.75 - val_loss: 1.1231432594090296\n",
      "EarlyStopping counter: 26 out of 50\n",
      "epoch: 54 - train_acc: 0.9617346938775511 - train_loss: 0.09851346537787228 - val_acc: 0.5714285714285714 - val_loss: 1.069431831238772\n",
      "EarlyStopping counter: 27 out of 50\n",
      "epoch: 55 - train_acc: 0.9591836734693877 - train_loss: 0.12214949960689125 - val_acc: 0.75 - val_loss: 0.8705002596231377\n",
      "EarlyStopping counter: 28 out of 50\n",
      "epoch: 56 - train_acc: 0.9311224489795918 - train_loss: 0.15985694413194768 - val_acc: 0.7142857142857143 - val_loss: 0.8332726614900943\n",
      "EarlyStopping counter: 29 out of 50\n",
      "epoch: 57 - train_acc: 0.9770408163265306 - train_loss: 0.10299951250573473 - val_acc: 0.75 - val_loss: 0.8379616566287278\n",
      "EarlyStopping counter: 30 out of 50\n",
      "epoch: 58 - train_acc: 0.9693877551020408 - train_loss: 0.08684902346686543 - val_acc: 0.7142857142857143 - val_loss: 1.0882800459525734\n",
      "EarlyStopping counter: 31 out of 50\n",
      "epoch: 59 - train_acc: 0.9668367346938775 - train_loss: 0.09242265074046385 - val_acc: 0.75 - val_loss: 0.9633124657693555\n",
      "EarlyStopping counter: 32 out of 50\n",
      "epoch: 60 - train_acc: 0.9693877551020408 - train_loss: 0.10549982325486365 - val_acc: 0.6785714285714286 - val_loss: 1.2537594950663922\n",
      "EarlyStopping counter: 33 out of 50\n",
      "epoch: 61 - train_acc: 0.9617346938775511 - train_loss: 0.09918045300103712 - val_acc: 0.6785714285714286 - val_loss: 1.2474483815809434\n",
      "EarlyStopping counter: 34 out of 50\n",
      "epoch: 62 - train_acc: 0.9668367346938775 - train_loss: 0.09027314317784729 - val_acc: 0.75 - val_loss: 0.9263413670609045\n",
      "EarlyStopping counter: 35 out of 50\n",
      "epoch: 63 - train_acc: 0.951530612244898 - train_loss: 0.10765247973301634 - val_acc: 0.75 - val_loss: 0.8551100748558793\n",
      "EarlyStopping counter: 36 out of 50\n",
      "epoch: 64 - train_acc: 0.9693877551020408 - train_loss: 0.07385014113966556 - val_acc: 0.7142857142857143 - val_loss: 1.4839346976849586\n",
      "EarlyStopping counter: 37 out of 50\n",
      "epoch: 65 - train_acc: 0.9719387755102041 - train_loss: 0.08195059963186878 - val_acc: 0.6785714285714286 - val_loss: 0.9255923311314138\n",
      "EarlyStopping counter: 38 out of 50\n",
      "epoch: 66 - train_acc: 0.9821428571428571 - train_loss: 0.06250360290834046 - val_acc: 0.7857142857142857 - val_loss: 0.7967597620999691\n",
      "EarlyStopping counter: 39 out of 50\n",
      "epoch: 67 - train_acc: 0.9668367346938775 - train_loss: 0.10078771153936587 - val_acc: 0.6071428571428571 - val_loss: 1.3558822880635315\n",
      "EarlyStopping counter: 40 out of 50\n",
      "epoch: 68 - train_acc: 0.9846938775510204 - train_loss: 0.07277184585912597 - val_acc: 0.7857142857142857 - val_loss: 0.8322805216306359\n",
      "EarlyStopping counter: 41 out of 50\n",
      "epoch: 69 - train_acc: 0.9617346938775511 - train_loss: 0.11337578288224275 - val_acc: 0.6428571428571429 - val_loss: 0.7401076572547207\n",
      "EarlyStopping counter: 42 out of 50\n",
      "epoch: 70 - train_acc: 0.9617346938775511 - train_loss: 0.1575509440286254 - val_acc: 0.7142857142857143 - val_loss: 0.7108142586506624\n",
      "EarlyStopping counter: 43 out of 50\n",
      "epoch: 71 - train_acc: 0.9642857142857143 - train_loss: 0.09371011625265636 - val_acc: 0.7857142857142857 - val_loss: 0.7334661262809878\n",
      "EarlyStopping counter: 44 out of 50\n",
      "epoch: 72 - train_acc: 0.9821428571428571 - train_loss: 0.07139172384196672 - val_acc: 0.6785714285714286 - val_loss: 0.917493825690463\n",
      "EarlyStopping counter: 45 out of 50\n",
      "epoch: 73 - train_acc: 0.9668367346938775 - train_loss: 0.08352364663454179 - val_acc: 0.7142857142857143 - val_loss: 0.6104509377818936\n",
      "EarlyStopping counter: 46 out of 50\n",
      "epoch: 74 - train_acc: 0.9846938775510204 - train_loss: 0.06999998304607633 - val_acc: 0.6785714285714286 - val_loss: 0.9527256711241844\n",
      "EarlyStopping counter: 47 out of 50\n",
      "epoch: 75 - train_acc: 0.9693877551020408 - train_loss: 0.09610096255413383 - val_acc: 0.6428571428571429 - val_loss: 0.8407371739934135\n",
      "EarlyStopping counter: 48 out of 50\n",
      "epoch: 76 - train_acc: 0.9719387755102041 - train_loss: 0.07549743406760051 - val_acc: 0.7857142857142857 - val_loss: 0.7524764752994172\n",
      "EarlyStopping counter: 49 out of 50\n",
      "epoch: 77 - train_acc: 0.9744897959183674 - train_loss: 0.06142318639776935 - val_acc: 0.7857142857142857 - val_loss: 0.7081388759556376\n",
      "EarlyStopping counter: 50 out of 50\n",
      "epoch: 78 - train_acc: 0.9948979591836735 - train_loss: 0.027030691433626537 - val_acc: 0.75 - val_loss: 0.8221818623058225\n",
      "=> Early stopped !!\n",
      "[total]\n",
      "Accuracy: 0.7\n",
      "Precision: 0.8181818181818182\n",
      "Recall: 0.5901639344262295\n",
      "F1: 0.6857142857142857\n",
      "[low]\n",
      "Accuracy: 0.6822916666666666\n",
      "Precision: 0.8055555555555556\n",
      "Recall: 0.5523809523809524\n",
      "F1: 0.655367231638418\n",
      "[high]\n",
      "Accuracy: 0.8\n",
      "Precision: 0.7777777777777778\n",
      "Recall: 0.7777777777777778\n",
      "F1: 0.7777777777777779\n",
      "\n",
      "[Fold 14]: \n",
      "len(train_sampler)=392\n",
      "len(val_sampler)=28\n",
      "[ 20  21  71  87  99 102 106 121 130 149 151 160 188 191 214 257 270 276\n",
      " 293 308 313 330 343 348 359 372 385 419]\n",
      "Validation loss decreased (inf --> 0.705085).  Saving model ...\n",
      "epoch: 0 - train_acc: 0.5280612244897959 - train_loss: 0.688504423820865 - val_acc: 0.4642857142857143 - val_loss: 0.7050850825775459\n",
      "Validation loss decreased (0.705085 --> 0.700679).  Saving model ...\n",
      "epoch: 1 - train_acc: 0.6530612244897959 - train_loss: 0.6329265464067254 - val_acc: 0.6071428571428571 - val_loss: 0.7006792361043576\n",
      "Validation loss decreased (0.700679 --> 0.653140).  Saving model ...\n",
      "epoch: 2 - train_acc: 0.6836734693877551 - train_loss: 0.5978067488439958 - val_acc: 0.5357142857142857 - val_loss: 0.6531395104397313\n",
      "Validation loss decreased (0.653140 --> 0.574694).  Saving model ...\n",
      "epoch: 3 - train_acc: 0.673469387755102 - train_loss: 0.5856254766290573 - val_acc: 0.6428571428571429 - val_loss: 0.5746941059102058\n",
      "Validation loss decreased (0.574694 --> 0.516982).  Saving model ...\n",
      "epoch: 4 - train_acc: 0.7219387755102041 - train_loss: 0.5547305379780214 - val_acc: 0.6785714285714286 - val_loss: 0.51698195308023\n",
      "Validation loss decreased (0.516982 --> 0.463045).  Saving model ...\n",
      "epoch: 5 - train_acc: 0.7372448979591837 - train_loss: 0.5190182109870086 - val_acc: 0.7142857142857143 - val_loss: 0.46304507397106687\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 6 - train_acc: 0.7525510204081632 - train_loss: 0.5183943877074939 - val_acc: 0.75 - val_loss: 0.4894973385911594\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 7 - train_acc: 0.7295918367346939 - train_loss: 0.5254200015688988 - val_acc: 0.7142857142857143 - val_loss: 0.4959078364378364\n",
      "Validation loss decreased (0.463045 --> 0.446857).  Saving model ...\n",
      "epoch: 8 - train_acc: 0.7857142857142857 - train_loss: 0.4864556377971128 - val_acc: 0.8214285714285714 - val_loss: 0.4468569628862494\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 9 - train_acc: 0.7831632653061225 - train_loss: 0.4577102455939841 - val_acc: 0.5714285714285714 - val_loss: 0.8257604862784786\n",
      "Validation loss decreased (0.446857 --> 0.384503).  Saving model ...\n",
      "epoch: 10 - train_acc: 0.7551020408163265 - train_loss: 0.501554124907477 - val_acc: 0.8214285714285714 - val_loss: 0.3845030979761227\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 11 - train_acc: 0.7933673469387755 - train_loss: 0.44019406788666715 - val_acc: 0.8214285714285714 - val_loss: 0.43874678899751496\n",
      "Validation loss decreased (0.384503 --> 0.362398).  Saving model ...\n",
      "epoch: 12 - train_acc: 0.8214285714285714 - train_loss: 0.4146290128057053 - val_acc: 0.75 - val_loss: 0.36239803403746906\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 13 - train_acc: 0.8367346938775511 - train_loss: 0.3920964805364654 - val_acc: 0.8214285714285714 - val_loss: 0.5561795548736965\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 14 - train_acc: 0.8443877551020408 - train_loss: 0.387905812390223 - val_acc: 0.7142857142857143 - val_loss: 0.525147003496563\n",
      "EarlyStopping counter: 3 out of 50\n",
      "epoch: 15 - train_acc: 0.8112244897959183 - train_loss: 0.398370788367697 - val_acc: 0.8571428571428571 - val_loss: 0.4012874736961019\n",
      "EarlyStopping counter: 4 out of 50\n",
      "epoch: 16 - train_acc: 0.8520408163265306 - train_loss: 0.31585753738364136 - val_acc: 0.8571428571428571 - val_loss: 0.37168260646553664\n",
      "EarlyStopping counter: 5 out of 50\n",
      "epoch: 17 - train_acc: 0.8520408163265306 - train_loss: 0.346519703583896 - val_acc: 0.7857142857142857 - val_loss: 0.6638563904078315\n",
      "Validation loss decreased (0.362398 --> 0.326768).  Saving model ...\n",
      "epoch: 18 - train_acc: 0.8775510204081632 - train_loss: 0.30114849202846317 - val_acc: 0.7857142857142857 - val_loss: 0.3267678633931419\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 19 - train_acc: 0.875 - train_loss: 0.2852994355708744 - val_acc: 0.8214285714285714 - val_loss: 0.4287620955048904\n",
      "Validation loss decreased (0.326768 --> 0.210880).  Saving model ...\n",
      "epoch: 20 - train_acc: 0.8698979591836735 - train_loss: 0.28885494225768266 - val_acc: 0.8928571428571429 - val_loss: 0.21088026653050665\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 21 - train_acc: 0.8852040816326531 - train_loss: 0.2851107147840486 - val_acc: 0.8571428571428571 - val_loss: 0.39864454651783154\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 22 - train_acc: 0.9056122448979592 - train_loss: 0.24986719074113722 - val_acc: 0.8928571428571429 - val_loss: 0.27770723789027735\n",
      "EarlyStopping counter: 3 out of 50\n",
      "epoch: 23 - train_acc: 0.9081632653061225 - train_loss: 0.23489617579117378 - val_acc: 0.75 - val_loss: 0.5966879633508837\n",
      "EarlyStopping counter: 4 out of 50\n",
      "epoch: 24 - train_acc: 0.8979591836734694 - train_loss: 0.2562804674343087 - val_acc: 0.8214285714285714 - val_loss: 0.34158475901576146\n",
      "EarlyStopping counter: 5 out of 50\n",
      "epoch: 25 - train_acc: 0.8954081632653061 - train_loss: 0.260280501196454 - val_acc: 0.9285714285714286 - val_loss: 0.2790989629189527\n",
      "EarlyStopping counter: 6 out of 50\n",
      "epoch: 26 - train_acc: 0.8392857142857143 - train_loss: 0.33986317483572465 - val_acc: 0.7857142857142857 - val_loss: 0.5685603152222037\n",
      "EarlyStopping counter: 7 out of 50\n",
      "epoch: 27 - train_acc: 0.9005102040816326 - train_loss: 0.2639959328566621 - val_acc: 0.8928571428571429 - val_loss: 0.27257042148881716\n",
      "EarlyStopping counter: 8 out of 50\n",
      "epoch: 28 - train_acc: 0.8852040816326531 - train_loss: 0.25915769671975775 - val_acc: 0.7857142857142857 - val_loss: 0.5943598761632263\n",
      "EarlyStopping counter: 9 out of 50\n",
      "epoch: 29 - train_acc: 0.9107142857142857 - train_loss: 0.23666317761015787 - val_acc: 0.8928571428571429 - val_loss: 0.29559838288960255\n",
      "EarlyStopping counter: 10 out of 50\n",
      "epoch: 30 - train_acc: 0.9285714285714286 - train_loss: 0.17839570851834044 - val_acc: 0.9285714285714286 - val_loss: 0.231583171345083\n",
      "EarlyStopping counter: 11 out of 50\n",
      "epoch: 31 - train_acc: 0.9387755102040817 - train_loss: 0.17836956071530935 - val_acc: 0.7857142857142857 - val_loss: 0.3626268409289094\n",
      "EarlyStopping counter: 12 out of 50\n",
      "epoch: 32 - train_acc: 0.9438775510204082 - train_loss: 0.15897918635392694 - val_acc: 0.8214285714285714 - val_loss: 0.42014625032265285\n",
      "EarlyStopping counter: 13 out of 50\n",
      "epoch: 33 - train_acc: 0.9489795918367347 - train_loss: 0.1433857375791172 - val_acc: 0.7857142857142857 - val_loss: 0.6419822143953857\n",
      "EarlyStopping counter: 14 out of 50\n",
      "epoch: 34 - train_acc: 0.9642857142857143 - train_loss: 0.12517014138749058 - val_acc: 0.8571428571428571 - val_loss: 0.37308190513439843\n",
      "Validation loss decreased (0.210880 --> 0.127948).  Saving model ...\n",
      "epoch: 35 - train_acc: 0.9438775510204082 - train_loss: 0.15489074841449427 - val_acc: 0.9642857142857143 - val_loss: 0.1279483002580987\n",
      "EarlyStopping counter: 1 out of 50\n",
      "epoch: 36 - train_acc: 0.9285714285714286 - train_loss: 0.16298441413471138 - val_acc: 0.8571428571428571 - val_loss: 0.27688834078679947\n",
      "EarlyStopping counter: 2 out of 50\n",
      "epoch: 37 - train_acc: 0.9362244897959183 - train_loss: 0.16923452464659144 - val_acc: 0.8571428571428571 - val_loss: 0.8215011566536001\n",
      "EarlyStopping counter: 3 out of 50\n",
      "epoch: 38 - train_acc: 0.9438775510204082 - train_loss: 0.15590895681082115 - val_acc: 0.9285714285714286 - val_loss: 0.1818403973779677\n",
      "EarlyStopping counter: 4 out of 50\n",
      "epoch: 39 - train_acc: 0.9489795918367347 - train_loss: 0.14487209787203578 - val_acc: 0.8571428571428571 - val_loss: 0.3548863288222446\n",
      "EarlyStopping counter: 5 out of 50\n",
      "epoch: 40 - train_acc: 0.9642857142857143 - train_loss: 0.1255151968961024 - val_acc: 0.9642857142857143 - val_loss: 0.16537928143962183\n",
      "EarlyStopping counter: 6 out of 50\n",
      "epoch: 41 - train_acc: 0.9566326530612245 - train_loss: 0.13551353499043028 - val_acc: 0.8928571428571429 - val_loss: 0.3731363529830514\n",
      "EarlyStopping counter: 7 out of 50\n",
      "epoch: 42 - train_acc: 0.9566326530612245 - train_loss: 0.11973944610935358 - val_acc: 0.8928571428571429 - val_loss: 0.3497189884365056\n",
      "EarlyStopping counter: 8 out of 50\n",
      "epoch: 43 - train_acc: 0.9438775510204082 - train_loss: 0.1421942877720692 - val_acc: 0.7857142857142857 - val_loss: 0.36798663356882455\n",
      "EarlyStopping counter: 9 out of 50\n",
      "epoch: 44 - train_acc: 0.9260204081632653 - train_loss: 0.17045730629412528 - val_acc: 0.7857142857142857 - val_loss: 0.5460717253221294\n",
      "EarlyStopping counter: 10 out of 50\n",
      "epoch: 45 - train_acc: 0.9464285714285714 - train_loss: 0.12033396722783743 - val_acc: 0.8928571428571429 - val_loss: 0.24775184370123038\n",
      "EarlyStopping counter: 11 out of 50\n",
      "epoch: 46 - train_acc: 0.9693877551020408 - train_loss: 0.09430004980843322 - val_acc: 0.8214285714285714 - val_loss: 0.3891240528519254\n",
      "EarlyStopping counter: 12 out of 50\n",
      "epoch: 47 - train_acc: 0.9438775510204082 - train_loss: 0.14599118005594877 - val_acc: 0.6785714285714286 - val_loss: 0.9002705115220122\n",
      "EarlyStopping counter: 13 out of 50\n",
      "epoch: 48 - train_acc: 0.9438775510204082 - train_loss: 0.16186702686398605 - val_acc: 0.8928571428571429 - val_loss: 0.19837846619532173\n",
      "EarlyStopping counter: 14 out of 50\n",
      "epoch: 49 - train_acc: 0.9642857142857143 - train_loss: 0.10099204209215105 - val_acc: 0.75 - val_loss: 0.7391699878674257\n",
      "EarlyStopping counter: 15 out of 50\n",
      "epoch: 50 - train_acc: 0.9591836734693877 - train_loss: 0.1045503950482595 - val_acc: 0.7857142857142857 - val_loss: 0.5481620531493941\n",
      "EarlyStopping counter: 16 out of 50\n",
      "epoch: 51 - train_acc: 0.9642857142857143 - train_loss: 0.1120991414965342 - val_acc: 0.8214285714285714 - val_loss: 0.511341880367872\n",
      "EarlyStopping counter: 17 out of 50\n",
      "epoch: 52 - train_acc: 0.9795918367346939 - train_loss: 0.08210162475989266 - val_acc: 0.8571428571428571 - val_loss: 0.3957331363817458\n",
      "EarlyStopping counter: 18 out of 50\n",
      "epoch: 53 - train_acc: 0.9821428571428571 - train_loss: 0.08774755796162285 - val_acc: 0.8214285714285714 - val_loss: 0.36680685843529026\n",
      "EarlyStopping counter: 19 out of 50\n",
      "epoch: 54 - train_acc: 0.9642857142857143 - train_loss: 0.10588738579385794 - val_acc: 0.7857142857142857 - val_loss: 0.4301123950019492\n",
      "EarlyStopping counter: 20 out of 50\n",
      "epoch: 55 - train_acc: 0.9183673469387755 - train_loss: 0.16677186127358257 - val_acc: 0.8214285714285714 - val_loss: 0.9881225083076781\n",
      "EarlyStopping counter: 21 out of 50\n",
      "epoch: 56 - train_acc: 0.951530612244898 - train_loss: 0.11490880706843151 - val_acc: 0.8214285714285714 - val_loss: 0.49690674734425616\n",
      "EarlyStopping counter: 22 out of 50\n",
      "epoch: 57 - train_acc: 0.9795918367346939 - train_loss: 0.07027609422114071 - val_acc: 0.8571428571428571 - val_loss: 0.3066932956080039\n",
      "EarlyStopping counter: 23 out of 50\n",
      "epoch: 58 - train_acc: 0.9948979591836735 - train_loss: 0.03603789818859634 - val_acc: 0.8928571428571429 - val_loss: 0.32506308204269063\n",
      "EarlyStopping counter: 24 out of 50\n",
      "epoch: 59 - train_acc: 0.9770408163265306 - train_loss: 0.06348891876584595 - val_acc: 0.8571428571428571 - val_loss: 0.40147231711813625\n",
      "EarlyStopping counter: 25 out of 50\n",
      "epoch: 60 - train_acc: 0.9617346938775511 - train_loss: 0.09521199029641692 - val_acc: 0.8928571428571429 - val_loss: 0.36682837468807283\n",
      "EarlyStopping counter: 26 out of 50\n",
      "epoch: 61 - train_acc: 0.9668367346938775 - train_loss: 0.08584205737544166 - val_acc: 0.8571428571428571 - val_loss: 0.45540551497851733\n",
      "EarlyStopping counter: 27 out of 50\n",
      "epoch: 62 - train_acc: 0.9591836734693877 - train_loss: 0.10828896586490999 - val_acc: 0.7857142857142857 - val_loss: 0.46089884343814175\n",
      "EarlyStopping counter: 28 out of 50\n",
      "epoch: 63 - train_acc: 0.9770408163265306 - train_loss: 0.05754236251802154 - val_acc: 0.7857142857142857 - val_loss: 0.4035548238769749\n",
      "EarlyStopping counter: 29 out of 50\n",
      "epoch: 64 - train_acc: 0.9846938775510204 - train_loss: 0.06354790613129706 - val_acc: 0.8571428571428571 - val_loss: 0.29919108009888884\n",
      "EarlyStopping counter: 30 out of 50\n",
      "epoch: 65 - train_acc: 0.9795918367346939 - train_loss: 0.06570213791259763 - val_acc: 0.8214285714285714 - val_loss: 0.5482671329893978\n",
      "EarlyStopping counter: 31 out of 50\n",
      "epoch: 66 - train_acc: 0.9566326530612245 - train_loss: 0.11430809924214577 - val_acc: 0.8214285714285714 - val_loss: 0.46204123977763345\n",
      "EarlyStopping counter: 32 out of 50\n",
      "epoch: 67 - train_acc: 0.9566326530612245 - train_loss: 0.1294170415548808 - val_acc: 0.8928571428571429 - val_loss: 0.20888564695323503\n",
      "EarlyStopping counter: 33 out of 50\n",
      "epoch: 68 - train_acc: 0.9617346938775511 - train_loss: 0.10621726777610598 - val_acc: 0.8214285714285714 - val_loss: 0.5690480725354421\n",
      "EarlyStopping counter: 34 out of 50\n",
      "epoch: 69 - train_acc: 0.9693877551020408 - train_loss: 0.09230138799367858 - val_acc: 0.8571428571428571 - val_loss: 0.5794826489243663\n",
      "EarlyStopping counter: 35 out of 50\n",
      "epoch: 70 - train_acc: 0.9795918367346939 - train_loss: 0.08412454780780335 - val_acc: 0.8571428571428571 - val_loss: 0.631932983035496\n",
      "EarlyStopping counter: 36 out of 50\n",
      "epoch: 71 - train_acc: 0.9846938775510204 - train_loss: 0.05662392386024123 - val_acc: 0.8928571428571429 - val_loss: 0.3834708890433187\n",
      "EarlyStopping counter: 37 out of 50\n",
      "epoch: 72 - train_acc: 0.9846938775510204 - train_loss: 0.05374710651737749 - val_acc: 0.8214285714285714 - val_loss: 0.4338037565001326\n",
      "EarlyStopping counter: 38 out of 50\n",
      "epoch: 73 - train_acc: 0.9897959183673469 - train_loss: 0.03926379384336308 - val_acc: 0.8928571428571429 - val_loss: 0.35069205547707305\n",
      "EarlyStopping counter: 39 out of 50\n",
      "epoch: 74 - train_acc: 0.9872448979591837 - train_loss: 0.04256025186844677 - val_acc: 0.8214285714285714 - val_loss: 0.3942047020362265\n",
      "EarlyStopping counter: 40 out of 50\n",
      "epoch: 75 - train_acc: 0.9693877551020408 - train_loss: 0.07019554703521975 - val_acc: 0.8214285714285714 - val_loss: 0.5758321888813377\n",
      "EarlyStopping counter: 41 out of 50\n",
      "epoch: 76 - train_acc: 0.9744897959183674 - train_loss: 0.056664154289668894 - val_acc: 0.9285714285714286 - val_loss: 0.29253624073440937\n",
      "EarlyStopping counter: 42 out of 50\n",
      "epoch: 77 - train_acc: 0.9795918367346939 - train_loss: 0.0500168231808084 - val_acc: 0.8571428571428571 - val_loss: 0.5780011764382895\n",
      "EarlyStopping counter: 43 out of 50\n",
      "epoch: 78 - train_acc: 0.9846938775510204 - train_loss: 0.057384160846676927 - val_acc: 0.7857142857142857 - val_loss: 0.5844529671506378\n",
      "EarlyStopping counter: 44 out of 50\n",
      "epoch: 79 - train_acc: 0.9668367346938775 - train_loss: 0.09024589587369457 - val_acc: 0.8928571428571429 - val_loss: 0.316851541405984\n",
      "EarlyStopping counter: 45 out of 50\n",
      "epoch: 80 - train_acc: 0.9693877551020408 - train_loss: 0.08464962508727733 - val_acc: 0.8214285714285714 - val_loss: 0.45022093861408646\n",
      "EarlyStopping counter: 46 out of 50\n",
      "epoch: 81 - train_acc: 0.9770408163265306 - train_loss: 0.0877292234159281 - val_acc: 0.75 - val_loss: 0.9386513710585037\n",
      "EarlyStopping counter: 47 out of 50\n",
      "epoch: 82 - train_acc: 0.9693877551020408 - train_loss: 0.10448008955409038 - val_acc: 0.6785714285714286 - val_loss: 1.1102076841801614\n",
      "EarlyStopping counter: 48 out of 50\n",
      "epoch: 83 - train_acc: 0.9795918367346939 - train_loss: 0.09626666455426823 - val_acc: 0.8571428571428571 - val_loss: 0.8236810372029363\n",
      "EarlyStopping counter: 49 out of 50\n",
      "epoch: 84 - train_acc: 0.9846938775510204 - train_loss: 0.05347384512913724 - val_acc: 0.7857142857142857 - val_loss: 0.8674852516185327\n",
      "EarlyStopping counter: 50 out of 50\n",
      "epoch: 85 - train_acc: 0.9872448979591837 - train_loss: 0.0462337226039003 - val_acc: 0.8571428571428571 - val_loss: 0.7371662177983199\n",
      "=> Early stopped !!\n",
      "[total]\n",
      "Accuracy: 0.6772727272727272\n",
      "Precision: 0.7339449541284404\n",
      "Recall: 0.6557377049180327\n",
      "F1: 0.6926406926406926\n",
      "[low]\n",
      "Accuracy: 0.6614583333333334\n",
      "Precision: 0.717391304347826\n",
      "Recall: 0.6285714285714286\n",
      "F1: 0.6700507614213197\n",
      "[high]\n",
      "Accuracy: 0.8\n",
      "Precision: 0.7272727272727273\n",
      "Recall: 0.8888888888888888\n",
      "F1: 0.8\n"
     ]
    }
   ],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "                            Default: False\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.fname = 'checkpoints/P2B2_SeparableConv.pt'\n",
    "\n",
    "    def __call__(self, val_loss, model, fname):\n",
    "        \n",
    "        self.fname = fname\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.fname)\n",
    "        self.val_loss_min = val_loss\n",
    "\n",
    "\n",
    "\n",
    "def get_num_correct(preds_logits, labels):\n",
    "    return (preds_logits.argmax(dim=1) == labels).sum().item()\n",
    "\n",
    "def count_TP(preds_logits, labels):\n",
    "    preds = preds_logits.argmax(dim=1)\n",
    "    tmp = torch.add(preds, labels)\n",
    "    return (tmp == 2).sum().item()\n",
    "\n",
    "def count_TN(preds_logits, labels):\n",
    "    preds = preds_logits.argmax(dim=1)\n",
    "    tmp = torch.add(preds, labels)\n",
    "    return (tmp == 0).sum().item()\n",
    "\n",
    "def count_FP(preds_logits, labels):\n",
    "    preds = preds_logits.argmax(dim=1)\n",
    "    tmp = torch.subtract(preds, labels)\n",
    "    return (tmp == 1).sum().item()\n",
    "\n",
    "def count_FN(preds_logits, labels):\n",
    "    preds = preds_logits.argmax(dim=1)\n",
    "    tmp = torch.subtract(preds, labels)\n",
    "    return (tmp == -1).sum().item()\n",
    "\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def fixed_seed(seed_value):\n",
    "    # 1. Set `PYTHONHASHSEED` environment variable at a fixed value\n",
    "    os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "    \n",
    "    # 2. Set `python` built-in pseudo-random generator at a fixed value\n",
    "    random.seed(seed_value)\n",
    "\n",
    "    # 3. Set `numpy` pseudo-random generator at a fixed value\n",
    "    np.random.seed(seed_value)\n",
    "\n",
    "    # 4. Set `pytorch` pseudo-random generator at a fixed value\n",
    "    torch.manual_seed(seed_value)\n",
    "\n",
    "fixed_seed(42)\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "data = EyeBlinkDataset(train=True)\n",
    "data_test = EyeBlinkDataset(train=False)\n",
    "data_test_low = EyeBlinkDataset(train=False, use_threshold=-1)\n",
    "data_test_high = EyeBlinkDataset(train=False, use_threshold=1)\n",
    "\n",
    "test_results = []\n",
    "\n",
    "CASE = \"cubic-lanczos4\"\n",
    "\n",
    "kfold = KFold(n_splits=15, shuffle=True)\n",
    "for fold, (train_ids, val_ids) in enumerate(kfold.split(data)):\n",
    "    print(f'\\n[Fold {fold}]: ')\n",
    "    train_sampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
    "    val_sampler = torch.utils.data.SubsetRandomSampler(val_ids)\n",
    "    train_loader = torch.utils.data.DataLoader(data, batch_size=16, sampler=train_sampler, pin_memory=True)\n",
    "    val_loader = torch.utils.data.DataLoader(data, batch_size=16, sampler=val_sampler, pin_memory=True)\n",
    "    print(f'len(train_sampler)={len(train_sampler)}')\n",
    "    print(f'len(val_sampler)={len(val_sampler)}')\n",
    "    print(val_ids)\n",
    "\n",
    "    # model = P2B2().double()\n",
    "    model = P2B2_SeparableConv().double()\n",
    "    # model = P3B3().double()\n",
    "    # model = P3B3_SeparableConv().double()\n",
    "    model.to(device)\n",
    "\n",
    "    # loss_func = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "    min_loss = float('inf')\n",
    "    num_epochs = 500\n",
    "\n",
    "    early_stopping = EarlyStopping(patience=50, verbose=True)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        ## train set\n",
    "        avg_train_loss = 0\n",
    "        total_train_correct = 0\n",
    "        model.train()\n",
    "\n",
    "        for seqs, labels in train_loader:\n",
    "            seqs = seqs.permute((0,4,3,1,2)).to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            preds_logits = model(seqs)\n",
    "            \n",
    "            # print('---',seqs.shape)\n",
    "            # print(preds_logits.shape)\n",
    "            # print(labels.shape,'---')\n",
    "        \n",
    "            loss = F.cross_entropy(preds_logits, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            avg_train_loss += loss.detach().item() / len(train_loader)\n",
    "            total_train_correct += get_num_correct(preds_logits, labels)\n",
    "\n",
    "        ## val set\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            avg_val_loss = 0\n",
    "            total_val_correct = 0\n",
    "\n",
    "            for seqs, labels in val_loader:\n",
    "                seqs = seqs.permute((0,4,3,1,2)).to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                preds = model(seqs)\n",
    "                loss = F.cross_entropy(preds, labels)\n",
    "                avg_val_loss += loss.detach().item() / len(val_loader)\n",
    "                total_val_correct += get_num_correct(preds, labels) \n",
    "\n",
    "        early_stopping(avg_val_loss, model, f'checkpoints/preprocessing/{CASE}/P2B2_SeparableConv-fold_{fold}.pt')\n",
    "        # early_stopping(avg_val_loss, model, f'checkpoints/preprocessing/{CASE}/P2B2-fold_{fold}.pt')\n",
    "        # early_stopping(avg_val_loss, model, f'checkpoints/P3B3-fold_{fold}.pt')\n",
    "        # early_stopping(avg_val_loss, model, f'checkpoints/P3B3_SeparableConv-fold_{fold}.pt')\n",
    "\n",
    "        print(f'epoch: {epoch} - train_acc: {total_train_correct/len(train_sampler)} - train_loss: {avg_train_loss} - val_acc: {total_val_correct/len(val_sampler)} - val_loss: {avg_val_loss}')\n",
    "\n",
    "        if early_stopping.early_stop:\n",
    "            print('=> Early stopped !!')\n",
    "            break\n",
    "    \n",
    "\n",
    "    ## test set\n",
    "    with torch.no_grad():\n",
    "        data_test_loader = torch.utils.data.DataLoader(data_test, batch_size=16, shuffle=True, pin_memory=True)\n",
    "        data_test_low_loader = torch.utils.data.DataLoader(data_test_low, batch_size=16, shuffle=True, pin_memory=True)\n",
    "        data_test_high_loader = torch.utils.data.DataLoader(data_test_high, batch_size=16, shuffle=True, pin_memory=True)\n",
    "        \n",
    "        # model.load_state_dict(torch.load(f'checkpoints/preprocessing/{CASE}/P2B2-fold_{fold}.pt'))\n",
    "        model.load_state_dict(torch.load(f'checkpoints/preprocessing/{CASE}/P2B2_SeparableConv-fold_{fold}.pt'))\n",
    "        # model.load_state_dict(torch.load(f'checkpoints/P3B3-fold_{fold}.pt'))\n",
    "        # model.load_state_dict(torch.load(f'checkpoints/P3B3_SeparableConv-fold_{fold}.pt'))\n",
    "        model.eval()\n",
    "        \n",
    "        total_TP = 0; total_TN = 0; total_FP = 0; total_FN = 0\n",
    "\n",
    "        for seqs, labels in data_test_loader:\n",
    "            seqs = seqs.permute((0,4,3,1,2)).to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            preds = model(seqs)\n",
    "            total_TP += count_TP(preds, labels)\n",
    "            total_TN += count_TN(preds, labels)\n",
    "            total_FP += count_FP(preds, labels)\n",
    "            total_FN += count_FN(preds, labels)\n",
    "    \n",
    "        total_TP_low = 0; total_TN_low = 0; total_FP_low = 0; total_FN_low = 0\n",
    "\n",
    "        for seqs, labels in data_test_low_loader:\n",
    "            seqs = seqs.permute((0,4,3,1,2)).to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            preds = model(seqs)\n",
    "            total_TP_low += count_TP(preds, labels)\n",
    "            total_TN_low += count_TN(preds, labels)\n",
    "            total_FP_low += count_FP(preds, labels)\n",
    "            total_FN_low += count_FN(preds, labels)\n",
    "\n",
    "        total_TP_high = 0; total_TN_high = 0; total_FP_high = 0; total_FN_high = 0\n",
    "\n",
    "        for seqs, labels in data_test_high_loader:\n",
    "            seqs = seqs.permute((0,4,3,1,2)).to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            preds = model(seqs)\n",
    "            total_TP_high += count_TP(preds, labels)\n",
    "            total_TN_high += count_TN(preds, labels)\n",
    "            total_FP_high += count_FP(preds, labels)\n",
    "            total_FN_high += count_FN(preds, labels)\n",
    "\n",
    "    accuracy  = (total_TP + total_TN)  / len(data_test)\n",
    "    precision = total_TP / (total_TP + total_FP)\n",
    "    recall    = total_TP / (total_TP + total_FN)\n",
    "    F1        = 2 / ((1 / precision) + (1 / recall))\n",
    "    print(f\"[total]\\nAccuracy: {accuracy}\")\n",
    "    print(f'Precision: {precision}')\n",
    "    print(f'Recall: {recall}')\n",
    "    print(f'F1: {F1}')\n",
    "\n",
    "    accuracy_low  = (total_TP_low + total_TN_low)  / len(data_test_low)\n",
    "    precision_low = total_TP_low / (total_TP_low + total_FP_low)\n",
    "    recall_low    = total_TP_low / (total_TP_low + total_FN_low)\n",
    "    F1_low        = 2 / ((1 / precision_low) + (1 / recall_low))\n",
    "    print(f\"[low]\\nAccuracy: {accuracy_low}\")\n",
    "    print(f'Precision: {precision_low}')\n",
    "    print(f'Recall: {recall_low}')\n",
    "    print(f'F1: {F1_low}')\n",
    "\n",
    "    accuracy_high  = (total_TP_high + total_TN_high)  / len(data_test_high)\n",
    "    precision_high = total_TP_high / (total_TP_high + total_FP_high)\n",
    "    recall_high    = total_TP_high / (total_TP_high + total_FN_high)\n",
    "    F1_high        = 2 / ((1 / precision_high) + (1 / recall_high))\n",
    "    print(f\"[high]\\nAccuracy: {accuracy_high}\")\n",
    "    print(f'Precision: {precision_high}')\n",
    "    print(f'Recall: {recall_high}')\n",
    "    print(f'F1: {F1_high}')\n",
    "    \n",
    "    result = {'acc':accuracy, 'pre':precision, 'rec': recall, 'f1': F1, 'acc_low':accuracy_low, 'pre_low':precision_low, 'rec_low': recall_low, 'f1_low': F1_low, 'acc_high':accuracy_high, 'pre_high':precision_high, 'rec_high': recall_high, 'f1_high': F1_high}\n",
    "    test_results.append(result)\n",
    "\n",
    "import json\n",
    "\n",
    "with open(f'checkpoints/preprocessing/{CASE}/P2B2_SeparableConv-results.txt','w') as fp:\n",
    "# with open(f'checkpoints/preprocessing/{CASE}/P2B2-results.txt','w') as fp:\n",
    "# with open('checkpoints/P3B3-results.txt','w') as fp:\n",
    "# with open('checkpoints/P3B3_SeparableConv-results.txt','w') as fp:\n",
    "    fp.write(json.dumps(test_results))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['datasets/test/test/blink/4', 'datasets/test/test/blink/69', 'datasets/test/test/blink/74', 'datasets/test/test/blink/92', 'datasets/test/test/blink/94']\n",
      "220\n",
      "Accuracy: 0.740909090909091\n",
      "Precision: 0.7559055118110236\n",
      "Recall: 0.7868852459016393\n",
      "F1: 0.7710843373493976\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "def get_num_correct(preds_logits, labels):\n",
    "    return (preds_logits.argmax(dim=1) == labels).sum().item()\n",
    "\n",
    "def count_TP(preds_logits, labels):\n",
    "    preds = preds_logits.argmax(dim=1)\n",
    "    tmp = torch.add(preds, labels)\n",
    "    return (tmp == 2).sum().item()\n",
    "\n",
    "def count_TN(preds_logits, labels):\n",
    "    preds = preds_logits.argmax(dim=1)\n",
    "    tmp = torch.add(preds, labels)\n",
    "    return (tmp == 0).sum().item()\n",
    "\n",
    "def count_FP(preds_logits, labels):\n",
    "    preds = preds_logits.argmax(dim=1)\n",
    "    tmp = torch.subtract(preds, labels)\n",
    "    return (tmp == 1).sum().item()\n",
    "\n",
    "def count_FN(preds_logits, labels):\n",
    "    preds = preds_logits.argmax(dim=1)\n",
    "    tmp = torch.subtract(preds, labels)\n",
    "    return (tmp == -1).sum().item()\n",
    "\n",
    "num_epochs=200\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "data_test = EyeBlinkDataset(train=False)\n",
    "data_loader_test = torch.utils.data.DataLoader(data_test, batch_size=16, shuffle=True)\n",
    "\n",
    "print(len(data_test))\n",
    "\n",
    "model_ = torch.load(f'checkpoints/P2B2_SeparableConv-num_epochs_{num_epochs}.pt', map_location='cuda:0')\n",
    "# model_ = P2B2_SeparableConv().double()\n",
    "# model_.load_state_dict(torch.load(f'checkpoints/P2B2_SeparableConv.pt'))\n",
    "\n",
    "with torch.no_grad():\n",
    "    model_.eval()\n",
    "    model_.to(device)\n",
    "    \n",
    "    total_TP = 0\n",
    "    total_TN = 0\n",
    "    total_FP = 0\n",
    "    total_FN = 0\n",
    "\n",
    "    for seqs, labels in data_loader_test:\n",
    "        seqs = seqs.permute((0,4,3,1,2)).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        preds = model_(seqs)\n",
    "        total_TP += count_TP(preds, labels)\n",
    "        total_TN += count_TN(preds, labels)\n",
    "        total_FP += count_FP(preds, labels)\n",
    "        total_FN += count_FN(preds, labels)\n",
    "    \n",
    "    accuracy  = (total_TP + total_TN)  / len(data_test)\n",
    "    precision = total_TP / (total_TP + total_FP)\n",
    "    recall    = total_TP / (total_TP + total_FN)\n",
    "    F1        = 2 / ((1 / precision) + (1 / recall))\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f'Precision: {precision}')\n",
    "    print(f'Recall: {recall}')\n",
    "    print(f'F1: {F1}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['datasets/test/test/blink/105']\n",
      "224\n",
      "datasets/test/test/unblink/10\n",
      "datasets/test/test/blink/43\n",
      "datasets/test/test/blink/127\n",
      "datasets/test/test/unblink/98\n",
      "datasets/test/test/unblink/65\n",
      "datasets/test/test/blink/11\n",
      "datasets/test/test/blink/54\n",
      "datasets/test/test/blink/124\n",
      "datasets/test/test/blink/108\n",
      "datasets/test/test/blink/56\n",
      "datasets/test/test/blink/83\n",
      "datasets/test/test/unblink/50\n",
      "datasets/test/test/blink/98\n",
      "datasets/test/test/blink/79\n",
      "datasets/test/test/unblink/68\n",
      "datasets/test/test/unblink/96\n",
      "datasets/test/test/blink/45\n",
      "datasets/test/test/blink/44\n",
      "datasets/test/test/blink/34\n",
      "datasets/test/test/unblink/44\n",
      "datasets/test/test/blink/101\n",
      "datasets/test/test/blink/118\n",
      "datasets/test/test/unblink/18\n",
      "datasets/test/test/unblink/69\n",
      "datasets/test/test/blink/84\n",
      "datasets/test/test/unblink/33\n",
      "datasets/test/test/unblink/83\n",
      "datasets/test/test/unblink/81\n",
      "datasets/test/test/unblink/31\n",
      "datasets/test/test/blink/80\n",
      "datasets/test/test/blink/63\n",
      "datasets/test/test/unblink/28\n",
      "datasets/test/test/blink/75\n",
      "datasets/test/test/unblink/70\n",
      "datasets/test/test/blink/1\n",
      "datasets/test/test/unblink/13\n",
      "datasets/test/test/unblink/79\n",
      "datasets/test/test/unblink/76\n",
      "datasets/test/test/unblink/41\n",
      "datasets/test/test/blink/114\n",
      "datasets/test/test/blink/31\n",
      "datasets/test/test/unblink/15\n",
      "datasets/test/test/unblink/1\n",
      "datasets/test/test/blink/119\n",
      "datasets/test/test/unblink/60\n",
      "datasets/test/test/unblink/17\n",
      "datasets/test/test/blink/125\n",
      "datasets/test/test/blink/91\n",
      "datasets/test/test/blink/29\n",
      "datasets/test/test/blink/52\n",
      "datasets/test/test/unblink/87\n",
      "datasets/test/test/blink/26\n",
      "datasets/test/test/unblink/12\n",
      "datasets/test/test/blink/103\n",
      "datasets/test/test/unblink/46\n",
      "datasets/test/test/blink/73\n",
      "datasets/test/test/blink/36\n",
      "datasets/test/test/unblink/59\n",
      "datasets/test/test/unblink/39\n",
      "datasets/test/test/blink/12\n",
      "datasets/test/test/unblink/73\n",
      "datasets/test/test/blink/77\n",
      "datasets/test/test/unblink/89\n",
      "datasets/test/test/unblink/67\n",
      "datasets/test/test/unblink/45\n",
      "datasets/test/test/unblink/63\n",
      "datasets/test/test/unblink/74\n",
      "datasets/test/test/blink/28\n",
      "datasets/test/test/unblink/58\n",
      "datasets/test/test/blink/123\n",
      "datasets/test/test/blink/102\n",
      "datasets/test/test/blink/8\n",
      "datasets/test/test/unblink/90\n",
      "datasets/test/test/blink/16\n",
      "datasets/test/test/blink/41\n",
      "datasets/test/test/unblink/40\n",
      "datasets/test/test/blink/64\n",
      "datasets/test/test/unblink/53\n",
      "datasets/test/test/blink/53\n",
      "datasets/test/test/blink/62\n",
      "datasets/test/test/blink/57\n",
      "datasets/test/test/unblink/6\n",
      "datasets/test/test/unblink/62\n",
      "datasets/test/test/blink/24\n",
      "datasets/test/test/unblink/64\n",
      "datasets/test/test/blink/70\n",
      "datasets/test/test/blink/92\n",
      "datasets/test/test/blink/4\n",
      "datasets/test/test/blink/100\n",
      "datasets/test/test/blink/106\n",
      "datasets/test/test/unblink/19\n",
      "datasets/test/test/unblink/11\n",
      "datasets/test/test/unblink/25\n",
      "datasets/test/test/unblink/85\n",
      "datasets/test/test/unblink/97\n",
      "datasets/test/test/unblink/36\n",
      "datasets/test/test/blink/68\n",
      "datasets/test/test/blink/65\n",
      "datasets/test/test/blink/19\n",
      "datasets/test/test/blink/48\n",
      "datasets/test/test/blink/97\n",
      "datasets/test/test/blink/37\n",
      "datasets/test/test/blink/110\n",
      "datasets/test/test/blink/67\n",
      "datasets/test/test/unblink/37\n",
      "datasets/test/test/blink/15\n",
      "datasets/test/test/unblink/91\n",
      "datasets/test/test/unblink/16\n",
      "datasets/test/test/unblink/94\n",
      "datasets/test/test/blink/72\n",
      "datasets/test/test/unblink/80\n",
      "datasets/test/test/unblink/84\n",
      "datasets/test/test/unblink/14\n",
      "datasets/test/test/unblink/77\n",
      "datasets/test/test/blink/71\n",
      "datasets/test/test/blink/46\n",
      "datasets/test/test/blink/82\n",
      "datasets/test/test/blink/6\n",
      "datasets/test/test/blink/107\n",
      "datasets/test/test/unblink/20\n",
      "datasets/test/test/unblink/47\n",
      "datasets/test/test/unblink/8\n",
      "datasets/test/test/blink/7\n",
      "datasets/test/test/unblink/9\n",
      "datasets/test/test/unblink/38\n",
      "datasets/test/test/unblink/66\n",
      "datasets/test/test/unblink/29\n",
      "datasets/test/test/blink/20\n",
      "datasets/test/test/blink/93\n",
      "datasets/test/test/blink/25\n",
      "datasets/test/test/blink/121\n",
      "datasets/test/test/blink/95\n",
      "datasets/test/test/unblink/32\n",
      "datasets/test/test/blink/109\n",
      "datasets/test/test/unblink/72\n",
      "datasets/test/test/unblink/43\n",
      "datasets/test/test/blink/39\n",
      "datasets/test/test/blink/50\n",
      "datasets/test/test/blink/99\n",
      "datasets/test/test/unblink/22\n",
      "datasets/test/test/blink/86\n",
      "datasets/test/test/unblink/88\n",
      "datasets/test/test/blink/85\n",
      "datasets/test/test/blink/9\n",
      "datasets/test/test/blink/115\n",
      "datasets/test/test/unblink/23\n",
      "datasets/test/test/unblink/49\n",
      "datasets/test/test/unblink/56\n",
      "datasets/test/test/blink/94\n",
      "datasets/test/test/blink/13\n",
      "datasets/test/test/unblink/95\n",
      "datasets/test/test/blink/51\n",
      "datasets/test/test/blink/59\n",
      "datasets/test/test/blink/58\n",
      "datasets/test/test/unblink/54\n",
      "datasets/test/test/blink/69\n",
      "datasets/test/test/blink/38\n",
      "datasets/test/test/blink/89\n",
      "datasets/test/test/blink/78\n",
      "datasets/test/test/unblink/78\n",
      "datasets/test/test/unblink/93\n",
      "datasets/test/test/blink/21\n",
      "datasets/test/test/blink/10\n",
      "datasets/test/test/unblink/3\n",
      "datasets/test/test/unblink/34\n",
      "datasets/test/test/unblink/86\n",
      "datasets/test/test/blink/126\n",
      "datasets/test/test/unblink/57\n",
      "datasets/test/test/unblink/82\n",
      "datasets/test/test/unblink/27\n",
      "datasets/test/test/blink/32\n",
      "datasets/test/test/unblink/2\n",
      "datasets/test/test/blink/22\n",
      "datasets/test/test/unblink/21\n",
      "datasets/test/test/blink/35\n",
      "datasets/test/test/unblink/4\n",
      "datasets/test/test/blink/112\n",
      "datasets/test/test/blink/49\n",
      "datasets/test/test/blink/61\n",
      "datasets/test/test/blink/33\n",
      "datasets/test/test/unblink/30\n",
      "datasets/test/test/blink/116\n",
      "datasets/test/test/unblink/7\n",
      "datasets/test/test/blink/3\n",
      "datasets/test/test/unblink/24\n",
      "datasets/test/test/blink/81\n",
      "datasets/test/test/unblink/5\n",
      "datasets/test/test/unblink/42\n",
      "datasets/test/test/unblink/92\n",
      "datasets/test/test/blink/104\n",
      "datasets/test/test/blink/117\n",
      "datasets/test/test/unblink/26\n",
      "datasets/test/test/blink/18\n",
      "datasets/test/test/blink/27\n",
      "datasets/test/test/blink/90\n",
      "datasets/test/test/unblink/51\n",
      "datasets/test/test/blink/96\n",
      "datasets/test/test/blink/122\n",
      "datasets/test/test/unblink/35\n",
      "datasets/test/test/blink/111\n",
      "datasets/test/test/blink/88\n",
      "datasets/test/test/blink/30\n",
      "datasets/test/test/unblink/75\n",
      "datasets/test/test/blink/2\n",
      "datasets/test/test/unblink/52\n",
      "datasets/test/test/blink/74\n",
      "datasets/test/test/blink/40\n",
      "datasets/test/test/blink/17\n",
      "datasets/test/test/blink/23\n",
      "datasets/test/test/blink/113\n",
      "datasets/test/test/blink/120\n",
      "datasets/test/test/blink/76\n",
      "datasets/test/test/blink/42\n",
      "datasets/test/test/unblink/61\n",
      "datasets/test/test/blink/66\n",
      "datasets/test/test/blink/87\n",
      "datasets/test/test/blink/60\n",
      "datasets/test/test/unblink/55\n",
      "datasets/test/test/blink/14\n",
      "datasets/test/test/blink/55\n",
      "datasets/test/test/unblink/48\n",
      "datasets/test/test/blink/5\n",
      "datasets/test/test/blink/47\n",
      "datasets/test/test/unblink/71\n",
      "---- [[78, 100], [79, 97], [80, 99], [82, 100], [83, 96], [85, 96], [87, 101], [88, 97], [208, 116], [209, 107], [210, 114], [211, 112], [212, 108], [213, 109], [214, 98], [215, 101], [216, 110], [217, 108], [218, 108], [219, 106], [220, 106], [221, 127], [222, 127], [223, 129], [224, 128], [225, 128], [226, 128], [227, 127], [228, 128], [229, 125], [230, 125], [231, 129], [232, 127], [233, 125], [286, 134], [287, 125], [288, 133], [289, 128], [290, 123], [291, 133], [292, 130], [293, 128], [294, 134], [295, 131], [296, 132], [297, 136], [298, 131], [390, 133], [391, 133], [392, 133], [393, 135], [394, 134], [395, 133], [396, 132], [397, 134], [398, 134], [399, 133], [400, 134], [401, 134], [402, 133], [533, 107], [534, 112], [535, 102], [536, 113], [537, 113], [538, 114], [539, 113], [540, 112], [541, 113], [542, 113], [543, 112], [544, 112], [545, 112], [585, 110], [586, 108], [587, 112], [588, 113], [589, 100], [590, 101], [591, 103], [592, 100], [593, 112], [594, 115], [595, 112], [596, 113], [597, 112], [598, 99], [599, 99], [600, 97], [601, 99], [602, 99], [603, 99], [604, 97], [605, 97], [648, 97], [663, 109], [664, 109], [665, 106], [666, 108], [667, 108], [668, 104], [669, 102], [670, 101], [671, 99], [672, 100], [673, 102], [674, 97], [1016, 97], [1020, 96], [1021, 100], [1022, 97], [1023, 99], [1025, 100], [1026, 96], [1170, 132], [1171, 133], [1172, 131], [1173, 137], [1174, 136], [1175, 129], [1176, 134], [1177, 131], [1178, 127], [1179, 130], [1180, 134], [1181, 136], [1182, 133], [1352, 157], [1353, 163], [1354, 159], [1355, 160], [1356, 171], [1357, 167], [1358, 177], [1359, 165], [1360, 162], [1361, 166], [1362, 168], [1363, 167], [1364, 161], [1391, 103], [1392, 112], [1393, 111], [1394, 105], [1395, 111], [1396, 102], [1397, 109], [1398, 111], [1399, 107], [1400, 113], [1401, 107], [1402, 111], [1403, 110], [1456, 112], [1457, 113], [1458, 110], [1459, 111], [1460, 110], [1461, 112], [1462, 111], [1463, 110], [1464, 110], [1465, 112], [1466, 107], [1467, 110], [1468, 110], [1495, 117], [1496, 121], [1497, 116], [1498, 112], [1499, 111], [1500, 104], [1501, 113], [1502, 107], [1503, 107], [1504, 107], [1505, 106], [1506, 106], [1507, 119], [1547, 131], [1548, 127], [1549, 133], [1550, 135], [1551, 135], [1552, 130], [1553, 134], [1554, 131], [1555, 135], [1556, 131], [1557, 134], [1558, 127], [1559, 133], [1612, 161], [1613, 166], [1614, 170], [1615, 164], [1616, 170], [1617, 164], [1618, 163], [1619, 164], [1620, 171], [1621, 159], [1622, 167], [1623, 164], [1624, 170], [1768, 135], [1769, 139], [1770, 145], [1771, 141], [1772, 139], [1773, 151], [1774, 149], [1775, 143], [1776, 138], [1777, 139], [1778, 149], [1779, 144], [1780, 145], [1807, 129], [1808, 128], [1809, 130], [1810, 132], [1811, 133], [1812, 124], [1813, 125], [1814, 127], [1815, 132], [1816, 121], [1817, 133], [1818, 120], [1819, 128], [1990, 97], [1991, 96], [1994, 97], [1995, 96], [2028, 151], [2029, 149], [2030, 149], [2031, 149], [2032, 150], [2033, 140], [2034, 149], [2035, 151], [2036, 142], [2037, 144], [2038, 147], [2039, 151], [2040, 143], [2249, 133], [2250, 136], [2251, 128], [2252, 133], [2253, 136], [2254, 135], [2255, 127], [2256, 131], [2257, 129], [2258, 128], [2259, 130], [2260, 132], [2261, 133], [2379, 123], [2380, 125], [2381, 122], [2382, 120], [2383, 122], [2384, 122], [2385, 120], [2386, 117], [2387, 121], [2388, 119], [2389, 116], [2390, 118], [2391, 123], [2639, 129], [2640, 127], [2641, 127], [2642, 123], [2643, 135], [2644, 124], [2645, 125], [2646, 126], [2647, 128], [2648, 131], [2649, 115], [2650, 114], [2651, 120], [2665, 101], [2666, 101], [2667, 101], [2668, 101], [2669, 101], [2670, 101], [2671, 101], [2672, 101], [2673, 101], [2674, 101], [2675, 101], [2676, 101], [2677, 101], [2678, 102], [2679, 107], [2680, 105], [2681, 100], [2682, 100], [2683, 97], [2684, 96], [2685, 108], [2686, 111], [2687, 108], [2688, 110], [2689, 102], [2690, 104], [2754, 96], [2755, 96], [2847, 96], [2849, 101], [2850, 97], [2878, 96], [2883, 97], [2884, 99], [2885, 99]] ----\n",
      "2912\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = EyeBlinkDataset(train=False, use_threshold=0)\n",
    "data_loader = torch.utils.data.DataLoader(data, batch_size=16, shuffle=True)\n",
    "\n",
    "print(len(data))\n",
    "\n",
    "for seqs, labels in data_loader:\n",
    "    continue\n",
    "\n",
    "# print(data.sizes)\n",
    "# tmp = [[x[0],x[1]['fname'],x[1]['size']] for x in enumerate(data.sizes) if x[1]['size'] >= 96]\n",
    "tmp = [[x[0],x[1]] for x in enumerate(data.sizes) if x[1] >= 96]\n",
    "print(\"----\",tmp, '----')\n",
    "print(len(data.sizes))\n",
    "# counts, bins = np.histogram(data.sizes)\n",
    "plt.style.use('ggplot')\n",
    "plt.hist(data.sizes, bins=50, rwidth=0.85)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['datasets/train/training/blink/8', 'datasets/train/training/blink/141', 'datasets/train/training/blink/151', 'datasets/train/training/blink/153', 'datasets/train/training/blink/155', 'datasets/train/training/blink/158', 'datasets/train/training/blink/160', 'datasets/train/training/blink/179', 'datasets/train/training/blink/181', 'datasets/train/training/blink/198', 'datasets/train/training/unblink/55', 'datasets/train/training/unblink/61', 'datasets/train/training/unblink/76', 'datasets/train/training/unblink/78', 'datasets/train/training/unblink/80', 'datasets/train/training/unblink/82', 'datasets/train/training/unblink/93', 'datasets/train/training/unblink/95', 'datasets/train/training/unblink/144', 'datasets/train/training/unblink/146']\n",
      "58178\n",
      "tensor([[0.5257, 0.4554],\n",
      "        [0.1879, 0.0254],\n",
      "        [0.0493, 0.2270],\n",
      "        [0.1437, 0.2547],\n",
      "        [0.1207, 0.1454],\n",
      "        [0.3567, 0.2765],\n",
      "        [0.3173, 0.2719],\n",
      "        [0.4734, 0.4015],\n",
      "        [0.2769, 0.1193],\n",
      "        [0.1770, 0.2312],\n",
      "        [0.1490, 0.1668],\n",
      "        [0.1640, 0.1154],\n",
      "        [0.6628, 0.4155],\n",
      "        [0.1181, 0.2053],\n",
      "        [0.0997, 0.1000],\n",
      "        [0.1050, 0.1377]], dtype=torch.float64, grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "data = EyeBlinkDataset(train=True)\n",
    "data_loader = torch.utils.data.DataLoader(data, batch_size=16, shuffle=True)\n",
    "\n",
    "seq, label = next(iter(data_loader))\n",
    "\n",
    "model = P2B2_SeparableConv().double()\n",
    "num_params = sum(param.numel() for param in model.parameters() if param.requires_grad) # only trainable\n",
    "print(num_params)\n",
    "\n",
    "seq = seq.permute((0,4,3,1,2))\n",
    "pred = model(seq)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['datasets/train/training/blink/8', 'datasets/train/training/blink/141', 'datasets/train/training/blink/151', 'datasets/train/training/blink/153', 'datasets/train/training/blink/155', 'datasets/train/training/blink/158', 'datasets/train/training/blink/160', 'datasets/train/training/blink/179', 'datasets/train/training/blink/181', 'datasets/train/training/blink/198', 'datasets/train/training/unblink/55', 'datasets/train/training/unblink/61', 'datasets/train/training/unblink/76', 'datasets/train/training/unblink/78', 'datasets/train/training/unblink/80', 'datasets/train/training/unblink/82', 'datasets/train/training/unblink/93', 'datasets/train/training/unblink/95', 'datasets/train/training/unblink/144', 'datasets/train/training/unblink/146']\n",
      "5460\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = EyeBlinkDataset(train=True)\n",
    "data_loader = torch.utils.data.DataLoader(data, batch_size=16, shuffle=True)\n",
    "\n",
    "for seqs, labels in data_loader:\n",
    "    continue\n",
    "\n",
    "# print(data.sizes)\n",
    "print(len(data.sizes))\n",
    "# counts, bins = np.histogram(data.sizes)\n",
    "plt.style.use('ggplot')\n",
    "plt.hist(data.sizes, bins=50, rwidth=0.85)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['datasets/train/training/blink/8', 'datasets/train/training/blink/141', 'datasets/train/training/blink/151', 'datasets/train/training/blink/153', 'datasets/train/training/blink/155', 'datasets/train/training/blink/158', 'datasets/train/training/blink/160', 'datasets/train/training/blink/179', 'datasets/train/training/blink/181', 'datasets/train/training/blink/198', 'datasets/train/training/unblink/55', 'datasets/train/training/unblink/61', 'datasets/train/training/unblink/76', 'datasets/train/training/unblink/78', 'datasets/train/training/unblink/80', 'datasets/train/training/unblink/82', 'datasets/train/training/unblink/93', 'datasets/train/training/unblink/95', 'datasets/train/training/unblink/144', 'datasets/train/training/unblink/146']\n",
      "27\n",
      "428\n",
      "Fold 0: \n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([6, 96, 96, 13, 3]) torch.Size([6])\n",
      "22\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([6, 96, 96, 13, 3]) torch.Size([6])\n",
      "6\n",
      "342\n",
      "86\n",
      "Fold 1: \n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([6, 96, 96, 13, 3]) torch.Size([6])\n",
      "22\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([6, 96, 96, 13, 3]) torch.Size([6])\n",
      "6\n",
      "342\n",
      "86\n",
      "Fold 2: \n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([6, 96, 96, 13, 3]) torch.Size([6])\n",
      "22\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([6, 96, 96, 13, 3]) torch.Size([6])\n",
      "6\n",
      "342\n",
      "86\n",
      "Fold 3: \n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([7, 96, 96, 13, 3]) torch.Size([7])\n",
      "22\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([5, 96, 96, 13, 3]) torch.Size([5])\n",
      "6\n",
      "343\n",
      "85\n",
      "Fold 4: \n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([7, 96, 96, 13, 3]) torch.Size([7])\n",
      "22\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([16, 96, 96, 13, 3]) torch.Size([16])\n",
      "torch.Size([5, 96, 96, 13, 3]) torch.Size([5])\n",
      "6\n",
      "343\n",
      "85\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def fixed_seed(seed_value):\n",
    "    # 1. Set `PYTHONHASHSEED` environment variable at a fixed value\n",
    "    os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "    \n",
    "    # 2. Set `python` built-in pseudo-random generator at a fixed value\n",
    "    random.seed(seed_value)\n",
    "\n",
    "    # 3. Set `numpy` pseudo-random generator at a fixed value\n",
    "    np.random.seed(seed_value)\n",
    "\n",
    "    # 4. Set `pytorch` pseudo-random generator at a fixed value\n",
    "    torch.manual_seed(seed_value)\n",
    "\n",
    "\n",
    "data = EyeBlinkDataset(train=True)\n",
    "tmp_loader = torch.utils.data.DataLoader(data, batch_size=16, shuffle=True)\n",
    "print(len(tmp_loader))\n",
    "\n",
    "# fixed_seed(42)\n",
    "\n",
    "print(len(data))\n",
    "\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "for fold, (train_ids, val_ids) in enumerate(kfold.split(data)):\n",
    "    print(f'Fold {fold}: ')\n",
    "    # print(f'train_ids: {train_ids}')\n",
    "    # print(f'val_ids: {val_ids}')\n",
    "    train_sampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
    "    val_sampler = torch.utils.data.SubsetRandomSampler(val_ids)\n",
    "    # print(f'train_sampler: {next(iter(train_sampler))}')\n",
    "    # print(f'val_sampler: {next(iter(val_sampler))}')\n",
    "    train_loader = torch.utils.data.DataLoader(data, batch_size=16, sampler=train_sampler)\n",
    "    val_loader = torch.utils.data.DataLoader(data, batch_size=16, sampler=val_sampler)\n",
    "\n",
    "    count = 0\n",
    "    for item_1, item_2 in train_loader:\n",
    "        # print(len(item))\n",
    "        print(item_1.shape,item_2.shape)\n",
    "        count+=1\n",
    "    print(count)\n",
    "    count = 0\n",
    "    for item_1, item_2 in val_loader:\n",
    "        count+=1\n",
    "        print(item_1.shape, item_2.shape)\n",
    "    print(count)\n",
    "\n",
    "    print(len(train_sampler))\n",
    "    print(len(val_sampler))\n",
    "    # print(len(train_loader[0]))\n",
    "    # print(len(val_loader[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<built-in function sum>\n",
      "4\n",
      "<built-in function sum>\n",
      "3\n",
      "<built-in function sum>\n",
      "2\n",
      "<built-in function sum>\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "def count_TP(preds_logits, labels):\n",
    "    #preds = preds_logits.argmax(dim=1)\n",
    "    tmp = torch.add(preds_logits, labels)\n",
    "    return (tmp == 2).sum().item()\n",
    "\n",
    "def count_TN(preds_logits, labels):\n",
    "    #preds = preds_logits.argmax(dim=1)\n",
    "    tmp = torch.add(preds_logits, labels)\n",
    "    return (tmp == 0).sum().item()\n",
    "\n",
    "def count_FP(preds_logits, labels):\n",
    "    #preds = preds_logits.argmax(dim=1)\n",
    "    tmp = torch.subtract(preds_logits, labels)\n",
    "    return (tmp == 1).sum().item()\n",
    "\n",
    "def count_FN(preds_logits, labels):\n",
    "    #preds = preds_logits.argmax(dim=1)\n",
    "    tmp = torch.subtract(preds_logits, labels)\n",
    "    return (tmp == -1).sum().item()\n",
    "\n",
    "arr1 = torch.Tensor([0,0,1,1,0,1,0,1,1,1,0])\n",
    "arr2 = torch.Tensor([0,1,1,0,0,1,1,1,0,1,0])\n",
    "print(count_TP(arr1,arr2))\n",
    "print(count_TN(arr1,arr2))\n",
    "print(count_FP(arr1,arr2))\n",
    "print(count_FN(arr1,arr2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "437442\n",
      "58178\n",
      "7583362\n",
      "455170\n"
     ]
    }
   ],
   "source": [
    "model1 = P2B2().double()\n",
    "model2 = P2B2_SeparableConv().double()\n",
    "model3 = P3B3().double()\n",
    "model4 = P3B3_SeparableConv().double()\n",
    "\n",
    "num_params = sum(param.numel() for param in model1.parameters() if param.requires_grad) # only trainable\n",
    "print(num_params)\n",
    "num_params = sum(param.numel() for param in model2.parameters() if param.requires_grad) # only trainable\n",
    "print(num_params)\n",
    "num_params = sum(param.numel() for param in model3.parameters() if param.requires_grad) # only trainable\n",
    "print(num_params)\n",
    "num_params = sum(param.numel() for param in model4.parameters() if param.requires_grad) # only trainable\n",
    "print(num_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['datasets/train/training/blink/8', 'datasets/train/training/blink/141', 'datasets/train/training/blink/151', 'datasets/train/training/blink/153', 'datasets/train/training/blink/155', 'datasets/train/training/blink/158', 'datasets/train/training/blink/160', 'datasets/train/training/blink/179', 'datasets/train/training/blink/181', 'datasets/train/training/blink/198', 'datasets/train/training/unblink/55', 'datasets/train/training/unblink/61', 'datasets/train/training/unblink/76', 'datasets/train/training/unblink/78', 'datasets/train/training/unblink/80', 'datasets/train/training/unblink/82', 'datasets/train/training/unblink/93', 'datasets/train/training/unblink/95', 'datasets/train/training/unblink/144', 'datasets/train/training/unblink/146']\n",
      "428\n",
      "tensor([[[[[0.1843, 0.1176, 0.0863],\n",
      "           [0.1843, 0.1176, 0.0863],\n",
      "           [0.1961, 0.1255, 0.0980],\n",
      "           ...,\n",
      "           [0.2000, 0.1255, 0.1020],\n",
      "           [0.1922, 0.1373, 0.0941],\n",
      "           [0.1922, 0.1373, 0.0941]],\n",
      "\n",
      "          [[0.1843, 0.1216, 0.0863],\n",
      "           [0.1843, 0.1216, 0.0863],\n",
      "           [0.1961, 0.1294, 0.0980],\n",
      "           ...,\n",
      "           [0.2039, 0.1294, 0.1059],\n",
      "           [0.1922, 0.1373, 0.0941],\n",
      "           [0.1922, 0.1373, 0.0941]],\n",
      "\n",
      "          [[0.1843, 0.1255, 0.0863],\n",
      "           [0.1843, 0.1255, 0.0863],\n",
      "           [0.1961, 0.1373, 0.0980],\n",
      "           ...,\n",
      "           [0.2039, 0.1294, 0.1059],\n",
      "           [0.1961, 0.1412, 0.0980],\n",
      "           [0.1961, 0.1412, 0.0980]],\n",
      "\n",
      "          ...,\n",
      "\n",
      "          [[0.1765, 0.0980, 0.0745],\n",
      "           [0.1765, 0.0980, 0.0745],\n",
      "           [0.1765, 0.1020, 0.0745],\n",
      "           ...,\n",
      "           [0.1765, 0.1020, 0.0745],\n",
      "           [0.1804, 0.1098, 0.0627],\n",
      "           [0.1804, 0.1098, 0.0627]],\n",
      "\n",
      "          [[0.1725, 0.0980, 0.0745],\n",
      "           [0.1725, 0.0980, 0.0745],\n",
      "           [0.1765, 0.1020, 0.0745],\n",
      "           ...,\n",
      "           [0.1725, 0.0980, 0.0745],\n",
      "           [0.1804, 0.1098, 0.0627],\n",
      "           [0.1804, 0.1098, 0.0627]],\n",
      "\n",
      "          [[0.1725, 0.0980, 0.0745],\n",
      "           [0.1725, 0.0980, 0.0745],\n",
      "           [0.1765, 0.1020, 0.0784],\n",
      "           ...,\n",
      "           [0.1725, 0.0980, 0.0745],\n",
      "           [0.1765, 0.1059, 0.0627],\n",
      "           [0.1765, 0.1059, 0.0627]]],\n",
      "\n",
      "\n",
      "         [[[0.1922, 0.1216, 0.0902],\n",
      "           [0.1922, 0.1216, 0.0902],\n",
      "           [0.2039, 0.1373, 0.1059],\n",
      "           ...,\n",
      "           [0.2039, 0.1294, 0.1059],\n",
      "           [0.1961, 0.1412, 0.0980],\n",
      "           [0.1961, 0.1412, 0.0980]],\n",
      "\n",
      "          [[0.1922, 0.1255, 0.0902],\n",
      "           [0.1922, 0.1255, 0.0902],\n",
      "           [0.2039, 0.1373, 0.1059],\n",
      "           ...,\n",
      "           [0.2039, 0.1294, 0.1059],\n",
      "           [0.2000, 0.1451, 0.1020],\n",
      "           [0.2000, 0.1451, 0.1020]],\n",
      "\n",
      "          [[0.1922, 0.1333, 0.0941],\n",
      "           [0.1922, 0.1333, 0.0941],\n",
      "           [0.2039, 0.1451, 0.1059],\n",
      "           ...,\n",
      "           [0.2078, 0.1333, 0.1098],\n",
      "           [0.2000, 0.1451, 0.1020],\n",
      "           [0.2000, 0.1451, 0.1020]],\n",
      "\n",
      "          ...,\n",
      "\n",
      "          [[0.1725, 0.0980, 0.0745],\n",
      "           [0.1725, 0.0980, 0.0745],\n",
      "           [0.1725, 0.0980, 0.0706],\n",
      "           ...,\n",
      "           [0.1804, 0.1059, 0.0784],\n",
      "           [0.1843, 0.1137, 0.0667],\n",
      "           [0.1843, 0.1137, 0.0667]],\n",
      "\n",
      "          [[0.1725, 0.0980, 0.0745],\n",
      "           [0.1725, 0.0980, 0.0745],\n",
      "           [0.1765, 0.1020, 0.0706],\n",
      "           ...,\n",
      "           [0.1804, 0.1059, 0.0784],\n",
      "           [0.1843, 0.1137, 0.0627],\n",
      "           [0.1843, 0.1137, 0.0627]],\n",
      "\n",
      "          [[0.1725, 0.0980, 0.0745],\n",
      "           [0.1725, 0.0980, 0.0745],\n",
      "           [0.1765, 0.1020, 0.0745],\n",
      "           ...,\n",
      "           [0.1765, 0.1020, 0.0784],\n",
      "           [0.1804, 0.1098, 0.0627],\n",
      "           [0.1804, 0.1098, 0.0627]]],\n",
      "\n",
      "\n",
      "         [[[0.2039, 0.1333, 0.0980],\n",
      "           [0.2039, 0.1333, 0.0980],\n",
      "           [0.2235, 0.1529, 0.1216],\n",
      "           ...,\n",
      "           [0.2078, 0.1333, 0.1098],\n",
      "           [0.2078, 0.1529, 0.1098],\n",
      "           [0.2078, 0.1529, 0.1098]],\n",
      "\n",
      "          [[0.2039, 0.1373, 0.0980],\n",
      "           [0.2039, 0.1373, 0.0980],\n",
      "           [0.2235, 0.1569, 0.1216],\n",
      "           ...,\n",
      "           [0.2078, 0.1333, 0.1098],\n",
      "           [0.2078, 0.1529, 0.1098],\n",
      "           [0.2078, 0.1529, 0.1098]],\n",
      "\n",
      "          [[0.2039, 0.1412, 0.1020],\n",
      "           [0.2039, 0.1412, 0.1020],\n",
      "           [0.2235, 0.1608, 0.1176],\n",
      "           ...,\n",
      "           [0.2118, 0.1373, 0.1137],\n",
      "           [0.2078, 0.1529, 0.1098],\n",
      "           [0.2078, 0.1529, 0.1098]],\n",
      "\n",
      "          ...,\n",
      "\n",
      "          [[0.1647, 0.0941, 0.0667],\n",
      "           [0.1647, 0.0941, 0.0667],\n",
      "           [0.1686, 0.0941, 0.0627],\n",
      "           ...,\n",
      "           [0.1882, 0.1176, 0.0863],\n",
      "           [0.1922, 0.1216, 0.0706],\n",
      "           [0.1922, 0.1216, 0.0706]],\n",
      "\n",
      "          [[0.1647, 0.0902, 0.0667],\n",
      "           [0.1647, 0.0902, 0.0667],\n",
      "           [0.1686, 0.0941, 0.0627],\n",
      "           ...,\n",
      "           [0.1882, 0.1137, 0.0902],\n",
      "           [0.1882, 0.1176, 0.0667],\n",
      "           [0.1882, 0.1176, 0.0667]],\n",
      "\n",
      "          [[0.1647, 0.0902, 0.0667],\n",
      "           [0.1647, 0.0902, 0.0667],\n",
      "           [0.1725, 0.0941, 0.0627],\n",
      "           ...,\n",
      "           [0.1882, 0.1137, 0.0902],\n",
      "           [0.1882, 0.1176, 0.0667],\n",
      "           [0.1882, 0.1176, 0.0667]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[0.2431, 0.1725, 0.1333],\n",
      "           [0.2431, 0.1725, 0.1333],\n",
      "           [0.2627, 0.1843, 0.1490],\n",
      "           ...,\n",
      "           [0.2627, 0.1882, 0.1569],\n",
      "           [0.2588, 0.1882, 0.1451],\n",
      "           [0.2588, 0.1882, 0.1451]],\n",
      "\n",
      "          [[0.2431, 0.1725, 0.1333],\n",
      "           [0.2431, 0.1725, 0.1333],\n",
      "           [0.2667, 0.1882, 0.1529],\n",
      "           ...,\n",
      "           [0.2627, 0.1882, 0.1529],\n",
      "           [0.2588, 0.1882, 0.1451],\n",
      "           [0.2588, 0.1882, 0.1451]],\n",
      "\n",
      "          [[0.2471, 0.1765, 0.1373],\n",
      "           [0.2471, 0.1765, 0.1373],\n",
      "           [0.2706, 0.1922, 0.1569],\n",
      "           ...,\n",
      "           [0.2588, 0.1882, 0.1490],\n",
      "           [0.2588, 0.1882, 0.1451],\n",
      "           [0.2588, 0.1882, 0.1451]],\n",
      "\n",
      "          ...,\n",
      "\n",
      "          [[0.4471, 0.3725, 0.3451],\n",
      "           [0.4471, 0.3725, 0.3451],\n",
      "           [0.4392, 0.3608, 0.3333],\n",
      "           ...,\n",
      "           [0.4784, 0.3922, 0.3725],\n",
      "           [0.4784, 0.3922, 0.3686],\n",
      "           [0.4784, 0.3922, 0.3686]],\n",
      "\n",
      "          [[0.4471, 0.3725, 0.3451],\n",
      "           [0.4471, 0.3725, 0.3451],\n",
      "           [0.4353, 0.3569, 0.3294],\n",
      "           ...,\n",
      "           [0.4784, 0.3882, 0.3725],\n",
      "           [0.4784, 0.3882, 0.3725],\n",
      "           [0.4784, 0.3882, 0.3725]],\n",
      "\n",
      "          [[0.4471, 0.3725, 0.3451],\n",
      "           [0.4471, 0.3725, 0.3451],\n",
      "           [0.4353, 0.3569, 0.3294],\n",
      "           ...,\n",
      "           [0.4784, 0.3882, 0.3725],\n",
      "           [0.4784, 0.3882, 0.3725],\n",
      "           [0.4784, 0.3882, 0.3725]]],\n",
      "\n",
      "\n",
      "         [[[0.2431, 0.1725, 0.1333],\n",
      "           [0.2431, 0.1725, 0.1333],\n",
      "           [0.2627, 0.1843, 0.1490],\n",
      "           ...,\n",
      "           [0.2588, 0.1882, 0.1490],\n",
      "           [0.2510, 0.1804, 0.1373],\n",
      "           [0.2510, 0.1804, 0.1373]],\n",
      "\n",
      "          [[0.2431, 0.1725, 0.1333],\n",
      "           [0.2431, 0.1725, 0.1333],\n",
      "           [0.2627, 0.1843, 0.1490],\n",
      "           ...,\n",
      "           [0.2588, 0.1882, 0.1490],\n",
      "           [0.2510, 0.1804, 0.1373],\n",
      "           [0.2510, 0.1804, 0.1373]],\n",
      "\n",
      "          [[0.2471, 0.1765, 0.1373],\n",
      "           [0.2471, 0.1765, 0.1373],\n",
      "           [0.2667, 0.1882, 0.1529],\n",
      "           ...,\n",
      "           [0.2588, 0.1882, 0.1451],\n",
      "           [0.2510, 0.1804, 0.1373],\n",
      "           [0.2510, 0.1804, 0.1373]],\n",
      "\n",
      "          ...,\n",
      "\n",
      "          [[0.4549, 0.3804, 0.3529],\n",
      "           [0.4549, 0.3804, 0.3529],\n",
      "           [0.4471, 0.3686, 0.3412],\n",
      "           ...,\n",
      "           [0.4784, 0.3843, 0.3686],\n",
      "           [0.4745, 0.3843, 0.3686],\n",
      "           [0.4745, 0.3843, 0.3686]],\n",
      "\n",
      "          [[0.4549, 0.3804, 0.3529],\n",
      "           [0.4549, 0.3804, 0.3529],\n",
      "           [0.4471, 0.3686, 0.3412],\n",
      "           ...,\n",
      "           [0.4784, 0.3843, 0.3686],\n",
      "           [0.4784, 0.3843, 0.3686],\n",
      "           [0.4784, 0.3843, 0.3686]],\n",
      "\n",
      "          [[0.4549, 0.3804, 0.3529],\n",
      "           [0.4549, 0.3804, 0.3529],\n",
      "           [0.4431, 0.3647, 0.3373],\n",
      "           ...,\n",
      "           [0.4784, 0.3843, 0.3686],\n",
      "           [0.4784, 0.3843, 0.3686],\n",
      "           [0.4784, 0.3843, 0.3686]]],\n",
      "\n",
      "\n",
      "         [[[0.2431, 0.1725, 0.1333],\n",
      "           [0.2431, 0.1725, 0.1333],\n",
      "           [0.2627, 0.1843, 0.1490],\n",
      "           ...,\n",
      "           [0.2588, 0.1882, 0.1490],\n",
      "           [0.2510, 0.1804, 0.1294],\n",
      "           [0.2510, 0.1804, 0.1294]],\n",
      "\n",
      "          [[0.2431, 0.1725, 0.1333],\n",
      "           [0.2431, 0.1725, 0.1333],\n",
      "           [0.2627, 0.1843, 0.1490],\n",
      "           ...,\n",
      "           [0.2588, 0.1882, 0.1451],\n",
      "           [0.2510, 0.1804, 0.1294],\n",
      "           [0.2510, 0.1804, 0.1294]],\n",
      "\n",
      "          [[0.2471, 0.1765, 0.1373],\n",
      "           [0.2471, 0.1765, 0.1373],\n",
      "           [0.2667, 0.1882, 0.1529],\n",
      "           ...,\n",
      "           [0.2588, 0.1882, 0.1412],\n",
      "           [0.2510, 0.1804, 0.1333],\n",
      "           [0.2510, 0.1804, 0.1333]],\n",
      "\n",
      "          ...,\n",
      "\n",
      "          [[0.4549, 0.3804, 0.3529],\n",
      "           [0.4549, 0.3804, 0.3529],\n",
      "           [0.4510, 0.3725, 0.3451],\n",
      "           ...,\n",
      "           [0.4784, 0.3843, 0.3686],\n",
      "           [0.4745, 0.3804, 0.3647],\n",
      "           [0.4745, 0.3804, 0.3647]],\n",
      "\n",
      "          [[0.4549, 0.3804, 0.3529],\n",
      "           [0.4549, 0.3804, 0.3529],\n",
      "           [0.4510, 0.3725, 0.3451],\n",
      "           ...,\n",
      "           [0.4784, 0.3843, 0.3686],\n",
      "           [0.4784, 0.3843, 0.3686],\n",
      "           [0.4784, 0.3843, 0.3686]],\n",
      "\n",
      "          [[0.4549, 0.3804, 0.3529],\n",
      "           [0.4549, 0.3804, 0.3529],\n",
      "           [0.4510, 0.3725, 0.3451],\n",
      "           ...,\n",
      "           [0.4784, 0.3843, 0.3686],\n",
      "           [0.4784, 0.3843, 0.3686],\n",
      "           [0.4784, 0.3843, 0.3686]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0.2941, 0.2235, 0.1686],\n",
      "           [0.2941, 0.2235, 0.1686],\n",
      "           [0.2941, 0.2235, 0.1686],\n",
      "           ...,\n",
      "           [0.3059, 0.2353, 0.1804],\n",
      "           [0.3059, 0.2353, 0.1804],\n",
      "           [0.3059, 0.2353, 0.1804]],\n",
      "\n",
      "          [[0.2941, 0.2235, 0.1686],\n",
      "           [0.2941, 0.2235, 0.1686],\n",
      "           [0.2941, 0.2235, 0.1686],\n",
      "           ...,\n",
      "           [0.3059, 0.2353, 0.1804],\n",
      "           [0.3059, 0.2353, 0.1804],\n",
      "           [0.3059, 0.2353, 0.1804]],\n",
      "\n",
      "          [[0.2941, 0.2235, 0.1686],\n",
      "           [0.2941, 0.2235, 0.1686],\n",
      "           [0.2941, 0.2235, 0.1686],\n",
      "           ...,\n",
      "           [0.3020, 0.2314, 0.1765],\n",
      "           [0.3020, 0.2314, 0.1765],\n",
      "           [0.3020, 0.2314, 0.1765]],\n",
      "\n",
      "          ...,\n",
      "\n",
      "          [[0.1255, 0.1059, 0.0824],\n",
      "           [0.1255, 0.1059, 0.0824],\n",
      "           [0.1176, 0.0980, 0.0745],\n",
      "           ...,\n",
      "           [0.1294, 0.0941, 0.0745],\n",
      "           [0.1294, 0.0941, 0.0745],\n",
      "           [0.1294, 0.0941, 0.0745]],\n",
      "\n",
      "          [[0.1255, 0.1059, 0.0824],\n",
      "           [0.1255, 0.1059, 0.0824],\n",
      "           [0.1176, 0.0980, 0.0745],\n",
      "           ...,\n",
      "           [0.1294, 0.0941, 0.0745],\n",
      "           [0.1294, 0.0941, 0.0745],\n",
      "           [0.1294, 0.0941, 0.0745]],\n",
      "\n",
      "          [[0.1294, 0.1098, 0.0863],\n",
      "           [0.1294, 0.1098, 0.0863],\n",
      "           [0.1216, 0.1020, 0.0784],\n",
      "           ...,\n",
      "           [0.1333, 0.0980, 0.0784],\n",
      "           [0.1333, 0.0980, 0.0784],\n",
      "           [0.1333, 0.0980, 0.0784]]],\n",
      "\n",
      "\n",
      "         [[[0.2980, 0.2275, 0.1725],\n",
      "           [0.2980, 0.2275, 0.1725],\n",
      "           [0.2980, 0.2275, 0.1725],\n",
      "           ...,\n",
      "           [0.3059, 0.2353, 0.1804],\n",
      "           [0.3059, 0.2353, 0.1804],\n",
      "           [0.3059, 0.2353, 0.1804]],\n",
      "\n",
      "          [[0.2980, 0.2275, 0.1725],\n",
      "           [0.2980, 0.2275, 0.1725],\n",
      "           [0.2980, 0.2275, 0.1725],\n",
      "           ...,\n",
      "           [0.3059, 0.2353, 0.1804],\n",
      "           [0.3059, 0.2353, 0.1804],\n",
      "           [0.3059, 0.2353, 0.1804]],\n",
      "\n",
      "          [[0.2980, 0.2275, 0.1725],\n",
      "           [0.2980, 0.2275, 0.1725],\n",
      "           [0.2980, 0.2275, 0.1725],\n",
      "           ...,\n",
      "           [0.3059, 0.2353, 0.1804],\n",
      "           [0.3059, 0.2353, 0.1804],\n",
      "           [0.3059, 0.2353, 0.1804]],\n",
      "\n",
      "          ...,\n",
      "\n",
      "          [[0.1294, 0.1098, 0.0863],\n",
      "           [0.1294, 0.1098, 0.0863],\n",
      "           [0.1216, 0.1020, 0.0784],\n",
      "           ...,\n",
      "           [0.1333, 0.0980, 0.0784],\n",
      "           [0.1333, 0.0980, 0.0784],\n",
      "           [0.1333, 0.0980, 0.0784]],\n",
      "\n",
      "          [[0.1294, 0.1098, 0.0863],\n",
      "           [0.1294, 0.1098, 0.0863],\n",
      "           [0.1216, 0.1020, 0.0784],\n",
      "           ...,\n",
      "           [0.1333, 0.0980, 0.0784],\n",
      "           [0.1333, 0.0980, 0.0784],\n",
      "           [0.1333, 0.0980, 0.0784]],\n",
      "\n",
      "          [[0.1294, 0.1098, 0.0863],\n",
      "           [0.1294, 0.1098, 0.0863],\n",
      "           [0.1216, 0.1020, 0.0784],\n",
      "           ...,\n",
      "           [0.1333, 0.0980, 0.0784],\n",
      "           [0.1333, 0.0980, 0.0784],\n",
      "           [0.1333, 0.0980, 0.0784]]],\n",
      "\n",
      "\n",
      "         [[[0.3059, 0.2353, 0.1804],\n",
      "           [0.3059, 0.2353, 0.1804],\n",
      "           [0.3059, 0.2353, 0.1804],\n",
      "           ...,\n",
      "           [0.3098, 0.2392, 0.1843],\n",
      "           [0.3098, 0.2392, 0.1843],\n",
      "           [0.3098, 0.2392, 0.1843]],\n",
      "\n",
      "          [[0.3059, 0.2353, 0.1804],\n",
      "           [0.3059, 0.2353, 0.1804],\n",
      "           [0.3059, 0.2353, 0.1804],\n",
      "           ...,\n",
      "           [0.3098, 0.2392, 0.1843],\n",
      "           [0.3098, 0.2392, 0.1843],\n",
      "           [0.3098, 0.2392, 0.1843]],\n",
      "\n",
      "          [[0.3059, 0.2353, 0.1804],\n",
      "           [0.3059, 0.2353, 0.1804],\n",
      "           [0.3059, 0.2353, 0.1804],\n",
      "           ...,\n",
      "           [0.3098, 0.2392, 0.1843],\n",
      "           [0.3098, 0.2392, 0.1843],\n",
      "           [0.3098, 0.2392, 0.1843]],\n",
      "\n",
      "          ...,\n",
      "\n",
      "          [[0.1373, 0.1176, 0.0941],\n",
      "           [0.1373, 0.1176, 0.0941],\n",
      "           [0.1294, 0.1098, 0.0863],\n",
      "           ...,\n",
      "           [0.1412, 0.1059, 0.0863],\n",
      "           [0.1412, 0.1059, 0.0863],\n",
      "           [0.1412, 0.1059, 0.0863]],\n",
      "\n",
      "          [[0.1373, 0.1176, 0.0941],\n",
      "           [0.1373, 0.1176, 0.0941],\n",
      "           [0.1294, 0.1098, 0.0863],\n",
      "           ...,\n",
      "           [0.1412, 0.1059, 0.0863],\n",
      "           [0.1412, 0.1059, 0.0863],\n",
      "           [0.1412, 0.1059, 0.0863]],\n",
      "\n",
      "          [[0.1373, 0.1176, 0.0941],\n",
      "           [0.1373, 0.1176, 0.0941],\n",
      "           [0.1294, 0.1098, 0.0863],\n",
      "           ...,\n",
      "           [0.1412, 0.1059, 0.0863],\n",
      "           [0.1412, 0.1059, 0.0863],\n",
      "           [0.1412, 0.1059, 0.0863]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[0.2353, 0.1804, 0.1294],\n",
      "           [0.2353, 0.1804, 0.1294],\n",
      "           [0.2353, 0.1804, 0.1294],\n",
      "           ...,\n",
      "           [0.2431, 0.1882, 0.1451],\n",
      "           [0.2431, 0.1882, 0.1451],\n",
      "           [0.2431, 0.1882, 0.1451]],\n",
      "\n",
      "          [[0.2353, 0.1804, 0.1294],\n",
      "           [0.2353, 0.1804, 0.1294],\n",
      "           [0.2353, 0.1804, 0.1294],\n",
      "           ...,\n",
      "           [0.2431, 0.1882, 0.1451],\n",
      "           [0.2431, 0.1882, 0.1451],\n",
      "           [0.2431, 0.1882, 0.1451]],\n",
      "\n",
      "          [[0.2314, 0.1804, 0.1255],\n",
      "           [0.2314, 0.1804, 0.1255],\n",
      "           [0.2314, 0.1804, 0.1333],\n",
      "           ...,\n",
      "           [0.2431, 0.1922, 0.1451],\n",
      "           [0.2431, 0.1922, 0.1451],\n",
      "           [0.2431, 0.1922, 0.1451]],\n",
      "\n",
      "          ...,\n",
      "\n",
      "          [[0.0980, 0.0863, 0.0588],\n",
      "           [0.0980, 0.0863, 0.0588],\n",
      "           [0.0980, 0.0863, 0.0588],\n",
      "           ...,\n",
      "           [0.0980, 0.0863, 0.0588],\n",
      "           [0.0980, 0.0863, 0.0588],\n",
      "           [0.0980, 0.0863, 0.0588]],\n",
      "\n",
      "          [[0.1020, 0.0902, 0.0627],\n",
      "           [0.1020, 0.0902, 0.0627],\n",
      "           [0.1020, 0.0902, 0.0627],\n",
      "           ...,\n",
      "           [0.1020, 0.0902, 0.0627],\n",
      "           [0.1020, 0.0902, 0.0627],\n",
      "           [0.1020, 0.0902, 0.0627]],\n",
      "\n",
      "          [[0.1020, 0.0902, 0.0627],\n",
      "           [0.1020, 0.0902, 0.0627],\n",
      "           [0.1020, 0.0902, 0.0627],\n",
      "           ...,\n",
      "           [0.1020, 0.0902, 0.0627],\n",
      "           [0.1020, 0.0902, 0.0627],\n",
      "           [0.1020, 0.0902, 0.0627]]],\n",
      "\n",
      "\n",
      "         [[[0.2353, 0.1804, 0.1294],\n",
      "           [0.2353, 0.1804, 0.1294],\n",
      "           [0.2353, 0.1804, 0.1294],\n",
      "           ...,\n",
      "           [0.2431, 0.1882, 0.1451],\n",
      "           [0.2431, 0.1882, 0.1451],\n",
      "           [0.2431, 0.1882, 0.1451]],\n",
      "\n",
      "          [[0.2353, 0.1804, 0.1294],\n",
      "           [0.2353, 0.1804, 0.1294],\n",
      "           [0.2353, 0.1804, 0.1294],\n",
      "           ...,\n",
      "           [0.2431, 0.1882, 0.1451],\n",
      "           [0.2431, 0.1882, 0.1451],\n",
      "           [0.2431, 0.1882, 0.1451]],\n",
      "\n",
      "          [[0.2314, 0.1804, 0.1255],\n",
      "           [0.2314, 0.1804, 0.1255],\n",
      "           [0.2314, 0.1804, 0.1333],\n",
      "           ...,\n",
      "           [0.2431, 0.1922, 0.1451],\n",
      "           [0.2471, 0.1922, 0.1490],\n",
      "           [0.2471, 0.1922, 0.1490]],\n",
      "\n",
      "          ...,\n",
      "\n",
      "          [[0.0980, 0.0863, 0.0588],\n",
      "           [0.0980, 0.0863, 0.0588],\n",
      "           [0.0980, 0.0863, 0.0588],\n",
      "           ...,\n",
      "           [0.0980, 0.0863, 0.0588],\n",
      "           [0.0980, 0.0863, 0.0588],\n",
      "           [0.0980, 0.0863, 0.0588]],\n",
      "\n",
      "          [[0.1020, 0.0902, 0.0627],\n",
      "           [0.1020, 0.0902, 0.0627],\n",
      "           [0.1020, 0.0902, 0.0627],\n",
      "           ...,\n",
      "           [0.1020, 0.0902, 0.0627],\n",
      "           [0.1020, 0.0902, 0.0627],\n",
      "           [0.1020, 0.0902, 0.0627]],\n",
      "\n",
      "          [[0.1020, 0.0902, 0.0627],\n",
      "           [0.1020, 0.0902, 0.0627],\n",
      "           [0.1020, 0.0902, 0.0627],\n",
      "           ...,\n",
      "           [0.1020, 0.0902, 0.0627],\n",
      "           [0.1020, 0.0902, 0.0627],\n",
      "           [0.1020, 0.0902, 0.0627]]],\n",
      "\n",
      "\n",
      "         [[[0.2353, 0.1804, 0.1294],\n",
      "           [0.2353, 0.1804, 0.1294],\n",
      "           [0.2353, 0.1804, 0.1294],\n",
      "           ...,\n",
      "           [0.2431, 0.1882, 0.1451],\n",
      "           [0.2431, 0.1882, 0.1451],\n",
      "           [0.2431, 0.1882, 0.1451]],\n",
      "\n",
      "          [[0.2353, 0.1804, 0.1294],\n",
      "           [0.2353, 0.1804, 0.1294],\n",
      "           [0.2353, 0.1804, 0.1294],\n",
      "           ...,\n",
      "           [0.2431, 0.1882, 0.1451],\n",
      "           [0.2431, 0.1882, 0.1451],\n",
      "           [0.2431, 0.1882, 0.1451]],\n",
      "\n",
      "          [[0.2314, 0.1804, 0.1255],\n",
      "           [0.2314, 0.1804, 0.1255],\n",
      "           [0.2314, 0.1804, 0.1333],\n",
      "           ...,\n",
      "           [0.2431, 0.1922, 0.1451],\n",
      "           [0.2471, 0.1922, 0.1490],\n",
      "           [0.2471, 0.1922, 0.1490]],\n",
      "\n",
      "          ...,\n",
      "\n",
      "          [[0.0980, 0.0863, 0.0588],\n",
      "           [0.0980, 0.0863, 0.0588],\n",
      "           [0.0980, 0.0863, 0.0588],\n",
      "           ...,\n",
      "           [0.0980, 0.0863, 0.0588],\n",
      "           [0.0980, 0.0863, 0.0588],\n",
      "           [0.0980, 0.0863, 0.0588]],\n",
      "\n",
      "          [[0.1020, 0.0902, 0.0627],\n",
      "           [0.1020, 0.0902, 0.0627],\n",
      "           [0.1020, 0.0902, 0.0627],\n",
      "           ...,\n",
      "           [0.1020, 0.0902, 0.0627],\n",
      "           [0.1020, 0.0902, 0.0627],\n",
      "           [0.1020, 0.0902, 0.0627]],\n",
      "\n",
      "          [[0.1020, 0.0902, 0.0627],\n",
      "           [0.1020, 0.0902, 0.0627],\n",
      "           [0.1020, 0.0902, 0.0627],\n",
      "           ...,\n",
      "           [0.1020, 0.0902, 0.0627],\n",
      "           [0.1020, 0.0902, 0.0627],\n",
      "           [0.1020, 0.0902, 0.0627]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0.6392, 0.5569, 0.4863],\n",
      "           [0.6392, 0.5451, 0.4784],\n",
      "           [0.5333, 0.4471, 0.3765],\n",
      "           ...,\n",
      "           [0.6235, 0.5490, 0.4941],\n",
      "           [0.5569, 0.4941, 0.4353],\n",
      "           [0.6549, 0.5804, 0.5255]],\n",
      "\n",
      "          [[0.6392, 0.5569, 0.4863],\n",
      "           [0.6588, 0.5686, 0.4980],\n",
      "           [0.5216, 0.4431, 0.3725],\n",
      "           ...,\n",
      "           [0.5961, 0.5294, 0.4745],\n",
      "           [0.5529, 0.4863, 0.4392],\n",
      "           [0.6157, 0.5412, 0.4902]],\n",
      "\n",
      "          [[0.6431, 0.5608, 0.4863],\n",
      "           [0.6510, 0.5686, 0.4980],\n",
      "           [0.5294, 0.4549, 0.3804],\n",
      "           ...,\n",
      "           [0.5765, 0.5098, 0.4549],\n",
      "           [0.5490, 0.4824, 0.4353],\n",
      "           [0.5961, 0.5216, 0.4784]],\n",
      "\n",
      "          ...,\n",
      "\n",
      "          [[0.4235, 0.3176, 0.2902],\n",
      "           [0.4078, 0.3098, 0.2784],\n",
      "           [0.4118, 0.3098, 0.2784],\n",
      "           ...,\n",
      "           [0.4471, 0.3373, 0.2980],\n",
      "           [0.4941, 0.3843, 0.3373],\n",
      "           [0.7843, 0.6745, 0.6039]],\n",
      "\n",
      "          [[0.4314, 0.3294, 0.3020],\n",
      "           [0.4157, 0.3176, 0.2824],\n",
      "           [0.4157, 0.3137, 0.2824],\n",
      "           ...,\n",
      "           [0.4510, 0.3412, 0.2941],\n",
      "           [0.5176, 0.4039, 0.3529],\n",
      "           [0.7843, 0.6784, 0.6000]],\n",
      "\n",
      "          [[0.4471, 0.3451, 0.3137],\n",
      "           [0.4196, 0.3255, 0.2941],\n",
      "           [0.4196, 0.3176, 0.2863],\n",
      "           ...,\n",
      "           [0.4627, 0.3529, 0.3098],\n",
      "           [0.5294, 0.4196, 0.3647],\n",
      "           [0.8157, 0.7098, 0.6235]]],\n",
      "\n",
      "\n",
      "         [[[0.6314, 0.5451, 0.4745],\n",
      "           [0.6314, 0.5373, 0.4706],\n",
      "           [0.5333, 0.4471, 0.3765],\n",
      "           ...,\n",
      "           [0.6157, 0.5412, 0.4784],\n",
      "           [0.5647, 0.4980, 0.4392],\n",
      "           [0.6980, 0.6235, 0.5686]],\n",
      "\n",
      "          [[0.6275, 0.5490, 0.4784],\n",
      "           [0.6510, 0.5608, 0.4941],\n",
      "           [0.5255, 0.4431, 0.3725],\n",
      "           ...,\n",
      "           [0.5882, 0.5255, 0.4667],\n",
      "           [0.5490, 0.4824, 0.4392],\n",
      "           [0.6745, 0.6000, 0.5451]],\n",
      "\n",
      "          [[0.6196, 0.5529, 0.4784],\n",
      "           [0.6431, 0.5608, 0.4902],\n",
      "           [0.5333, 0.4549, 0.3804],\n",
      "           ...,\n",
      "           [0.5765, 0.5098, 0.4588],\n",
      "           [0.5412, 0.4745, 0.4235],\n",
      "           [0.6588, 0.5843, 0.5333]],\n",
      "\n",
      "          ...,\n",
      "\n",
      "          [[0.4196, 0.3176, 0.2824],\n",
      "           [0.4078, 0.3098, 0.2784],\n",
      "           [0.4078, 0.3020, 0.2745],\n",
      "           ...,\n",
      "           [0.4353, 0.3373, 0.2941],\n",
      "           [0.4784, 0.3647, 0.3255],\n",
      "           [0.7647, 0.6549, 0.5843]],\n",
      "\n",
      "          [[0.4235, 0.3176, 0.2902],\n",
      "           [0.4235, 0.3176, 0.2863],\n",
      "           [0.4078, 0.3020, 0.2784],\n",
      "           ...,\n",
      "           [0.4353, 0.3373, 0.2863],\n",
      "           [0.4980, 0.3882, 0.3412],\n",
      "           [0.8039, 0.6941, 0.6157]],\n",
      "\n",
      "          [[0.4353, 0.3373, 0.3059],\n",
      "           [0.4275, 0.3216, 0.2902],\n",
      "           [0.4078, 0.3020, 0.2745],\n",
      "           ...,\n",
      "           [0.4471, 0.3451, 0.3059],\n",
      "           [0.5216, 0.4118, 0.3608],\n",
      "           [0.8314, 0.7216, 0.6392]]],\n",
      "\n",
      "\n",
      "         [[[0.6118, 0.5294, 0.4588],\n",
      "           [0.6000, 0.5176, 0.4431],\n",
      "           [0.5333, 0.4471, 0.3765],\n",
      "           ...,\n",
      "           [0.6000, 0.5294, 0.4667],\n",
      "           [0.5569, 0.4941, 0.4392],\n",
      "           [0.7294, 0.6549, 0.5922]],\n",
      "\n",
      "          [[0.6157, 0.5333, 0.4667],\n",
      "           [0.6431, 0.5608, 0.4902],\n",
      "           [0.5333, 0.4471, 0.3765],\n",
      "           ...,\n",
      "           [0.5804, 0.5137, 0.4588],\n",
      "           [0.5490, 0.4824, 0.4392],\n",
      "           [0.7059, 0.6314, 0.5725]],\n",
      "\n",
      "          [[0.6235, 0.5529, 0.4784],\n",
      "           [0.6314, 0.5569, 0.4824],\n",
      "           [0.5333, 0.4471, 0.3765],\n",
      "           ...,\n",
      "           [0.5725, 0.5059, 0.4588],\n",
      "           [0.5333, 0.4706, 0.4118],\n",
      "           [0.6863, 0.6118, 0.5569]],\n",
      "\n",
      "          ...,\n",
      "\n",
      "          [[0.4157, 0.3137, 0.2824],\n",
      "           [0.4118, 0.3098, 0.2784],\n",
      "           [0.4039, 0.2941, 0.2784],\n",
      "           ...,\n",
      "           [0.4314, 0.3255, 0.2902],\n",
      "           [0.4667, 0.3529, 0.3137],\n",
      "           [0.7412, 0.6314, 0.5608]],\n",
      "\n",
      "          [[0.4314, 0.3216, 0.2941],\n",
      "           [0.4157, 0.3137, 0.2824],\n",
      "           [0.4039, 0.2980, 0.2824],\n",
      "           ...,\n",
      "           [0.4392, 0.3294, 0.2902],\n",
      "           [0.4863, 0.3765, 0.3333],\n",
      "           [0.7804, 0.6745, 0.6000]],\n",
      "\n",
      "          [[0.4431, 0.3333, 0.3020],\n",
      "           [0.4196, 0.3137, 0.2824],\n",
      "           [0.4039, 0.2980, 0.2706],\n",
      "           ...,\n",
      "           [0.4471, 0.3412, 0.3020],\n",
      "           [0.5020, 0.3922, 0.3451],\n",
      "           [0.8196, 0.7098, 0.6353]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[0.6784, 0.5804, 0.4706],\n",
      "           [0.6784, 0.5804, 0.4706],\n",
      "           [0.6980, 0.6000, 0.4667],\n",
      "           ...,\n",
      "           [0.6627, 0.5765, 0.4510],\n",
      "           [0.6549, 0.5765, 0.4510],\n",
      "           [0.7059, 0.6157, 0.5412]],\n",
      "\n",
      "          [[0.6745, 0.5804, 0.4667],\n",
      "           [0.6706, 0.5765, 0.4588],\n",
      "           [0.6824, 0.5882, 0.4588],\n",
      "           ...,\n",
      "           [0.6588, 0.5725, 0.4471],\n",
      "           [0.6471, 0.5804, 0.4627],\n",
      "           [0.6471, 0.5529, 0.4902]],\n",
      "\n",
      "          [[0.6627, 0.5765, 0.4510],\n",
      "           [0.6627, 0.5804, 0.4549],\n",
      "           [0.6745, 0.5765, 0.4431],\n",
      "           ...,\n",
      "           [0.6627, 0.5804, 0.4588],\n",
      "           [0.6745, 0.5961, 0.4784],\n",
      "           [0.5961, 0.5059, 0.4353]],\n",
      "\n",
      "          ...,\n",
      "\n",
      "          [[0.8745, 0.7804, 0.6980],\n",
      "           [0.8431, 0.7608, 0.6824],\n",
      "           [0.8431, 0.7569, 0.6588],\n",
      "           ...,\n",
      "           [0.8667, 0.7765, 0.6980],\n",
      "           [0.8353, 0.7451, 0.6510],\n",
      "           [0.8392, 0.7333, 0.6510]],\n",
      "\n",
      "          [[0.8706, 0.7804, 0.6863],\n",
      "           [0.8667, 0.7882, 0.7059],\n",
      "           [0.8431, 0.7412, 0.6471],\n",
      "           ...,\n",
      "           [0.8588, 0.7686, 0.6784],\n",
      "           [0.8353, 0.7412, 0.6431],\n",
      "           [0.8392, 0.7255, 0.6431]],\n",
      "\n",
      "          [[0.8549, 0.7647, 0.6667],\n",
      "           [0.8706, 0.7804, 0.6941],\n",
      "           [0.8314, 0.7333, 0.6392],\n",
      "           ...,\n",
      "           [0.8471, 0.7569, 0.6706],\n",
      "           [0.8314, 0.7333, 0.6275],\n",
      "           [0.8235, 0.7059, 0.6275]]],\n",
      "\n",
      "\n",
      "         [[[0.6824, 0.5882, 0.4745],\n",
      "           [0.6784, 0.5804, 0.4706],\n",
      "           [0.6980, 0.6000, 0.4627],\n",
      "           ...,\n",
      "           [0.6627, 0.5765, 0.4510],\n",
      "           [0.6549, 0.5804, 0.4549],\n",
      "           [0.7020, 0.6039, 0.5294]],\n",
      "\n",
      "          [[0.6745, 0.5804, 0.4667],\n",
      "           [0.6745, 0.5804, 0.4667],\n",
      "           [0.6863, 0.5922, 0.4549],\n",
      "           ...,\n",
      "           [0.6627, 0.5765, 0.4510],\n",
      "           [0.6510, 0.5843, 0.4627],\n",
      "           [0.6471, 0.5490, 0.4824]],\n",
      "\n",
      "          [[0.6627, 0.5804, 0.4549],\n",
      "           [0.6706, 0.5843, 0.4627],\n",
      "           [0.6706, 0.5725, 0.4431],\n",
      "           ...,\n",
      "           [0.6588, 0.5804, 0.4588],\n",
      "           [0.6667, 0.5961, 0.4784],\n",
      "           [0.6000, 0.5020, 0.4314]],\n",
      "\n",
      "          ...,\n",
      "\n",
      "          [[0.8667, 0.7765, 0.6902],\n",
      "           [0.8510, 0.7725, 0.6863],\n",
      "           [0.8353, 0.7412, 0.6510],\n",
      "           ...,\n",
      "           [0.8627, 0.7725, 0.6863],\n",
      "           [0.8353, 0.7451, 0.6510],\n",
      "           [0.8431, 0.7373, 0.6549]],\n",
      "\n",
      "          [[0.8627, 0.7725, 0.6824],\n",
      "           [0.8706, 0.7882, 0.7020],\n",
      "           [0.8353, 0.7333, 0.6431],\n",
      "           ...,\n",
      "           [0.8510, 0.7608, 0.6745],\n",
      "           [0.8353, 0.7412, 0.6431],\n",
      "           [0.8431, 0.7294, 0.6471]],\n",
      "\n",
      "          [[0.8471, 0.7608, 0.6627],\n",
      "           [0.8745, 0.7843, 0.6980],\n",
      "           [0.8314, 0.7333, 0.6392],\n",
      "           ...,\n",
      "           [0.8471, 0.7569, 0.6627],\n",
      "           [0.8314, 0.7373, 0.6314],\n",
      "           [0.8275, 0.7098, 0.6314]]],\n",
      "\n",
      "\n",
      "         [[[0.6902, 0.5961, 0.4745],\n",
      "           [0.6824, 0.5843, 0.4745],\n",
      "           [0.6980, 0.6000, 0.4627],\n",
      "           ...,\n",
      "           [0.6627, 0.5765, 0.4510],\n",
      "           [0.6627, 0.5843, 0.4627],\n",
      "           [0.6941, 0.5922, 0.5216]],\n",
      "\n",
      "          [[0.6824, 0.5882, 0.4706],\n",
      "           [0.6784, 0.5843, 0.4706],\n",
      "           [0.6863, 0.5961, 0.4588],\n",
      "           ...,\n",
      "           [0.6627, 0.5765, 0.4510],\n",
      "           [0.6588, 0.5922, 0.4745],\n",
      "           [0.6431, 0.5451, 0.4784]],\n",
      "\n",
      "          [[0.6784, 0.5843, 0.4627],\n",
      "           [0.6706, 0.5843, 0.4627],\n",
      "           [0.6784, 0.5843, 0.4510],\n",
      "           ...,\n",
      "           [0.6588, 0.5804, 0.4588],\n",
      "           [0.6706, 0.6000, 0.4824],\n",
      "           [0.6000, 0.4980, 0.4314]],\n",
      "\n",
      "          ...,\n",
      "\n",
      "          [[0.8863, 0.7961, 0.7098],\n",
      "           [0.8510, 0.7686, 0.6824],\n",
      "           [0.8235, 0.7333, 0.6353],\n",
      "           ...,\n",
      "           [0.8549, 0.7686, 0.6824],\n",
      "           [0.8353, 0.7451, 0.6510],\n",
      "           [0.8431, 0.7373, 0.6549]],\n",
      "\n",
      "          [[0.8627, 0.7725, 0.6863],\n",
      "           [0.8627, 0.7843, 0.6941],\n",
      "           [0.8275, 0.7294, 0.6353],\n",
      "           ...,\n",
      "           [0.8431, 0.7608, 0.6745],\n",
      "           [0.8392, 0.7412, 0.6471],\n",
      "           [0.8431, 0.7294, 0.6471]],\n",
      "\n",
      "          [[0.8471, 0.7569, 0.6706],\n",
      "           [0.8706, 0.7804, 0.6941],\n",
      "           [0.8314, 0.7333, 0.6392],\n",
      "           ...,\n",
      "           [0.8314, 0.7529, 0.6627],\n",
      "           [0.8392, 0.7373, 0.6431],\n",
      "           [0.8353, 0.7176, 0.6353]]]],\n",
      "\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0.2902, 0.1725, 0.0941],\n",
      "           [0.2941, 0.1765, 0.0980],\n",
      "           [0.2667, 0.1490, 0.0863],\n",
      "           ...,\n",
      "           [0.2980, 0.1686, 0.1059],\n",
      "           [0.3059, 0.1686, 0.1020],\n",
      "           [0.2980, 0.1569, 0.0902]],\n",
      "\n",
      "          [[0.2902, 0.1725, 0.0941],\n",
      "           [0.2902, 0.1765, 0.0941],\n",
      "           [0.2667, 0.1490, 0.0863],\n",
      "           ...,\n",
      "           [0.2980, 0.1647, 0.0980],\n",
      "           [0.3059, 0.1686, 0.1020],\n",
      "           [0.2941, 0.1569, 0.0902]],\n",
      "\n",
      "          [[0.2863, 0.1686, 0.0941],\n",
      "           [0.2863, 0.1686, 0.0863],\n",
      "           [0.2627, 0.1451, 0.0824],\n",
      "           ...,\n",
      "           [0.2941, 0.1608, 0.0941],\n",
      "           [0.3020, 0.1686, 0.0941],\n",
      "           [0.2941, 0.1529, 0.0863]],\n",
      "\n",
      "          ...,\n",
      "\n",
      "          [[0.2627, 0.1451, 0.0824],\n",
      "           [0.2627, 0.1451, 0.0824],\n",
      "           [0.2471, 0.1373, 0.0706],\n",
      "           ...,\n",
      "           [0.2784, 0.1490, 0.0980],\n",
      "           [0.2627, 0.1373, 0.0941],\n",
      "           [0.2431, 0.1294, 0.0863]],\n",
      "\n",
      "          [[0.2627, 0.1451, 0.0824],\n",
      "           [0.2627, 0.1451, 0.0824],\n",
      "           [0.2471, 0.1373, 0.0706],\n",
      "           ...,\n",
      "           [0.2824, 0.1529, 0.1020],\n",
      "           [0.2667, 0.1412, 0.0980],\n",
      "           [0.2392, 0.1373, 0.0824]],\n",
      "\n",
      "          [[0.2627, 0.1451, 0.0824],\n",
      "           [0.2627, 0.1451, 0.0824],\n",
      "           [0.2471, 0.1373, 0.0706],\n",
      "           ...,\n",
      "           [0.2824, 0.1529, 0.1020],\n",
      "           [0.2706, 0.1412, 0.0980],\n",
      "           [0.2392, 0.1373, 0.0824]]],\n",
      "\n",
      "\n",
      "         [[[0.2902, 0.1725, 0.0902],\n",
      "           [0.2941, 0.1765, 0.0980],\n",
      "           [0.2627, 0.1490, 0.0863],\n",
      "           ...,\n",
      "           [0.2980, 0.1686, 0.1020],\n",
      "           [0.3059, 0.1686, 0.1020],\n",
      "           [0.2902, 0.1569, 0.0902]],\n",
      "\n",
      "          [[0.2863, 0.1686, 0.0902],\n",
      "           [0.2902, 0.1725, 0.0941],\n",
      "           [0.2588, 0.1451, 0.0863],\n",
      "           ...,\n",
      "           [0.2980, 0.1647, 0.0980],\n",
      "           [0.3020, 0.1686, 0.0980],\n",
      "           [0.2902, 0.1529, 0.0902]],\n",
      "\n",
      "          [[0.2824, 0.1647, 0.0902],\n",
      "           [0.2824, 0.1647, 0.0863],\n",
      "           [0.2588, 0.1451, 0.0824],\n",
      "           ...,\n",
      "           [0.2941, 0.1569, 0.0902],\n",
      "           [0.2980, 0.1647, 0.0941],\n",
      "           [0.2863, 0.1529, 0.0863]],\n",
      "\n",
      "          ...,\n",
      "\n",
      "          [[0.2627, 0.1412, 0.0784],\n",
      "           [0.2588, 0.1451, 0.0824],\n",
      "           [0.2471, 0.1373, 0.0745],\n",
      "           ...,\n",
      "           [0.2784, 0.1490, 0.0980],\n",
      "           [0.2627, 0.1373, 0.0941],\n",
      "           [0.2392, 0.1294, 0.0824]],\n",
      "\n",
      "          [[0.2627, 0.1412, 0.0784],\n",
      "           [0.2588, 0.1451, 0.0824],\n",
      "           [0.2471, 0.1373, 0.0745],\n",
      "           ...,\n",
      "           [0.2824, 0.1529, 0.1020],\n",
      "           [0.2667, 0.1412, 0.0980],\n",
      "           [0.2392, 0.1333, 0.0824]],\n",
      "\n",
      "          [[0.2627, 0.1412, 0.0784],\n",
      "           [0.2588, 0.1451, 0.0824],\n",
      "           [0.2471, 0.1373, 0.0706],\n",
      "           ...,\n",
      "           [0.2824, 0.1529, 0.1020],\n",
      "           [0.2706, 0.1412, 0.0980],\n",
      "           [0.2392, 0.1373, 0.0824]]],\n",
      "\n",
      "\n",
      "         [[[0.2824, 0.1647, 0.0824],\n",
      "           [0.2863, 0.1686, 0.0941],\n",
      "           [0.2549, 0.1451, 0.0902],\n",
      "           ...,\n",
      "           [0.2941, 0.1686, 0.0980],\n",
      "           [0.3020, 0.1686, 0.0980],\n",
      "           [0.2824, 0.1529, 0.0941]],\n",
      "\n",
      "          [[0.2784, 0.1608, 0.0824],\n",
      "           [0.2824, 0.1647, 0.0902],\n",
      "           [0.2510, 0.1451, 0.0902],\n",
      "           ...,\n",
      "           [0.2941, 0.1608, 0.0941],\n",
      "           [0.2980, 0.1647, 0.0941],\n",
      "           [0.2824, 0.1490, 0.0902]],\n",
      "\n",
      "          [[0.2784, 0.1569, 0.0863],\n",
      "           [0.2784, 0.1569, 0.0863],\n",
      "           [0.2510, 0.1412, 0.0863],\n",
      "           ...,\n",
      "           [0.2941, 0.1569, 0.0902],\n",
      "           [0.2941, 0.1569, 0.0902],\n",
      "           [0.2784, 0.1490, 0.0902]],\n",
      "\n",
      "          ...,\n",
      "\n",
      "          [[0.2588, 0.1373, 0.0784],\n",
      "           [0.2510, 0.1412, 0.0784],\n",
      "           [0.2471, 0.1373, 0.0824],\n",
      "           ...,\n",
      "           [0.2824, 0.1490, 0.0980],\n",
      "           [0.2627, 0.1373, 0.0941],\n",
      "           [0.2353, 0.1294, 0.0824]],\n",
      "\n",
      "          [[0.2588, 0.1373, 0.0784],\n",
      "           [0.2510, 0.1412, 0.0784],\n",
      "           [0.2471, 0.1373, 0.0745],\n",
      "           ...,\n",
      "           [0.2824, 0.1529, 0.1020],\n",
      "           [0.2706, 0.1412, 0.0980],\n",
      "           [0.2353, 0.1333, 0.0824]],\n",
      "\n",
      "          [[0.2588, 0.1373, 0.0784],\n",
      "           [0.2510, 0.1412, 0.0784],\n",
      "           [0.2471, 0.1373, 0.0706],\n",
      "           ...,\n",
      "           [0.2824, 0.1529, 0.1020],\n",
      "           [0.2745, 0.1412, 0.0980],\n",
      "           [0.2353, 0.1333, 0.0863]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[0.4118, 0.2706, 0.1686],\n",
      "           [0.4157, 0.2627, 0.1647],\n",
      "           [0.4275, 0.2784, 0.1686],\n",
      "           ...,\n",
      "           [0.4510, 0.3098, 0.1922],\n",
      "           [0.4549, 0.3176, 0.1961],\n",
      "           [0.4549, 0.3176, 0.1882]],\n",
      "\n",
      "          [[0.4118, 0.2706, 0.1725],\n",
      "           [0.4118, 0.2588, 0.1608],\n",
      "           [0.4275, 0.2784, 0.1686],\n",
      "           ...,\n",
      "           [0.4510, 0.3098, 0.1922],\n",
      "           [0.4549, 0.3137, 0.1961],\n",
      "           [0.4510, 0.3137, 0.1843]],\n",
      "\n",
      "          [[0.4157, 0.2745, 0.1804],\n",
      "           [0.4078, 0.2588, 0.1569],\n",
      "           [0.4275, 0.2784, 0.1686],\n",
      "           ...,\n",
      "           [0.4471, 0.3059, 0.1882],\n",
      "           [0.4510, 0.3098, 0.1922],\n",
      "           [0.4510, 0.3137, 0.1843]],\n",
      "\n",
      "          ...,\n",
      "\n",
      "          [[0.4275, 0.2941, 0.1843],\n",
      "           [0.4196, 0.2863, 0.1804],\n",
      "           [0.4078, 0.2824, 0.1686],\n",
      "           ...,\n",
      "           [0.4353, 0.2980, 0.1843],\n",
      "           [0.4275, 0.2863, 0.1804],\n",
      "           [0.4392, 0.2980, 0.1765]],\n",
      "\n",
      "          [[0.4275, 0.2941, 0.1843],\n",
      "           [0.4157, 0.2863, 0.1765],\n",
      "           [0.4078, 0.2824, 0.1686],\n",
      "           ...,\n",
      "           [0.4392, 0.2980, 0.1882],\n",
      "           [0.4314, 0.2902, 0.1843],\n",
      "           [0.4392, 0.2980, 0.1804]],\n",
      "\n",
      "          [[0.4275, 0.2941, 0.1843],\n",
      "           [0.4118, 0.2863, 0.1725],\n",
      "           [0.4078, 0.2824, 0.1686],\n",
      "           ...,\n",
      "           [0.4392, 0.2980, 0.1882],\n",
      "           [0.4392, 0.2980, 0.1843],\n",
      "           [0.4431, 0.3020, 0.1843]]],\n",
      "\n",
      "\n",
      "         [[[0.4275, 0.2824, 0.1725],\n",
      "           [0.4235, 0.2706, 0.1725],\n",
      "           [0.4392, 0.2863, 0.1765],\n",
      "           ...,\n",
      "           [0.4667, 0.3137, 0.1961],\n",
      "           [0.4588, 0.3216, 0.2000],\n",
      "           [0.4588, 0.3176, 0.1882]],\n",
      "\n",
      "          [[0.4275, 0.2824, 0.1804],\n",
      "           [0.4235, 0.2706, 0.1686],\n",
      "           [0.4392, 0.2863, 0.1765],\n",
      "           ...,\n",
      "           [0.4627, 0.3137, 0.1961],\n",
      "           [0.4588, 0.3216, 0.2000],\n",
      "           [0.4588, 0.3176, 0.1882]],\n",
      "\n",
      "          [[0.4275, 0.2824, 0.1843],\n",
      "           [0.4196, 0.2706, 0.1686],\n",
      "           [0.4392, 0.2863, 0.1765],\n",
      "           ...,\n",
      "           [0.4588, 0.3137, 0.1961],\n",
      "           [0.4588, 0.3176, 0.1961],\n",
      "           [0.4549, 0.3176, 0.1882]],\n",
      "\n",
      "          ...,\n",
      "\n",
      "          [[0.4275, 0.2941, 0.1843],\n",
      "           [0.4196, 0.2863, 0.1804],\n",
      "           [0.4039, 0.2784, 0.1647],\n",
      "           ...,\n",
      "           [0.4392, 0.2980, 0.1882],\n",
      "           [0.4353, 0.2941, 0.1843],\n",
      "           [0.4431, 0.3020, 0.1843]],\n",
      "\n",
      "          [[0.4275, 0.2941, 0.1843],\n",
      "           [0.4157, 0.2863, 0.1765],\n",
      "           [0.4039, 0.2784, 0.1647],\n",
      "           ...,\n",
      "           [0.4431, 0.3020, 0.1922],\n",
      "           [0.4353, 0.2941, 0.1882],\n",
      "           [0.4471, 0.3059, 0.1843]],\n",
      "\n",
      "          [[0.4275, 0.2941, 0.1843],\n",
      "           [0.4118, 0.2863, 0.1725],\n",
      "           [0.4039, 0.2784, 0.1647],\n",
      "           ...,\n",
      "           [0.4431, 0.3020, 0.1922],\n",
      "           [0.4392, 0.2980, 0.1882],\n",
      "           [0.4471, 0.3059, 0.1882]]],\n",
      "\n",
      "\n",
      "         [[[0.4392, 0.2863, 0.1765],\n",
      "           [0.4275, 0.2745, 0.1765],\n",
      "           [0.4431, 0.2902, 0.1843],\n",
      "           ...,\n",
      "           [0.4745, 0.3176, 0.2000],\n",
      "           [0.4627, 0.3216, 0.2000],\n",
      "           [0.4588, 0.3176, 0.1882]],\n",
      "\n",
      "          [[0.4392, 0.2863, 0.1804],\n",
      "           [0.4275, 0.2745, 0.1765],\n",
      "           [0.4431, 0.2902, 0.1843],\n",
      "           ...,\n",
      "           [0.4706, 0.3176, 0.2000],\n",
      "           [0.4627, 0.3216, 0.2000],\n",
      "           [0.4588, 0.3176, 0.1882]],\n",
      "\n",
      "          [[0.4353, 0.2863, 0.1843],\n",
      "           [0.4275, 0.2745, 0.1804],\n",
      "           [0.4431, 0.2902, 0.1843],\n",
      "           ...,\n",
      "           [0.4627, 0.3176, 0.2000],\n",
      "           [0.4627, 0.3216, 0.2000],\n",
      "           [0.4588, 0.3176, 0.1882]],\n",
      "\n",
      "          ...,\n",
      "\n",
      "          [[0.4275, 0.2941, 0.1843],\n",
      "           [0.4196, 0.2863, 0.1804],\n",
      "           [0.4039, 0.2784, 0.1647],\n",
      "           ...,\n",
      "           [0.4392, 0.2980, 0.1882],\n",
      "           [0.4392, 0.2980, 0.1843],\n",
      "           [0.4471, 0.3059, 0.1882]],\n",
      "\n",
      "          [[0.4275, 0.2941, 0.1843],\n",
      "           [0.4157, 0.2863, 0.1765],\n",
      "           [0.4039, 0.2784, 0.1647],\n",
      "           ...,\n",
      "           [0.4431, 0.3020, 0.1922],\n",
      "           [0.4392, 0.2980, 0.1882],\n",
      "           [0.4471, 0.3059, 0.1882]],\n",
      "\n",
      "          [[0.4275, 0.2941, 0.1843],\n",
      "           [0.4118, 0.2863, 0.1725],\n",
      "           [0.4039, 0.2784, 0.1647],\n",
      "           ...,\n",
      "           [0.4431, 0.3020, 0.1922],\n",
      "           [0.4392, 0.2980, 0.1882],\n",
      "           [0.4471, 0.3059, 0.1882]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0.3765, 0.2980, 0.2980],\n",
      "           [0.3765, 0.2980, 0.2980],\n",
      "           [0.3216, 0.2588, 0.2667],\n",
      "           ...,\n",
      "           [0.3882, 0.3176, 0.3255],\n",
      "           [0.3882, 0.3059, 0.3059],\n",
      "           [0.4353, 0.3451, 0.3490]],\n",
      "\n",
      "          [[0.3725, 0.2941, 0.2941],\n",
      "           [0.3725, 0.2941, 0.2941],\n",
      "           [0.3137, 0.2510, 0.2549],\n",
      "           ...,\n",
      "           [0.3882, 0.3176, 0.3137],\n",
      "           [0.3882, 0.3059, 0.3059],\n",
      "           [0.4235, 0.3333, 0.3373]],\n",
      "\n",
      "          [[0.3725, 0.2941, 0.2941],\n",
      "           [0.3725, 0.2941, 0.2941],\n",
      "           [0.3137, 0.2510, 0.2588],\n",
      "           ...,\n",
      "           [0.3882, 0.3176, 0.3059],\n",
      "           [0.3882, 0.3059, 0.3059],\n",
      "           [0.4235, 0.3333, 0.3333]],\n",
      "\n",
      "          ...,\n",
      "\n",
      "          [[0.2314, 0.1804, 0.1961],\n",
      "           [0.2314, 0.1804, 0.1961],\n",
      "           [0.1961, 0.1490, 0.1608],\n",
      "           ...,\n",
      "           [0.2510, 0.1961, 0.2118],\n",
      "           [0.2510, 0.2000, 0.2000],\n",
      "           [0.2510, 0.2000, 0.2000]],\n",
      "\n",
      "          [[0.2275, 0.1804, 0.1922],\n",
      "           [0.2275, 0.1804, 0.1922],\n",
      "           [0.1922, 0.1451, 0.1569],\n",
      "           ...,\n",
      "           [0.2510, 0.1961, 0.2118],\n",
      "           [0.2510, 0.2000, 0.2000],\n",
      "           [0.2510, 0.2000, 0.2000]],\n",
      "\n",
      "          [[0.2235, 0.1765, 0.1882],\n",
      "           [0.2235, 0.1765, 0.1882],\n",
      "           [0.1922, 0.1451, 0.1569],\n",
      "           ...,\n",
      "           [0.2510, 0.1961, 0.2118],\n",
      "           [0.2510, 0.2000, 0.2000],\n",
      "           [0.2510, 0.2000, 0.2000]]],\n",
      "\n",
      "\n",
      "         [[[0.3882, 0.3059, 0.3059],\n",
      "           [0.3882, 0.3059, 0.3059],\n",
      "           [0.3333, 0.2745, 0.2784],\n",
      "           ...,\n",
      "           [0.3843, 0.3137, 0.3098],\n",
      "           [0.3765, 0.2980, 0.2980],\n",
      "           [0.4510, 0.3529, 0.3490]],\n",
      "\n",
      "          [[0.3765, 0.2980, 0.2980],\n",
      "           [0.3765, 0.2980, 0.2980],\n",
      "           [0.3255, 0.2667, 0.2667],\n",
      "           ...,\n",
      "           [0.3804, 0.3098, 0.3059],\n",
      "           [0.3765, 0.2980, 0.2980],\n",
      "           [0.4431, 0.3451, 0.3412]],\n",
      "\n",
      "          [[0.3725, 0.2941, 0.2941],\n",
      "           [0.3725, 0.2941, 0.2941],\n",
      "           [0.3294, 0.2667, 0.2706],\n",
      "           ...,\n",
      "           [0.3765, 0.3098, 0.3020],\n",
      "           [0.3765, 0.2980, 0.2980],\n",
      "           [0.4353, 0.3412, 0.3373]],\n",
      "\n",
      "          ...,\n",
      "\n",
      "          [[0.2353, 0.1882, 0.2000],\n",
      "           [0.2353, 0.1882, 0.2000],\n",
      "           [0.2000, 0.1490, 0.1608],\n",
      "           ...,\n",
      "           [0.2667, 0.2118, 0.2235],\n",
      "           [0.2588, 0.2039, 0.2078],\n",
      "           [0.2510, 0.2000, 0.2000]],\n",
      "\n",
      "          [[0.2314, 0.1804, 0.1961],\n",
      "           [0.2314, 0.1804, 0.1961],\n",
      "           [0.1922, 0.1451, 0.1569],\n",
      "           ...,\n",
      "           [0.2667, 0.2118, 0.2235],\n",
      "           [0.2588, 0.2078, 0.2078],\n",
      "           [0.2510, 0.2000, 0.2000]],\n",
      "\n",
      "          [[0.2314, 0.1804, 0.1961],\n",
      "           [0.2314, 0.1804, 0.1961],\n",
      "           [0.1922, 0.1451, 0.1569],\n",
      "           ...,\n",
      "           [0.2667, 0.2118, 0.2235],\n",
      "           [0.2588, 0.2078, 0.2078],\n",
      "           [0.2510, 0.2000, 0.2000]]],\n",
      "\n",
      "\n",
      "         [[[0.3882, 0.3059, 0.3059],\n",
      "           [0.3882, 0.3059, 0.3059],\n",
      "           [0.3490, 0.2745, 0.2824],\n",
      "           ...,\n",
      "           [0.3765, 0.3059, 0.3020],\n",
      "           [0.3647, 0.2863, 0.2863],\n",
      "           [0.4471, 0.3490, 0.3451]],\n",
      "\n",
      "          [[0.3882, 0.3059, 0.3059],\n",
      "           [0.3882, 0.3059, 0.3059],\n",
      "           [0.3451, 0.2745, 0.2824],\n",
      "           ...,\n",
      "           [0.3765, 0.3059, 0.3020],\n",
      "           [0.3765, 0.2980, 0.2980],\n",
      "           [0.4510, 0.3490, 0.3451]],\n",
      "\n",
      "          [[0.3765, 0.2980, 0.2980],\n",
      "           [0.3765, 0.2980, 0.2980],\n",
      "           [0.3451, 0.2745, 0.2824],\n",
      "           ...,\n",
      "           [0.3686, 0.2980, 0.2980],\n",
      "           [0.3843, 0.3020, 0.3020],\n",
      "           [0.4431, 0.3451, 0.3412]],\n",
      "\n",
      "          ...,\n",
      "\n",
      "          [[0.2353, 0.1922, 0.2000],\n",
      "           [0.2353, 0.1922, 0.2000],\n",
      "           [0.1843, 0.1451, 0.1569],\n",
      "           ...,\n",
      "           [0.2706, 0.2157, 0.2275],\n",
      "           [0.2627, 0.2078, 0.2118],\n",
      "           [0.2471, 0.1961, 0.1961]],\n",
      "\n",
      "          [[0.2353, 0.1882, 0.2000],\n",
      "           [0.2353, 0.1882, 0.2000],\n",
      "           [0.1843, 0.1451, 0.1569],\n",
      "           ...,\n",
      "           [0.2706, 0.2157, 0.2275],\n",
      "           [0.2627, 0.2078, 0.2118],\n",
      "           [0.2471, 0.2000, 0.1961]],\n",
      "\n",
      "          [[0.2353, 0.1922, 0.2000],\n",
      "           [0.2353, 0.1922, 0.2000],\n",
      "           [0.1922, 0.1529, 0.1647],\n",
      "           ...,\n",
      "           [0.2706, 0.2157, 0.2275],\n",
      "           [0.2627, 0.2078, 0.2118],\n",
      "           [0.2471, 0.2000, 0.1961]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[0.3412, 0.2510, 0.2510],\n",
      "           [0.3412, 0.2510, 0.2510],\n",
      "           [0.3490, 0.2588, 0.2667],\n",
      "           ...,\n",
      "           [0.3373, 0.2510, 0.2627],\n",
      "           [0.2902, 0.2275, 0.2510],\n",
      "           [0.3137, 0.2392, 0.2549]],\n",
      "\n",
      "          [[0.3412, 0.2510, 0.2510],\n",
      "           [0.3412, 0.2510, 0.2510],\n",
      "           [0.3451, 0.2549, 0.2588],\n",
      "           ...,\n",
      "           [0.3294, 0.2510, 0.2627],\n",
      "           [0.2941, 0.2314, 0.2549],\n",
      "           [0.3098, 0.2353, 0.2510]],\n",
      "\n",
      "          [[0.3373, 0.2510, 0.2471],\n",
      "           [0.3373, 0.2510, 0.2471],\n",
      "           [0.3412, 0.2510, 0.2549],\n",
      "           ...,\n",
      "           [0.3255, 0.2471, 0.2588],\n",
      "           [0.2941, 0.2314, 0.2549],\n",
      "           [0.3020, 0.2353, 0.2471]],\n",
      "\n",
      "          ...,\n",
      "\n",
      "          [[0.2588, 0.1961, 0.1922],\n",
      "           [0.2588, 0.1961, 0.1922],\n",
      "           [0.2549, 0.1922, 0.1882],\n",
      "           ...,\n",
      "           [0.2627, 0.2000, 0.1961],\n",
      "           [0.2471, 0.1843, 0.1725],\n",
      "           [0.2824, 0.2196, 0.2157]],\n",
      "\n",
      "          [[0.2588, 0.1961, 0.1922],\n",
      "           [0.2588, 0.1961, 0.1922],\n",
      "           [0.2627, 0.1922, 0.1882],\n",
      "           ...,\n",
      "           [0.2627, 0.2000, 0.1961],\n",
      "           [0.2471, 0.1843, 0.1725],\n",
      "           [0.2902, 0.2314, 0.2275]],\n",
      "\n",
      "          [[0.2549, 0.1922, 0.1882],\n",
      "           [0.2549, 0.1922, 0.1882],\n",
      "           [0.2667, 0.1961, 0.1922],\n",
      "           ...,\n",
      "           [0.2627, 0.2000, 0.1961],\n",
      "           [0.2471, 0.1843, 0.1725],\n",
      "           [0.2941, 0.2314, 0.2275]]],\n",
      "\n",
      "\n",
      "         [[[0.3412, 0.2510, 0.2510],\n",
      "           [0.3412, 0.2510, 0.2510],\n",
      "           [0.3490, 0.2588, 0.2627],\n",
      "           ...,\n",
      "           [0.3373, 0.2549, 0.2667],\n",
      "           [0.2902, 0.2275, 0.2510],\n",
      "           [0.3176, 0.2431, 0.2588]],\n",
      "\n",
      "          [[0.3412, 0.2510, 0.2510],\n",
      "           [0.3412, 0.2510, 0.2510],\n",
      "           [0.3412, 0.2510, 0.2510],\n",
      "           ...,\n",
      "           [0.3294, 0.2588, 0.2667],\n",
      "           [0.2902, 0.2275, 0.2510],\n",
      "           [0.3137, 0.2392, 0.2549]],\n",
      "\n",
      "          [[0.3373, 0.2510, 0.2471],\n",
      "           [0.3373, 0.2510, 0.2471],\n",
      "           [0.3412, 0.2510, 0.2510],\n",
      "           ...,\n",
      "           [0.3216, 0.2510, 0.2549],\n",
      "           [0.2941, 0.2314, 0.2549],\n",
      "           [0.3098, 0.2353, 0.2510]],\n",
      "\n",
      "          ...,\n",
      "\n",
      "          [[0.2588, 0.1961, 0.1922],\n",
      "           [0.2588, 0.1961, 0.1922],\n",
      "           [0.2549, 0.1922, 0.1882],\n",
      "           ...,\n",
      "           [0.2627, 0.2000, 0.1961],\n",
      "           [0.2510, 0.1882, 0.1765],\n",
      "           [0.2902, 0.2275, 0.2275]],\n",
      "\n",
      "          [[0.2588, 0.1961, 0.1922],\n",
      "           [0.2588, 0.1961, 0.1922],\n",
      "           [0.2627, 0.1922, 0.1882],\n",
      "           ...,\n",
      "           [0.2627, 0.2000, 0.1961],\n",
      "           [0.2510, 0.1882, 0.1765],\n",
      "           [0.2980, 0.2392, 0.2353]],\n",
      "\n",
      "          [[0.2549, 0.1922, 0.1882],\n",
      "           [0.2549, 0.1922, 0.1882],\n",
      "           [0.2667, 0.1961, 0.1922],\n",
      "           ...,\n",
      "           [0.2627, 0.2000, 0.1961],\n",
      "           [0.2510, 0.1882, 0.1765],\n",
      "           [0.3059, 0.2392, 0.2353]]],\n",
      "\n",
      "\n",
      "         [[[0.3412, 0.2510, 0.2510],\n",
      "           [0.3412, 0.2510, 0.2510],\n",
      "           [0.3490, 0.2588, 0.2627],\n",
      "           ...,\n",
      "           [0.3176, 0.2392, 0.2471],\n",
      "           [0.2941, 0.2314, 0.2549],\n",
      "           [0.3216, 0.2471, 0.2667]],\n",
      "\n",
      "          [[0.3412, 0.2510, 0.2510],\n",
      "           [0.3412, 0.2510, 0.2510],\n",
      "           [0.3412, 0.2510, 0.2510],\n",
      "           ...,\n",
      "           [0.3137, 0.2392, 0.2471],\n",
      "           [0.2863, 0.2235, 0.2471],\n",
      "           [0.3098, 0.2392, 0.2549]],\n",
      "\n",
      "          [[0.3373, 0.2510, 0.2471],\n",
      "           [0.3373, 0.2510, 0.2471],\n",
      "           [0.3412, 0.2510, 0.2510],\n",
      "           ...,\n",
      "           [0.3059, 0.2353, 0.2431],\n",
      "           [0.2863, 0.2235, 0.2471],\n",
      "           [0.3020, 0.2314, 0.2471]],\n",
      "\n",
      "          ...,\n",
      "\n",
      "          [[0.2588, 0.1961, 0.1922],\n",
      "           [0.2588, 0.1961, 0.1922],\n",
      "           [0.2549, 0.1922, 0.1882],\n",
      "           ...,\n",
      "           [0.2627, 0.2000, 0.1961],\n",
      "           [0.2549, 0.1922, 0.1804],\n",
      "           [0.3059, 0.2353, 0.2353]],\n",
      "\n",
      "          [[0.2588, 0.1961, 0.1922],\n",
      "           [0.2588, 0.1961, 0.1922],\n",
      "           [0.2627, 0.1922, 0.1882],\n",
      "           ...,\n",
      "           [0.2627, 0.2000, 0.1961],\n",
      "           [0.2549, 0.1922, 0.1804],\n",
      "           [0.3059, 0.2353, 0.2353]],\n",
      "\n",
      "          [[0.2549, 0.1922, 0.1882],\n",
      "           [0.2549, 0.1922, 0.1882],\n",
      "           [0.2667, 0.1961, 0.1922],\n",
      "           ...,\n",
      "           [0.2588, 0.1961, 0.1922],\n",
      "           [0.2549, 0.1922, 0.1804],\n",
      "           [0.3137, 0.2392, 0.2392]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0.5961, 0.3490, 0.1765],\n",
      "           [0.5961, 0.3490, 0.1882],\n",
      "           [0.6275, 0.3451, 0.1961],\n",
      "           ...,\n",
      "           [0.5765, 0.3098, 0.1569],\n",
      "           [0.5686, 0.3137, 0.1569],\n",
      "           [0.5569, 0.2863, 0.1216]],\n",
      "\n",
      "          [[0.5922, 0.3451, 0.1804],\n",
      "           [0.5961, 0.3490, 0.1882],\n",
      "           [0.6275, 0.3451, 0.1961],\n",
      "           ...,\n",
      "           [0.5765, 0.3098, 0.1569],\n",
      "           [0.5686, 0.3137, 0.1569],\n",
      "           [0.5608, 0.2902, 0.1255]],\n",
      "\n",
      "          [[0.5922, 0.3451, 0.1804],\n",
      "           [0.5961, 0.3490, 0.1882],\n",
      "           [0.6275, 0.3451, 0.1961],\n",
      "           ...,\n",
      "           [0.5843, 0.3176, 0.1647],\n",
      "           [0.5686, 0.3137, 0.1569],\n",
      "           [0.5647, 0.2941, 0.1294]],\n",
      "\n",
      "          ...,\n",
      "\n",
      "          [[0.4824, 0.2471, 0.1020],\n",
      "           [0.5059, 0.2627, 0.1176],\n",
      "           [0.4824, 0.2510, 0.1020],\n",
      "           ...,\n",
      "           [0.4863, 0.2431, 0.0980],\n",
      "           [0.4863, 0.2431, 0.0980],\n",
      "           [0.4706, 0.2235, 0.0627]],\n",
      "\n",
      "          [[0.4824, 0.2471, 0.1020],\n",
      "           [0.5059, 0.2627, 0.1176],\n",
      "           [0.4784, 0.2471, 0.0941],\n",
      "           ...,\n",
      "           [0.4902, 0.2471, 0.1020],\n",
      "           [0.4863, 0.2431, 0.0980],\n",
      "           [0.4745, 0.2275, 0.0667]],\n",
      "\n",
      "          [[0.4824, 0.2471, 0.1020],\n",
      "           [0.5098, 0.2667, 0.1216],\n",
      "           [0.4784, 0.2471, 0.0863],\n",
      "           ...,\n",
      "           [0.4902, 0.2471, 0.1020],\n",
      "           [0.4863, 0.2431, 0.0980],\n",
      "           [0.4745, 0.2275, 0.0667]]],\n",
      "\n",
      "\n",
      "         [[[0.5882, 0.3412, 0.1725],\n",
      "           [0.5843, 0.3373, 0.1765],\n",
      "           [0.6196, 0.3333, 0.1843],\n",
      "           ...,\n",
      "           [0.5765, 0.3098, 0.1569],\n",
      "           [0.5686, 0.3137, 0.1569],\n",
      "           [0.5529, 0.2824, 0.1176]],\n",
      "\n",
      "          [[0.5882, 0.3412, 0.1765],\n",
      "           [0.5843, 0.3373, 0.1765],\n",
      "           [0.6196, 0.3373, 0.1882],\n",
      "           ...,\n",
      "           [0.5804, 0.3137, 0.1608],\n",
      "           [0.5686, 0.3137, 0.1569],\n",
      "           [0.5569, 0.2863, 0.1216]],\n",
      "\n",
      "          [[0.5882, 0.3412, 0.1765],\n",
      "           [0.5882, 0.3412, 0.1804],\n",
      "           [0.6196, 0.3373, 0.1882],\n",
      "           ...,\n",
      "           [0.5804, 0.3176, 0.1647],\n",
      "           [0.5686, 0.3137, 0.1569],\n",
      "           [0.5647, 0.2941, 0.1294]],\n",
      "\n",
      "          ...,\n",
      "\n",
      "          [[0.4863, 0.2510, 0.1059],\n",
      "           [0.5020, 0.2588, 0.1137],\n",
      "           [0.4863, 0.2510, 0.1059],\n",
      "           ...,\n",
      "           [0.4863, 0.2431, 0.0980],\n",
      "           [0.4824, 0.2392, 0.0941],\n",
      "           [0.4667, 0.2235, 0.0627]],\n",
      "\n",
      "          [[0.4863, 0.2510, 0.1059],\n",
      "           [0.5020, 0.2588, 0.1137],\n",
      "           [0.4824, 0.2510, 0.0980],\n",
      "           ...,\n",
      "           [0.4863, 0.2431, 0.0980],\n",
      "           [0.4863, 0.2431, 0.0980],\n",
      "           [0.4706, 0.2275, 0.0667]],\n",
      "\n",
      "          [[0.4863, 0.2510, 0.1059],\n",
      "           [0.5020, 0.2588, 0.1137],\n",
      "           [0.4784, 0.2471, 0.0902],\n",
      "           ...,\n",
      "           [0.4902, 0.2471, 0.1020],\n",
      "           [0.4863, 0.2431, 0.0980],\n",
      "           [0.4745, 0.2275, 0.0667]]],\n",
      "\n",
      "\n",
      "         [[[0.5843, 0.3373, 0.1647],\n",
      "           [0.5647, 0.3176, 0.1569],\n",
      "           [0.6039, 0.3216, 0.1725],\n",
      "           ...,\n",
      "           [0.5843, 0.3176, 0.1647],\n",
      "           [0.5647, 0.3098, 0.1529],\n",
      "           [0.5529, 0.2824, 0.1176]],\n",
      "\n",
      "          [[0.5843, 0.3373, 0.1686],\n",
      "           [0.5686, 0.3216, 0.1608],\n",
      "           [0.6078, 0.3216, 0.1725],\n",
      "           ...,\n",
      "           [0.5843, 0.3176, 0.1647],\n",
      "           [0.5647, 0.3098, 0.1529],\n",
      "           [0.5569, 0.2863, 0.1216]],\n",
      "\n",
      "          [[0.5843, 0.3373, 0.1725],\n",
      "           [0.5765, 0.3294, 0.1686],\n",
      "           [0.6078, 0.3255, 0.1765],\n",
      "           ...,\n",
      "           [0.5804, 0.3137, 0.1608],\n",
      "           [0.5647, 0.3098, 0.1529],\n",
      "           [0.5647, 0.2941, 0.1294]],\n",
      "\n",
      "          ...,\n",
      "\n",
      "          [[0.4902, 0.2549, 0.1098],\n",
      "           [0.4980, 0.2549, 0.1059],\n",
      "           [0.4902, 0.2588, 0.1098],\n",
      "           ...,\n",
      "           [0.4863, 0.2431, 0.0980],\n",
      "           [0.4824, 0.2392, 0.0941],\n",
      "           [0.4667, 0.2235, 0.0627]],\n",
      "\n",
      "          [[0.4902, 0.2549, 0.1098],\n",
      "           [0.4941, 0.2510, 0.1059],\n",
      "           [0.4863, 0.2549, 0.1020],\n",
      "           ...,\n",
      "           [0.4863, 0.2431, 0.0980],\n",
      "           [0.4824, 0.2392, 0.0941],\n",
      "           [0.4706, 0.2235, 0.0627]],\n",
      "\n",
      "          [[0.4941, 0.2588, 0.1137],\n",
      "           [0.4902, 0.2471, 0.1020],\n",
      "           [0.4824, 0.2549, 0.0941],\n",
      "           ...,\n",
      "           [0.4863, 0.2431, 0.0980],\n",
      "           [0.4863, 0.2431, 0.0980],\n",
      "           [0.4745, 0.2235, 0.0627]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[0.7765, 0.4510, 0.2549],\n",
      "           [0.7961, 0.4706, 0.2745],\n",
      "           [0.8118, 0.4863, 0.2902],\n",
      "           ...,\n",
      "           [0.7686, 0.4471, 0.2510],\n",
      "           [0.8157, 0.4902, 0.2941],\n",
      "           [0.8196, 0.5059, 0.3137]],\n",
      "\n",
      "          [[0.7725, 0.4471, 0.2510],\n",
      "           [0.7882, 0.4627, 0.2667],\n",
      "           [0.8157, 0.4902, 0.2941],\n",
      "           ...,\n",
      "           [0.7725, 0.4510, 0.2549],\n",
      "           [0.8118, 0.4863, 0.2902],\n",
      "           [0.8039, 0.4902, 0.2980]],\n",
      "\n",
      "          [[0.7686, 0.4431, 0.2471],\n",
      "           [0.7765, 0.4510, 0.2549],\n",
      "           [0.8196, 0.4941, 0.2980],\n",
      "           ...,\n",
      "           [0.7843, 0.4627, 0.2667],\n",
      "           [0.8039, 0.4784, 0.2824],\n",
      "           [0.7765, 0.4627, 0.2706]],\n",
      "\n",
      "          ...,\n",
      "\n",
      "          [[0.6471, 0.3451, 0.1647],\n",
      "           [0.7373, 0.4353, 0.2549],\n",
      "           [0.7373, 0.4353, 0.2510],\n",
      "           ...,\n",
      "           [0.6471, 0.3451, 0.1686],\n",
      "           [0.6824, 0.3843, 0.2157],\n",
      "           [0.7176, 0.4039, 0.2431]],\n",
      "\n",
      "          [[0.6471, 0.3451, 0.1647],\n",
      "           [0.7490, 0.4471, 0.2627],\n",
      "           [0.7529, 0.4510, 0.2706],\n",
      "           ...,\n",
      "           [0.6471, 0.3451, 0.1725],\n",
      "           [0.6863, 0.3843, 0.2157],\n",
      "           [0.7373, 0.4157, 0.2588]],\n",
      "\n",
      "          [[0.6510, 0.3490, 0.1686],\n",
      "           [0.7529, 0.4510, 0.2706],\n",
      "           [0.7647, 0.4627, 0.2824],\n",
      "           ...,\n",
      "           [0.6471, 0.3451, 0.1725],\n",
      "           [0.6902, 0.3882, 0.2118],\n",
      "           [0.7490, 0.4235, 0.2667]]],\n",
      "\n",
      "\n",
      "         [[[0.7608, 0.4353, 0.2392],\n",
      "           [0.7765, 0.4510, 0.2549],\n",
      "           [0.8353, 0.5098, 0.3137],\n",
      "           ...,\n",
      "           [0.7686, 0.4471, 0.2510],\n",
      "           [0.8039, 0.4784, 0.2824],\n",
      "           [0.8039, 0.4902, 0.2980]],\n",
      "\n",
      "          [[0.7569, 0.4314, 0.2353],\n",
      "           [0.7725, 0.4471, 0.2510],\n",
      "           [0.8314, 0.5059, 0.3098],\n",
      "           ...,\n",
      "           [0.7725, 0.4510, 0.2549],\n",
      "           [0.8039, 0.4784, 0.2824],\n",
      "           [0.7922, 0.4784, 0.2863]],\n",
      "\n",
      "          [[0.7490, 0.4235, 0.2275],\n",
      "           [0.7647, 0.4392, 0.2431],\n",
      "           [0.8235, 0.4980, 0.3020],\n",
      "           ...,\n",
      "           [0.7804, 0.4588, 0.2627],\n",
      "           [0.8000, 0.4745, 0.2784],\n",
      "           [0.7686, 0.4549, 0.2627]],\n",
      "\n",
      "          ...,\n",
      "\n",
      "          [[0.6627, 0.3608, 0.1804],\n",
      "           [0.7373, 0.4353, 0.2549],\n",
      "           [0.7294, 0.4275, 0.2471],\n",
      "           ...,\n",
      "           [0.6784, 0.3765, 0.2039],\n",
      "           [0.6902, 0.3882, 0.2157],\n",
      "           [0.7137, 0.4000, 0.2392]],\n",
      "\n",
      "          [[0.6627, 0.3608, 0.1804],\n",
      "           [0.7451, 0.4431, 0.2588],\n",
      "           [0.7608, 0.4549, 0.2745],\n",
      "           ...,\n",
      "           [0.6745, 0.3725, 0.2000],\n",
      "           [0.6980, 0.3961, 0.2196],\n",
      "           [0.7333, 0.4157, 0.2549]],\n",
      "\n",
      "          [[0.6627, 0.3608, 0.1804],\n",
      "           [0.7490, 0.4510, 0.2588],\n",
      "           [0.7765, 0.4706, 0.2902],\n",
      "           ...,\n",
      "           [0.6745, 0.3725, 0.2000],\n",
      "           [0.7059, 0.4000, 0.2196],\n",
      "           [0.7451, 0.4235, 0.2667]]],\n",
      "\n",
      "\n",
      "         [[[0.7529, 0.4275, 0.2314],\n",
      "           [0.7647, 0.4392, 0.2431],\n",
      "           [0.8510, 0.5255, 0.3294],\n",
      "           ...,\n",
      "           [0.7725, 0.4510, 0.2549],\n",
      "           [0.8000, 0.4745, 0.2784],\n",
      "           [0.7922, 0.4784, 0.2863]],\n",
      "\n",
      "          [[0.7451, 0.4196, 0.2235],\n",
      "           [0.7647, 0.4392, 0.2431],\n",
      "           [0.8392, 0.5137, 0.3176],\n",
      "           ...,\n",
      "           [0.7725, 0.4510, 0.2549],\n",
      "           [0.7961, 0.4706, 0.2745],\n",
      "           [0.7843, 0.4706, 0.2784]],\n",
      "\n",
      "          [[0.7373, 0.4118, 0.2157],\n",
      "           [0.7569, 0.4314, 0.2353],\n",
      "           [0.8235, 0.4980, 0.3020],\n",
      "           ...,\n",
      "           [0.7765, 0.4549, 0.2588],\n",
      "           [0.7961, 0.4706, 0.2745],\n",
      "           [0.7647, 0.4510, 0.2588]],\n",
      "\n",
      "          ...,\n",
      "\n",
      "          [[0.6706, 0.3686, 0.1882],\n",
      "           [0.7373, 0.4353, 0.2549],\n",
      "           [0.7255, 0.4235, 0.2431],\n",
      "           ...,\n",
      "           [0.6980, 0.3961, 0.2235],\n",
      "           [0.6941, 0.3882, 0.2118],\n",
      "           [0.7098, 0.3961, 0.2392]],\n",
      "\n",
      "          [[0.6706, 0.3686, 0.1882],\n",
      "           [0.7412, 0.4431, 0.2549],\n",
      "           [0.7647, 0.4588, 0.2784],\n",
      "           ...,\n",
      "           [0.6941, 0.3922, 0.2196],\n",
      "           [0.7059, 0.4000, 0.2196],\n",
      "           [0.7333, 0.4118, 0.2549]],\n",
      "\n",
      "          [[0.6706, 0.3686, 0.1882],\n",
      "           [0.7451, 0.4471, 0.2549],\n",
      "           [0.7843, 0.4784, 0.2980],\n",
      "           ...,\n",
      "           [0.6902, 0.3882, 0.2157],\n",
      "           [0.7137, 0.4078, 0.2275],\n",
      "           [0.7451, 0.4235, 0.2667]]]]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "data = EyeBlinkDataset(train=True)\n",
    "data_loader = torch.utils.data.DataLoader(data, batch_size=16, shuffle=True)\n",
    "\n",
    "print(len(data))\n",
    "\n",
    "seq, label = next(iter(data_loader))\n",
    "print(seq)\n",
    "\n",
    "#for seq, label in data_loader:\n",
    "#    print(seq.shape)\n",
    "#    print(label)\n",
    "\n",
    "\n",
    "\n",
    "# seq_ = seq.permute((0,4,3,1,2))\n",
    "# print(seq_.shape)\n",
    "# model = P2B2().double()\n",
    "# seq_ = model(seq_)\n",
    "# print(seq_)\n",
    "# print(seq_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['datasets/train/training/blink/8', 'datasets/train/training/blink/141', 'datasets/train/training/blink/151', 'datasets/train/training/blink/153', 'datasets/train/training/blink/155', 'datasets/train/training/blink/158', 'datasets/train/training/blink/160', 'datasets/train/training/blink/179', 'datasets/train/training/blink/181', 'datasets/train/training/blink/198', 'datasets/train/training/unblink/55', 'datasets/train/training/unblink/61', 'datasets/train/training/unblink/76', 'datasets/train/training/unblink/78', 'datasets/train/training/unblink/80', 'datasets/train/training/unblink/82', 'datasets/train/training/unblink/93', 'datasets/train/training/unblink/95', 'datasets/train/training/unblink/144', 'datasets/train/training/unblink/146']\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 13 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 96, 96, 13, 3])\n",
      "torch.Size([16, 3, 13, 96, 96])\n",
      "torch.Size([16, 64, 7, 96, 96])\n",
      "torch.Size([16, 64, 4, 96, 96])\n",
      "torch.Size([16, 64, 4, 48, 48])\n",
      "torch.Size([16, 64, 1, 1, 1])\n",
      "torch.Size([16, 64])\n",
      "tensor([[-0.2923,  0.0033],\n",
      "        [ 0.1952, -0.3001],\n",
      "        [-0.1977, -0.1335],\n",
      "        [-0.1878, -0.1265],\n",
      "        [-0.0150, -0.1348],\n",
      "        [-0.0646, -0.0888],\n",
      "        [-0.0077, -0.2171],\n",
      "        [-0.0121, -0.2385],\n",
      "        [-0.0279, -0.2462],\n",
      "        [-0.0685, -0.1611],\n",
      "        [ 0.0223, -0.1528],\n",
      "        [ 0.0178, -0.1747],\n",
      "        [-0.1238, -0.1717],\n",
      "        [-0.2035, -0.1850],\n",
      "        [-0.2284, -0.1847],\n",
      "        [-0.2527, -0.1699]], dtype=torch.float64, grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = EyeBlinkDataset(train=True)\n",
    "data_loader = torch.utils.data.DataLoader(data, batch_size=16)\n",
    "seq, label = next(iter(data_loader))\n",
    "def display_seq(seq):\n",
    "    plt.figure(figsize=(5,5))\n",
    "    for i in range(seq.shape[3]):\n",
    "        plt.subplot(3,5,i+1)\n",
    "        img = seq[5,:,:,i,:3]\n",
    "        plt.imshow(img.astype(int))\n",
    "    plt.show()\n",
    "\n",
    "# print(len(data))\n",
    "# print(seq[3,:,:,:,:])\n",
    "\n",
    "# tmp = seq[0,:,:,0,:].numpy()\n",
    "# print(tmp.shape)\n",
    "\n",
    "display_seq(seq.numpy())\n",
    "print(seq.shape)\n",
    "seq_ = seq.permute((0,4,3,1,2))\n",
    "print(seq_.shape)\n",
    "\n",
    "class Conv(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Conv, self).__init__()\n",
    "        self.conv1 = nn.Conv3d(in_channels=3, out_channels=64, kernel_size=(3,3,3), stride=(2,1,1), padding=(1,1,1))\n",
    "        self.batchnorm = nn.BatchNorm3d(num_features=64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool3d(kernel_size=(3,3,3), stride=(2,1,1), padding=(1,1,1))\n",
    "        self.conv2 = nn.Conv3d(in_channels=64, out_channels=64, kernel_size=(3,1,1), stride=(1,2,2), padding=(1,0,0))\n",
    "        self.avgpool = nn.AvgPool3d(kernel_size=(4,48,48))\n",
    "        self.linear = nn.Linear(64,2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.batchnorm(x)\n",
    "        x = self.relu(x)\n",
    "        print(x.shape)\n",
    "        x= self.maxpool(x)\n",
    "        print(x.shape)\n",
    "        x = self.conv2(x)\n",
    "        print(x.shape)\n",
    "        x = self.avgpool(x)\n",
    "        print(x.shape)\n",
    "        x = x.squeeze()\n",
    "        print(x.shape)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "model = Conv().double()\n",
    "seq_ = model(seq_)\n",
    "print(seq_)\n",
    "\n",
    "# seq__ = model(seq.permute((0,4,3,1,2)))\n",
    "# tmp = torch.add(seq_, seq__)\n",
    "# print(\"seq_: \", seq_)\n",
    "# print(\"seq__: \", seq__)\n",
    "# print(\"tmp: \", tmp)\n",
    "# print(seq_.shape)\n",
    "# print(seq__.shape)\n",
    "# print(tmp.shape)\n",
    "\n",
    "# print(seq__)\n",
    "# print(seq_.shape)\n",
    "\n",
    "# display_seq(seq_.permute((0,3,4,2,1)).detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "tmp = torch.randn(20, 16, 10, 50, 100)\n",
    "print(tmp.dtype)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-projects",
   "language": "python",
   "name": "pytorch-projects"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
